            """

            for addr in chain['payload']:
                html += f"{addr}\n"

            html += """
            </pre>
            """

        html += """
        </body>
        </html>
        """

        # Save to file if filename provided
        if filename:
            try:
                with open(filename, 'w') as f:
                    f.write(html)
                self.logger.info(f"Report saved to {filename}")
                return filename
            except Exception as e:
                self.logger.error(f"Error saving report: {e}")
                return None
        else:
            return html

def run_rop_chain_generator(app):
    """Initialize and run the ROP chain generator"""

    # Check if binary is loaded
    if not app.binary_path:
        app.update_output.emit(log_message("[ROP Chain Generator] No binary loaded"))
        return

    # Create and configure the generator
    generator = ROPChainGenerator({
        'max_chain_length': 20,
        'max_gadget_size': 10,
        'arch': 'x86_64'  # Default to x86_64
    })

    # Set binary
    app.update_output.emit(log_message("[ROP Chain Generator] Setting binary..."))
    if generator.set_binary(app.binary_path):
        app.update_output.emit(log_message(f"[ROP Chain Generator] Binary set: {app.binary_path}"))

        # Ask for architecture
        arch_options = ['x86_64', 'x86', 'arm', 'arm64', 'mips']
        arch, ok = QInputDialog.getItem(
            app,
            "Architecture",
            "Select architecture:",
            arch_options,
            0,  # Default to x86_64
            False
        )

        if not ok:
            app.update_output.emit(log_message("[ROP Chain Generator] Cancelled"))
            return

        generator.arch = arch
        app.update_output.emit(log_message(f"[ROP Chain Generator] Architecture: {arch}"))

        # Ask for target function
        target_function, ok = QInputDialog.getText(
            app,
            "Target Function",
            "Enter target function name (leave empty for default targets):"
        )

        if not ok:
            app.update_output.emit(log_message("[ROP Chain Generator] Cancelled"))
            return

        if target_function:
            generator.add_target_function(target_function)
        else:
            generator._add_default_targets()

        # Find gadgets
        app.update_output.emit(log_message("[ROP Chain Generator] Finding gadgets..."))
        if generator.find_gadgets():
            app.update_output.emit(log_message(f"[ROP Chain Generator] Found {len(generator.gadgets)} gadgets"))

            # Generate chains
            app.update_output.emit(log_message("[ROP Chain Generator] Generating chains..."))
            if generator.generate_chains():
                app.update_output.emit(log_message(f"[ROP Chain Generator] Generated {len(generator.chains)} chains"))

                # Get results
                results = generator.get_results()

                # Display summary
                app.update_output.emit(log_message("[ROP Chain Generator] Results:"))
                app.update_output.emit(log_message(f"- Total gadgets: {results['summary']['total_gadgets']}"))
                app.update_output.emit(log_message(f"- Total chains: {results['summary']['total_chains']}"))
                app.update_output.emit(log_message(f"- Total targets: {results['summary']['total_targets']}"))

                # Add to analyze results
                if not hasattr(app, "analyze_results"):
                    app.analyze_results = []

                app.analyze_results.append("\n=== ROP CHAIN GENERATOR RESULTS ===")
                app.analyze_results.append(f"Total gadgets: {results['summary']['total_gadgets']}")
                app.analyze_results.append(f"Total chains: {results['summary']['total_chains']}")
                app.analyze_results.append(f"Total targets: {results['summary']['total_targets']}")

                # Display chains
                for i, chain in enumerate(results['chains']):
                    app.analyze_results.append(f"\nChain {i+1}: {chain['description']}")
                    app.analyze_results.append(f"Target: {chain['target']['name']}")
                    app.analyze_results.append(f"Length: {chain['length']} gadgets")

                    app.analyze_results.append("Gadgets:")
                    for j, gadget in enumerate(chain['gadgets']):
                        app.analyze_results.append(f"  {j+1}. {gadget['address']}: {gadget['instruction']}")

                    app.analyze_results.append("Payload:")
                    for addr in chain['payload']:
                        app.analyze_results.append(f"  {addr}")

                # Ask if user wants to generate a report
                generate_report = QMessageBox.question(
                    app,
                    "Generate Report",
                    "Do you want to generate a report of the ROP chain generation results?",
                    QMessageBox.Yes | QMessageBox.No
                ) == QMessageBox.Yes

                if generate_report:
                    # Ask for report filename
                    filename, _ = QFileDialog.getSaveFileName(
                        app,
                        "Save Report",
                        "",
                        "HTML Files (*.html);;All Files (*)"
                    )

                    if filename:
                        if not filename.endswith('.html'):
                            filename += '.html'

                        report_path = generator.generate_report(filename)
                        if report_path:
                            app.update_output.emit(log_message(f"[ROP Chain Generator] Report saved to {report_path}"))

                            # Ask if user wants to open the report
                            open_report = QMessageBox.question(
                                app,
                                "Open Report",
                                "Do you want to open the report?",
                                QMessageBox.Yes | QMessageBox.No
                            ) == QMessageBox.Yes

                            if open_report:
                                webbrowser.open(f"file://{os.path.abspath(report_path)}")
                        else:
                            app.update_output.emit(log_message("[ROP Chain Generator] Failed to generate report"))
            else:
                app.update_output.emit(log_message("[ROP Chain Generator] Failed to generate chains"))
        else:
            app.update_output.emit(log_message("[ROP Chain Generator] Failed to find gadgets"))
    else:
        app.update_output.emit(log_message("[ROP Chain Generator] Failed to set binary"))

    # Store the generator instance
    app.rop_chain_generator = generator

def run_symbolic_execution(app):
    """Initialize and run the symbolic execution engine"""

    # Check if binary is loaded
    if not app.binary_path:
        app.update_output.emit(log_message("[Symbolic Execution] No binary loaded"))
        return

    # Create and configure the engine
    engine = SymbolicExecutionEngine(app.binary_path, max_paths=1000, timeout=300)

    # Set binary
    app.update_output.emit(log_message("[Symbolic Execution] Setting binary..."))
    if engine.set_binary(app.binary_path):
        app.update_output.emit(log_message(f"[Symbolic Execution] Binary set: {app.binary_path}"))

        # Ask for target function
        target_function, ok = QInputDialog.getText(
            app,
            "Target Function",
            "Enter target function name (leave empty for auto-detection):"
        )

        if not ok:
            app.update_output.emit(log_message("[Symbolic Execution] Cancelled"))
            return

        if not target_function:
            target_function = None

        # Add default symbolic variables
        engine._add_default_symbolic_variables()

        # Run analysis
        app.update_output.emit(log_message("[Symbolic Execution] Running analysis..."))
        if engine.run_analysis(target_function):
            app.update_output.emit(log_message("[Symbolic Execution] Analysis completed"))

            # Get results
            results = engine.get_results()

            # Display summary
            app.update_output.emit(log_message("[Symbolic Execution] Results:"))
            app.update_output.emit(log_message(f"- Total execution paths: {results['summary']['total_paths']}"))
            app.update_output.emit(log_message(f"- Satisfiable paths: {results['summary']['satisfiable_paths']}"))
            app.update_output.emit(log_message(f"- Unsatisfiable paths: {results['summary']['unsatisfiable_paths']}"))
            app.update_output.emit(log_message(f"- Vulnerabilities found: {results['summary']['vulnerabilities_found']}"))
            app.update_output.emit(log_message(f"- License bypasses found: {results['summary']['license_bypasses_found']}"))

            # Add to analyze results
            if not hasattr(app, "analyze_results"):
                app.analyze_results = []

            app.analyze_results.append("\n=== SYMBOLIC EXECUTION RESULTS ===")
            app.analyze_results.append(f"Total execution paths: {results['summary']['total_paths']}")
            app.analyze_results.append(f"Satisfiable paths: {results['summary']['satisfiable_paths']}")
            app.analyze_results.append(f"Unsatisfiable paths: {results['summary']['unsatisfiable_paths']}")
            app.analyze_results.append(f"Vulnerabilities found: {results['summary']['vulnerabilities_found']}")
            app.analyze_results.append(f"License bypasses found: {results['summary']['license_bypasses_found']}")

            # Display vulnerabilities
            if results['summary']['vulnerabilities_found'] > 0:
                app.analyze_results.append("\nVulnerabilities:")
                for path in results['paths']:
                    if 'vulnerability' in path:
                        vuln = path['vulnerability']
                        app.analyze_results.append(f"- {vuln['type']} at {vuln['address']}: {vuln['description']}")

            # Ask if user wants to generate a report
            generate_report = QMessageBox.question(
                app,
                "Generate Report",
                "Do you want to generate a report of the symbolic execution results?",
                QMessageBox.Yes | QMessageBox.No
            ) == QMessageBox.Yes

            if generate_report:
                # Ask for report filename
                filename, _ = QFileDialog.getSaveFileName(
                    app,
                    "Save Report",
                    "",
                    "HTML Files (*.html);;All Files (*)"
                )

                if filename:
                    if not filename.endswith('.html'):
                        filename += '.html'

                    report_path = engine.generate_report(filename)
                    if report_path:
                        app.update_output.emit(log_message(f"[Symbolic Execution] Report saved to {report_path}"))

                        # Ask if user wants to open the report
                        open_report = QMessageBox.question(
                            app,
                            "Open Report",
                            "Do you want to open the report?",
                            QMessageBox.Yes | QMessageBox.No
                        ) == QMessageBox.Yes

                        if open_report:
                            webbrowser.open(f"file://{os.path.abspath(report_path)}")
                    else:
                        app.update_output.emit(log_message("[Symbolic Execution] Failed to generate report"))
        else:
            app.update_output.emit(log_message("[Symbolic Execution] Analysis failed"))
    else:
        app.update_output.emit(log_message("[Symbolic Execution] Failed to set binary"))

    # Store the engine instance
    app.symbolic_execution_engine = engine

class IncrementalAnalysisManager:
    """
    Incremental Analysis to Avoid Reprocessing Unchanged Code.

    This class manages incremental analysis of binaries, tracking changes between
    analysis runs to avoid reprocessing unchanged code sections, significantly
    improving performance for large binaries.
    """

    def __init__(self, config=None):
        """Initialize the incremental analysis manager with configuration"""
        self.config = config or {}
        self.logger = logging.getLogger("IntellicrackLogger.IncrementalAnalysis")
        self.cache_dir = self.config.get('cache_dir', os.path.join(os.getcwd(), 'analysis_cache'))
        self.enable_caching = self.config.get('enable_caching', True)
        self.cache = {}
        self.current_binary = None
        self.current_binary_hash = None

        # Create cache directory if it doesn't exist
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)

        # Load cache index
        self._load_cache_index()

    def _load_cache_index(self):
        """Load the cache index from disk"""
        index_path = os.path.join(self.cache_dir, 'index.json')
        if os.path.exists(index_path):
            try:
                with open(index_path, 'r') as f:
                    self.cache = json.load(f)
            except Exception as e:
                self.logger.error(f"Error loading cache index: {e}")
                self.cache = {}

    def _save_cache_index(self):
        """Save the cache index to disk"""
        index_path = os.path.join(self.cache_dir, 'index.json')
        try:
            with open(index_path, 'w') as f:
                json.dump(self.cache, f, indent=2)
        except Exception as e:
            self.logger.error(f"Error saving cache index: {e}")

    def set_binary(self, binary_path):
        """Set the current binary for analysis"""
        if not os.path.exists(binary_path):
            self.logger.error(f"Binary not found: {binary_path}")
            return False

        self.current_binary = binary_path

        # Calculate hash of binary
        self.current_binary_hash = self._calculate_file_hash(binary_path)

        # Check if binary is in cache
        if self.current_binary_hash in self.cache:
            self.logger.info(f"Binary found in cache: {binary_path}")
            return True
        else:
            self.logger.info(f"Binary not found in cache: {binary_path}")
            return False

    def _calculate_file_hash(self, file_path):
        """Calculate a hash of the file contents"""

        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def get_cached_analysis(self, analysis_type):
        """Get cached analysis results for the current binary"""
        if not self.enable_caching:
            return None

        if not self.current_binary_hash:
            self.logger.error("No binary set")
            return None

        if self.current_binary_hash not in self.cache:
            return None

        if analysis_type not in self.cache[self.current_binary_hash]:
            return None

        # Get cache file path
        cache_file = self.cache[self.current_binary_hash][analysis_type]

        if not os.path.exists(cache_file):
            self.logger.error(f"Cache file not found: {cache_file}")
            return None

        try:
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            self.logger.error(f"Error loading cache file: {e}")
            return None

    def cache_analysis(self, analysis_type, results):
        """Cache analysis results for the current binary"""
        if not self.enable_caching:
            return False

        if not self.current_binary_hash:
            self.logger.error("No binary set")
            return False

        # Create cache entry for binary if it doesn't exist
        if self.current_binary_hash not in self.cache:
            self.cache[self.current_binary_hash] = {
                'binary_path': self.current_binary,
                'timestamp': datetime.datetime.now().isoformat()
            }

        # Generate cache file path
        cache_file = os.path.join(
            self.cache_dir,
            f"{self.current_binary_hash}_{analysis_type}.cache"
        )

        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(results, f)

            # Update cache index
            self.cache[self.current_binary_hash][analysis_type] = cache_file
            self._save_cache_index()

            self.logger.info(f"Cached analysis results: {analysis_type}")
            return True
        except Exception as e:
            self.logger.error(f"Error caching analysis results: {e}")
            return False

    def clear_cache(self, binary_hash=None):
        """Clear the cache for a specific binary or all binaries"""
        if binary_hash:
            if binary_hash in self.cache:
                # Delete cache files
                for analysis_type, cache_file in self.cache[binary_hash].items():
                    if analysis_type not in ['binary_path', 'timestamp'] and os.path.exists(cache_file):
                        os.remove(cache_file)

                # Remove from cache index
                del self.cache[binary_hash]
                self._save_cache_index()

                self.logger.info(f"Cleared cache for binary: {binary_hash}")
                return True
            else:
                self.logger.error(f"Binary not found in cache: {binary_hash}")
                return False
        else:
            # Clear all cache
            for binary_hash in list(self.cache.keys()):
                self.clear_cache(binary_hash)

            self.logger.info("Cleared all cache")
            return True

def run_analysis_manager(app):
    """Initialize and run the incremental analysis manager

    This is the main entry point for the standalone incremental analysis feature.
    """
    # Track feature usage
    app.update_output.emit(log_message("[Incremental Analysis] Starting analysis manager"))

    # Performance metrics
    start_time = time.time()

    # Check if binary is loaded
    if not app.binary_path:
        app.update_output.emit(log_message("[Incremental Analysis] No binary loaded"))
        return

    # Log binary details before analysis
    binary_size = os.path.getsize(app.binary_path)
    app.update_output.emit(log_message(f"[Incremental Analysis] Analyzing binary: {os.path.basename(app.binary_path)} ({binary_size/1024:.1f} KB)"))

    # Create and configure the manager
    manager = IncrementalAnalysisManager({
        'cache_dir': os.path.join(os.getcwd(), 'analysis_cache'),
        'enable_caching': True
    })

    # Set binary and track performance metrics
    analysis_phases = []
    app.update_output.emit(log_message("[Incremental Analysis] Setting binary..."))
    if manager.set_binary(app.binary_path):
        app.update_output.emit(log_message("[Incremental Analysis] Binary found in cache"))

        # Ask if user wants to use cached results
        use_cache = QMessageBox.question(
            app,
            "Use Cached Results",
            "Cached analysis results found for this binary. Do you want to use them?",
            QMessageBox.Yes | QMessageBox.No
        ) == QMessageBox.Yes

        if use_cache:
            # Get cached results
            app.update_output.emit(log_message("[Incremental Analysis] Loading cached results..."))

            # Example: Load basic analysis
            basic_analysis = manager.get_cached_analysis('basic')
            if basic_analysis:
                app.update_output.emit(log_message("[Incremental Analysis] Loaded basic analysis from cache"))

                # Apply cached results
                if hasattr(app, "analyze_results"):
                    app.analyze_results = basic_analysis
                    app.update_output.emit(log_message("[Incremental Analysis] Applied cached results"))

                    # Add to analyze results
                    app.analyze_results.append("\n=== INCREMENTAL ANALYSIS ===")
                    app.analyze_results.append("Loaded analysis results from cache")
                    app.analyze_results.append("Binary hash: " + manager.current_binary_hash)

                    # Update UI
                    for result in app.analyze_results:
                        app.update_analysis_results.emit(result)
            else:
                app.update_output.emit(log_message("[Incremental Analysis] No cached basic analysis found"))
        else:
            app.update_output.emit(log_message("[Incremental Analysis] Not using cached results"))
    else:
        app.update_output.emit(log_message("[Incremental Analysis] Binary not found in cache"))

    # Store the manager instance
    app.incremental_analysis_manager = manager

    # Calculate and report performance metrics
    end_time = time.time()
    elapsed_time = end_time - start_time

    # Add performance data to analysis_phases
    analysis_phases.append({
        'phase': 'total',
        'start_time': start_time,
        'end_time': end_time,
        'elapsed_time': elapsed_time,
        'binary_size': binary_size
    })

    # Report performance metrics
    app.update_output.emit(log_message(f"[Incremental Analysis] Analysis completed in {elapsed_time:.2f} seconds"))

    # Save performance metrics for future optimization
    if not hasattr(app, 'performance_metrics'):
        app.performance_metrics = {}

    # Store this run's metrics
    app.performance_metrics['incremental_analysis'] = {
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
        'binary_size': binary_size,
        'total_time': elapsed_time,
        'phases': analysis_phases,
        'cached_used': use_cache if 'use_cache' in locals() else False
    }

    # Update status with performance info
    app.analyze_status.setText(f"Incremental Analysis Complete ({elapsed_time:.2f}s)")

# -------------------------------
# Full System Emulation with QEMU
# -------------------------------

class DockerContainer:
    """
    Manages Docker container operations for distributed analysis
    """

    def __init__(self, binary_path=None, image="ubuntu:latest"):
        """
        Initialize Docker container manager

        Args:
            binary_path: Path to the binary to analyze
            image: Docker image to use
        """
        self.binary_path = binary_path
        self.image = image
        self.container_id = None
        self.snapshots = {}
        self.logger = logging.getLogger(__name__)

        # Check if Docker is available
        try:
            result = subprocess.run(["docker", "--version"], capture_output=True, text=True)
            if result.returncode != 0:
                raise RuntimeError("Docker not available on this system")
            self.logger.info(f"Docker available: {result.stdout.strip()}")
        except Exception as e:
            self.logger.error(f"Docker initialization error: {str(e)}")
            raise

    def start_container(self):
        """
        Start a Docker container with the specified image

        Returns:
            bool: True if container started successfully, False otherwise
        """
        try:
            # Pull the image if not already available
            self.logger.info(f"Pulling Docker image: {self.image}")
            subprocess.run(["docker", "pull", self.image], check=True)

            # Run the container with proper mounts and settings
            self.logger.info(f"Starting Docker container with image {self.image}")
            result = subprocess.run(
                [
                    "docker", "run",
                    "-d",  # Detached mode
                    "--privileged",  # For system level access
                    "-v", f"{os.path.dirname(self.binary_path)}:/mnt/host:ro",  # Mount binary directory
                    "--name", f"intellicrack_analysis_{int(time.time())}",  # Unique name
                    self.image,
                    "tail", "-f", "/dev/null"  # Keep container running
                ],
                capture_output=True,
                text=True
            )

            if result.returncode != 0:
                self.logger.error(f"Failed to start container: {result.stderr}")
                return False

            self.container_id = result.stdout.strip()
            self.logger.info(f"Container started with ID: {self.container_id}")
            return True

        except Exception as e:
            self.logger.error(f"Error starting Docker container: {str(e)}")
            return False

    def stop_container(self):
        """
        Stop and remove the Docker container

        Returns:
            bool: True if container stopped successfully, False otherwise
        """
        if not self.container_id:
            self.logger.warning("No container ID available to stop")
            return False

        try:
            # Stop the container
            self.logger.info(f"Stopping container {self.container_id}")
            subprocess.run(["docker", "stop", self.container_id], check=True)

            # Remove the container
            self.logger.info(f"Removing container {self.container_id}")
            subprocess.run(["docker", "rm", self.container_id], check=True)

            self.logger.info(f"Container {self.container_id} stopped and removed")
            return True

        except Exception as e:
            self.logger.error(f"Error stopping Docker container: {str(e)}")
            return False

    def execute_command(self, command):
        """
        Execute a command in the Docker container

        Args:
            command: Command to execute

        Returns:
            str: Command output
        """
        if not self.container_id:
            self.logger.error("No container ID available to execute command")
            return "ERROR: Container not running"

        try:
            self.logger.info(f"Executing in container: {command}")
            result = subprocess.run(
                ["docker", "exec", self.container_id, "bash", "-c", command],
                capture_output=True,
                text=True
            )

            if result.returncode != 0:
                self.logger.warning(f"Command exited with non-zero status: {result.returncode}")
                self.logger.warning(f"Stderr: {result.stderr}")

            return result.stdout

        except Exception as e:
            self.logger.error(f"Error executing command in Docker container: {str(e)}")
            return f"ERROR: {str(e)}"

    def copy_file_to_container(self, source_path, dest_path):
        """
        Copy a file to the Docker container

        Args:
            source_path: Source file path on host
            dest_path: Destination path in container

        Returns:
            bool: True if file copied successfully, False otherwise
        """
        if not self.container_id:
            self.logger.error("No container ID available for file copy")
            return False

        try:
            # Create target directory if it doesn't exist
            dest_dir = os.path.dirname(dest_path)
            self.execute_command(f"mkdir -p {dest_dir}")

            # Copy file using docker cp
            self.logger.info(f"Copying {source_path} to container:{dest_path}")
            result = subprocess.run(
                ["docker", "cp", source_path, f"{self.container_id}:{dest_path}"],
                capture_output=True,
                text=True
            )

            if result.returncode != 0:
                self.logger.error(f"Failed to copy file: {result.stderr}")
                return False

            return True

        except Exception as e:
            self.logger.error(f"Error copying file to Docker container: {str(e)}")
            return False

    def create_snapshot(self, name):
        """
        Create a snapshot of the container state

        Args:
            name: Snapshot name

        Returns:
            bool: True if snapshot created successfully, False otherwise
        """
        if not self.container_id:
            self.logger.error("No container ID available for snapshot")
            return False

        try:
            # Get filesystem state
            files = self.execute_command("find / -type f -mtime -1 -not -path \"/proc/*\" -not -path \"/sys/*\" -not -path \"/dev/*\" | sort")

            # Get process list
            processes = self.execute_command("ps aux")

            # Get network connections
            network = self.execute_command("netstat -tuln")

            # Store snapshot
            self.snapshots[name] = {
                "timestamp": time.time(),
                "files": files,
                "processes": processes,
                "network": network
            }

            self.logger.info(f"Created container snapshot: {name}")
            return True

        except Exception as e:
            self.logger.error(f"Error creating container snapshot: {str(e)}")
            return False

    def compare_snapshots(self, snapshot1, snapshot2):
        """
        Compare two container snapshots

        Args:
            snapshot1: First snapshot name
            snapshot2: Second snapshot name

        Returns:
            dict: Differences between snapshots
        """
        if snapshot1 not in self.snapshots or snapshot2 not in self.snapshots:
            self.logger.error(f"Snapshot not found: {snapshot1 if snapshot1 not in self.snapshots else snapshot2}")
            return {"error": "Snapshot not found"}

        try:
            # Get snapshots
            s1 = self.snapshots[snapshot1]
            s2 = self.snapshots[snapshot2]

            # Compare files
            files1 = set(s1["files"].splitlines())
            files2 = set(s2["files"].splitlines())

            new_files = files2 - files1
            deleted_files = files1 - files2

            # Compare processes
            processes1 = s1["processes"].splitlines()
            processes2 = s2["processes"].splitlines()

            # Extract just the command part for comparison
            proc1 = [' '.join(p.split()[10:]) for p in processes1 if len(p.split()) > 10]
            proc2 = [' '.join(p.split()[10:]) for p in processes2 if len(p.split()) > 10]

            new_processes = set(proc2) - set(proc1)
            ended_processes = set(proc1) - set(proc2)

            # Compare network connections
            networks1 = set(s1["network"].splitlines())
            networks2 = set(s2["network"].splitlines())

            new_connections = networks2 - networks1
            closed_connections = networks1 - networks2

            return {
                "new_files": list(new_files)[:100],  # Limit to first 100 entries
                "deleted_files": list(deleted_files)[:100],
                "new_processes": list(new_processes),
                "ended_processes": list(ended_processes),
                "new_connections": list(new_connections),
                "closed_connections": list(closed_connections),
                "total_changes": len(new_files) + len(deleted_files) + len(new_processes) +
                                len(ended_processes) + len(new_connections) + len(closed_connections)
            }

        except Exception as e:
            self.logger.error(f"Error comparing snapshots: {str(e)}")
            return {"error": str(e)}

    def collect_analysis_artifacts(self):
        """
        Collect analysis artifacts from container

        Returns:
            dict: Analysis artifacts
        """
        if not self.container_id:
            self.logger.error("No container ID available for artifact collection")
            return {"error": "Container not running"}

        try:
            # Collect interesting files
            self.logger.info("Collecting analysis artifacts from container")

            # Check for created or modified files
            modified_files = self.execute_command("find / -type f -mtime -1 -not -path \"/proc/*\" -not -path \"/sys/*\" -not -path \"/dev/*\" | head -20")

            # Check for log entries
            logs = self.execute_command("cat /var/log/syslog 2>/dev/null | tail -50")

            # Check for network activity
            network = self.execute_command("netstat -tan")

            # Check for open files
            open_files = self.execute_command("lsof 2>/dev/null | grep -v '/lib/' | head -20")

            return {
                "modified_files": modified_files,
                "logs": logs,
                "network": network,
                "open_files": open_files
            }

        except Exception as e:
            self.logger.error(f"Error collecting analysis artifacts: {str(e)}")
            return {"error": str(e)}

class QEMUSystemEmulator:
    """
    Full system emulation using QEMU for comprehensive dynamic analysis
    """

    def __init__(self, binary_path=None, arch="x86_64", rootfs=None):
        """
        Initialize the QEMU system emulator

        Args:
            binary_path: Path to the binary to analyze
            arch: Architecture to emulate (x86_64, arm64, etc.)
            rootfs: Path to the root filesystem to use
        """
        self.binary_path = binary_path
        self.arch = arch
        self.rootfs = rootfs or self._get_default_rootfs(arch)
        self.snapshots = {}
        self.qemu_process = None
        self.logger = logging.getLogger(__name__)

    def _get_default_rootfs(self, arch):
        """Get the default rootfs path for the specified architecture"""
        rootfs_base = os.path.join(os.getcwd(), "rootfs")

        arch_map = {
            "x86_64": "x8664_linux",
            "x86": "x86_linux",
            "arm64": "arm64_linux",
            "arm": "arm_linux",
            "mips": "mips32_linux",
            "mips64": "mips64_linux",
            "windows": "x8664_windows"
        }

        if arch in arch_map:
            return os.path.join(rootfs_base, arch_map[arch])
        else:
            return os.path.join(rootfs_base, "x8664_linux")  # Default to x86_64 Linux

    def start_system(self, memory_mb=1024, with_network=False):
        """
        Start the QEMU system emulation

        Args:
            memory_mb: Amount of memory to allocate to the VM
            with_network: Whether to enable networking

        Returns:
            bool: True if successful, False otherwise
        """
        if not os.path.exists(self.rootfs):
            self.logger.error(f"Rootfs not found: {self.rootfs}")
            return False

        # Build QEMU command
        qemu_bin = f"qemu-system-{self.arch}"
        if self.arch == "x86_64":
            qemu_bin = "qemu-system-x86_64"
        elif self.arch == "arm64":
            qemu_bin = "qemu-system-aarch64"

        cmd = [
            qemu_bin,
            "-m", str(memory_mb),
            "-kernel", os.path.join(self.rootfs, "vmlinuz"),
            "-initrd", os.path.join(self.rootfs, "initrd.img"),
            "-append", "root=/dev/ram0 console=ttyS0 quiet",
            "-nographic",
            "-no-reboot"
        ]

        if with_network:
            cmd.extend(["-netdev", "user,id=net0", "-device", "virtio-net-pci,netdev=net0"])

        # Add binary as a virtio-9p shared folder
        if self.binary_path:
            binary_dir = os.path.dirname(os.path.abspath(self.binary_path))
            cmd.extend([
                "-virtfs", f"local,path={binary_dir},mount_tag=host0,security_model=none,id=host0"
            ])

        try:
            self.logger.info(f"Starting QEMU: {' '.join(cmd)}")
            self.qemu_process = subprocess.Popen(
                cmd,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            return True
        except Exception as e:
            self.logger.error(f"Failed to start QEMU: {e}")
            return False

    def create_snapshot(self, name="pre_license_check"):
        """
        Create a VM snapshot for later comparison

        Args:
            name: Name of the snapshot

        Returns:
            bool: True if successful, False otherwise
        """
        if not self.qemu_process:
            self.logger.error("QEMU not running")
            return False

        try:
            # Send QEMU monitor command to create snapshot
            self.qemu_process.stdin.write(f"savevm {name}\n")
            self.qemu_process.stdin.flush()

            # Wait for confirmation
            for _ in range(10):  # Wait up to 10 seconds
                line = self.qemu_process.stdout.readline()
                if "saved" in line:
                    self.snapshots[name] = {
                        "time": datetime.datetime.now(),
                        "name": name
                    }
                    self.logger.info(f"Created snapshot: {name}")
                    return True
                time.sleep(1)

            self.logger.warning(f"Timeout waiting for snapshot creation: {name}")
            return False
        except Exception as e:
            self.logger.error(f"Failed to create snapshot: {e}")
            return False

    def restore_snapshot(self, name="pre_license_check"):
        """
        Restore a previously created VM snapshot

        Args:
            name: Name of the snapshot to restore

        Returns:
