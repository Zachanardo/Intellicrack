                    all_windows.extend(result.get('windows', []))

        # Sort windows by offset
        all_windows.sort(key=lambda x: x['offset'])

        # Calculate overall entropy (weighted by chunk size)
        total_size = sum(size for _, size in chunk_entropies)
        overall_entropy = sum(entropy * size for entropy, size in chunk_entropies) / total_size if total_size > 0 else 0

        # Find high entropy regions
        high_entropy_regions = [w for w in all_windows if w['entropy'] > 7.0]

        return {
            'overall_entropy': overall_entropy,
            'windows': all_windows,
            'high_entropy_regions': high_entropy_regions,
            'high_entropy_count': len(high_entropy_regions)
        }

    def generate_report(self, filename=None):
        """
        Generate a report of the distributed processing results.

        Args:
            filename: Path to save the HTML report (None to return HTML as string)

        Returns:
            str or None: HTML report as string if filename is None, else path to saved file
        """
        if not self.results:
            self.logger.error("No results to report")
            return None

        # Generate HTML report
        html = f"""
        <html>
        <head>
            <title>Distributed Processing Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                h1, h2, h3 {{ color: #333; }}
                table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
                tr:nth-child(even) {{ background-color: #f9f9f9; }}
                .success {{ color: green; }}
                .failure {{ color: red; }}
            </style>
        </head>
        <body>
            <h1>Distributed Processing Report</h1>
            <p>Binary: {self.binary_path}</p>

            <h2>Summary</h2>
            <table>
                <tr><th>Metric</th><th>Value</th></tr>
                <tr><td>Workers</td><td>{self.num_workers}</td></tr>
                <tr><td>Tasks Completed</td><td>{self.results['tasks_completed']}</td></tr>
                <tr><td>Tasks Failed</td><td>{self.results['tasks_failed']}</td></tr>
                <tr><td>Total Processing Time</td><td>{self.results['total_processing_time']:.2f} seconds</td></tr>
            </table>
        """

        # Add task-specific results
        for task_type, results in self.results['task_results'].items():
            html += f"""
            <h2>{task_type.capitalize()} Results</h2>
            <p>Total: {len(results)}</p>

            <table>
                <tr>
                    <th>Worker</th>
                    <th>Status</th>
                    <th>Time</th>
            """

            # Add task-specific headers
            if task_type == 'analyze_section':
                html += """
                    <th>Section</th>
                    <th>Size</th>
                    <th>Entropy</th>
                    <th>Strings</th>
                """
            elif task_type == 'find_patterns':
                html += """
                    <th>Patterns Found</th>
                """
            elif task_type == 'analyze_entropy':
                html += """
                    <th>Chunk Offset</th>
                    <th>Chunk Size</th>
                    <th>Entropy</th>
                    <th>High Entropy Regions</th>
                """
            elif task_type == 'symbolic_execution':
                html += """
                    <th>Target Function</th>
                    <th>Paths Explored</th>
                    <th>Constraints Solved</th>
                    <th>Vulnerabilities</th>
                """

            html += """
                </tr>
            """

            # Add results
            for result in results:
                status_class = 'success' if result.get('success', False) else 'failure'
                status_text = 'Success' if result.get('success', False) else 'Failure'

                html += f"""
                <tr>
                    <td>{result.get('worker_id', 'N/A')}</td>
                    <td class="{status_class}">{status_text}</td>
                    <td>{result.get('processing_time', 0.0):.2f}s</td>
                """

                # Add task-specific data
                if task_type == 'analyze_section':
                    html += f"""
                    <td>{result.get('section_name', 'N/A')}</td>
                    <td>{result.get('section_size', 0)}</td>
                    <td>{result.get('entropy', 0.0):.2f}</td>
                    <td>{result.get('strings_found', 0)}</td>
                    """
                elif task_type == 'find_patterns':
                    html += f"""
                    <td>{result.get('patterns_found', 0)}</td>
                    """
                elif task_type == 'analyze_entropy':
                    html += f"""
                    <td>0x{result.get('chunk_offset', 0):x}</td>
                    <td>{result.get('chunk_size', 0)}</td>
                    <td>{result.get('chunk_entropy', 0.0):.2f}</td>
                    <td>{result.get('high_entropy_count', 0)}</td>
                    """
                elif task_type == 'symbolic_execution':
                    html += f"""
                    <td>{result.get('target_function', 'N/A')}</td>
                    <td>{result.get('paths_explored', 0)}</td>
                    <td>{result.get('constraints_solved', 0)}</td>
                    <td>{result.get('vulnerabilities_found', 0)}</td>
                    """

                html += """
                </tr>
                """

            html += """
            </table>
            """

            # Add detailed results for pattern finding
            if task_type == 'find_patterns' and any('matches' in r for r in results if r.get('success', False)):
                html += """
                <h3>Pattern Matches</h3>
                <table>
                    <tr>
                        <th>Address</th>
                        <th>Pattern</th>
                        <th>Match</th>
                    </tr>
                """

                all_matches = []
                for result in results:
                    if result.get('success', False) and 'matches' in result:
                        all_matches.extend(result['matches'])

                # Sort by position and limit to first 1000
                all_matches.sort(key=lambda x: x['position'])
                for match in all_matches[:1000]:
                    pattern_str = str(match['pattern'])
                    match_str = str(match['match']) if isinstance(match['match'], str) else str(match['match'])[:20]
                    html += f"""
                    <tr>
                        <td>0x{match['position']:x}</td>
                        <td>{pattern_str}</td>
                        <td>{match_str}</td>
                    </tr>
                    """

                html += """
                </table>
                """

            # Add detailed results for high entropy regions
            if task_type == 'analyze_entropy':
                high_entropy_regions = []
                for result in results:
                    if result.get('success', False) and 'high_entropy_regions' in result:
                        high_entropy_regions.extend(result['high_entropy_regions'])

                if high_entropy_regions:
                    html += """
                    <h3>High Entropy Regions</h3>
                    <table>
                        <tr>
                            <th>Offset</th>
                            <th>Size</th>
                            <th>Entropy</th>
                        </tr>
                    """

                    # Sort by offset and limit to first 1000
                    high_entropy_regions.sort(key=lambda x: x['offset'])
                    for region in high_entropy_regions[:1000]:
                        html += f"""
                        <tr>
                            <td>0x{region['offset']:x}</td>
                            <td>{region['size']}</td>
                            <td>{region['entropy']:.2f}</td>
                        </tr>
                        """

                    html += """
                    </table>
                    """

        html += """
        </body>
        </html>
        """

        # Save to file if filename provided
        if filename:
            try:
                with open(filename, 'w') as f:
                    f.write(html)
                self.logger.info(f"Report saved to {filename}")
                return filename
            except Exception as e:
                self.logger.error(f"Error saving report: {e}")
                return None
        else:
            return html

def run_distributed_processing(app):
    """
    Initialize and run the distributed processing manager with the combined implementation.

    This function sets up the distributed processing manager with user-selected options,
    runs various analysis tasks on the binary, and displays/saves the results.

    Args:
        app: The main application instance with binary_path and update_output attributes
    """

    # Check if binary is loaded
    if not app.binary_path:
        app.update_output.emit(log_message("[Distributed Processing] No binary loaded"))
        return

    # Ask user for processing configuration using a custom dialog
    config_dialog = DistributedProcessingConfigDialog(app.binary_path)
    if not config_dialog.exec_():
        app.update_output.emit(log_message("[Distributed Processing] Cancelled"))
        return

    # Get configuration from dialog
    config = config_dialog.get_config()

    # Create and configure the manager with enhanced options
    app.update_output.emit(log_message("[Distributed Processing] Initializing manager..."))
    try:
        manager = DistributedProcessingManager(config)

        # Set binary
        app.update_output.emit(log_message("[Distributed Processing] Setting binary..."))
        if manager.set_binary(app.binary_path):
            app.update_output.emit(log_message(f"[Distributed Processing] Binary set: {app.binary_path}"))

            # Add tasks based on user selection
            app.update_output.emit(log_message("[Distributed Processing] Adding tasks..."))

            # Add section analysis tasks if selected
            if config.get('run_section_analysis', True):
                # Get sections from PE file if possible, otherwise use default list
                sections = config.get('sections', ['.text', '.data', '.rdata', '.rsrc', '.reloc'])
                for section in sections:
                    manager.add_task('analyze_section', {'section_name': section}, f"Analyze section: {section}")
                app.update_output.emit(log_message(f"[Distributed Processing] Added {len(sections)} section analysis tasks"))

            # Add pattern finding tasks if selected
            if config.get('run_pattern_search', True):
                # Add default patterns plus any user-specified ones
                patterns = []

                # License related patterns
                if config.get('search_license_patterns', True):
                    license_patterns = [
                        rb'license[_-]key',
                        rb'registration[_-]code',
                        rb'serial[_-]number',
                        rb'activation[_-]code',
                        rb'product[_-]key'
                    ]
                    patterns.extend(license_patterns)

                # Hardware ID patterns
                if config.get('search_hardware_patterns', True):
                    hardware_patterns = [
                        rb'hardware[_-]id',
                        rb'machine[_-]id',
                        rb'cpu[_-]id',
                        rb'disk[_-]serial',
                        rb'mac[_-]address'
                    ]
                    patterns.extend(hardware_patterns)

                # Encryption/decryption patterns
                if config.get('search_crypto_patterns', True):
                    crypto_patterns = [
                        rb'aes[_-]',
                        rb'rsa[_-]',
                        rb'decrypt',
                        rb'encrypt',
                        rb'sha[_-]?[1-5]'
                    ]
                    patterns.extend(crypto_patterns)

                # Add custom patterns if provided
                custom_patterns = config.get('custom_patterns', [])
                if custom_patterns:
                    patterns.extend([p.encode() if isinstance(p, str) else p for p in custom_patterns])

                # Use the enhanced pattern search from the new implementation
                if patterns:
                    app.update_output.emit(log_message(f"[Distributed Processing] Running pattern search with {len(patterns)} patterns..."))

                    # Check if we should use the convenience method or add tasks manually
                    if config.get('use_convenience_methods', True):
                        # Store these for later - we'll get the results directly from the method
                        config['pattern_search_results'] = None
                    else:
                        # Add individual tasks for pattern searching
                        chunk_size = config.get('chunk_size', 1024 * 1024)
                        file_size = os.path.getsize(app.binary_path)
                        num_chunks = (file_size + chunk_size - 1) // chunk_size

                        for i in range(num_chunks):
                            start = i * chunk_size
                            end = min((i + 1) * chunk_size, file_size)
                            manager.add_task('find_patterns', {
                                'patterns': patterns,
                                'chunk_start': start,
                                'chunk_end': end
                            }, f"Find patterns in chunk {i+1}/{num_chunks}")

            # Add entropy analysis if selected
            if config.get('run_entropy_analysis', True):
                window_size_kb = config.get('window_size_kb', 64)
                app.update_output.emit(log_message(f"[Distributed Processing] Adding entropy analysis (window size: {window_size_kb}KB)..."))

                if config.get('use_convenience_methods', True):
                    # Store this for later - we'll get the results directly from the method
                    config['entropy_analysis_results'] = None
                else:
                    # Add individual tasks for entropy analysis
                    chunk_size = config.get('chunk_size', 1024 * 1024)
                    file_size = os.path.getsize(app.binary_path)
                    num_chunks = (file_size + chunk_size - 1) // chunk_size
                    window_size = window_size_kb * 1024

                    for i in range(num_chunks):
                        start = i * chunk_size
                        end = min((i + 1) * chunk_size, file_size)
                        manager.add_task('analyze_entropy', {
                            'window_size': window_size,
                            'chunk_start': start,
                            'chunk_end': end
                        }, f"Analyze entropy in chunk {i+1}/{num_chunks}")

            # Add symbolic execution tasks if selected (kept for compatibility)
            if config.get('run_symbolic_execution', False):
                target_functions = config.get('target_functions', ['check_license', 'validate_key', 'is_activated'])
                for target_function in target_functions:
                    manager.add_task('symbolic_execution', {'target_function': target_function}, f"Symbolic execution: {target_function}")
                app.update_output.emit(log_message(f"[Distributed Processing] Added {len(target_functions)} symbolic execution tasks"))

            app.update_output.emit(log_message(f"[Distributed Processing] Added {len(manager.tasks)} tasks in total"))

            # If using convenience methods, run those instead of the task queue
            if config.get('use_convenience_methods', True) and (
                config.get('run_pattern_search', True) or
                config.get('run_entropy_analysis', True)
            ):
                results = {}

                # Run pattern search if selected
                if config.get('run_pattern_search', True) and patterns:
                    app.update_output.emit(log_message("[Distributed Processing] Running pattern search..."))
                    pattern_results = manager.run_distributed_pattern_search(
                        patterns,
                        chunk_size_mb=config.get('chunk_size', 1024*1024) // (1024*1024)
                    )
                    config['pattern_search_results'] = pattern_results
                    results['pattern_search'] = pattern_results
                    app.update_output.emit(log_message(f"[Distributed Processing] Found {len(pattern_results)} pattern matches"))

                # Run entropy analysis if selected
                if config.get('run_entropy_analysis', True):
                    app.update_output.emit(log_message("[Distributed Processing] Running entropy analysis..."))
                    entropy_results = manager.run_distributed_entropy_analysis(
                        window_size_kb=config.get('window_size_kb', 64),
                        chunk_size_mb=config.get('chunk_size', 1024*1024) // (1024*1024)
                    )
                    config['entropy_analysis_results'] = entropy_results
                    results['entropy_analysis'] = entropy_results
                    high_entropy_count = len(entropy_results.get('high_entropy_regions', []))
                    app.update_output.emit(log_message(f"[Distributed Processing] Found {high_entropy_count} high entropy regions"))
                    app.update_output.emit(log_message(f"[Distributed Processing] Overall entropy: {entropy_results.get('overall_entropy', 0):.2f}"))

                # Process and display results
                process_distributed_results(app, manager, results, config)
                return

            # Otherwise, use the traditional task queue approach
            app.update_output.emit(log_message("[Distributed Processing] Starting processing..."))
            if manager.start_processing():
                app.update_output.emit(log_message(f"[Distributed Processing] Started {manager.num_workers} workers"))

                # Collect results
                app.update_output.emit(log_message("[Distributed Processing] Collecting results..."))
                timeout = config.get('timeout', 60)  # Default 60 second timeout
                if manager.collect_results(timeout=timeout):
                    app.update_output.emit(log_message("[Distributed Processing] Results collected"))

                    # Get results
                    results = manager.get_results()

                    # Process and display results
                    process_distributed_results(app, manager, results, config)
                else:
                    app.update_output.emit(log_message("[Distributed Processing] Failed to collect results - timeout or error"))
            else:
                app.update_output.emit(log_message("[Distributed Processing] Failed to start processing"))
        else:
            app.update_output.emit(log_message("[Distributed Processing] Failed to set binary"))
    except Exception as e:
        app.update_output.emit(log_message(f"[Distributed Processing] Error: {str(e)}"))
        logging.exception("Error in distributed processing")


def process_distributed_results(app, manager, results, config):
    """
    Process and display the distributed processing results.

    Args:
        app: Main application instance
        manager: DistributedProcessingManager instance
        results: Results from the manager (either from task queue or convenience methods)
        config: Configuration dictionary
    """

    # Initialize analysis results if not present
    if not hasattr(app, "analyze_results"):
        app.analyze_results = []

    # Create a header for the results
    app.analyze_results.append("\n=== DISTRIBUTED PROCESSING RESULTS ===")
    app.analyze_results.append(f"Backend: {config.get('preferred_backend', 'auto')}")
    app.analyze_results.append(f"Workers: {manager.num_workers}")

    # Check if we have task queue results
    if 'tasks_completed' in results:
        # Display summary from task queue
        app.update_output.emit(log_message("[Distributed Processing] Results:"))
        app.update_output.emit(log_message(f"- Tasks completed: {results['tasks_completed']}"))
        app.update_output.emit(log_message(f"- Tasks failed: {results['tasks_failed']}"))
        app.update_output.emit(log_message(f"- Total processing time: {results['total_processing_time']:.2f} seconds"))

        app.analyze_results.append(f"Tasks completed: {results['tasks_completed']}")
        app.analyze_results.append(f"Tasks failed: {results['tasks_failed']}")
        app.analyze_results.append(f"Total processing time: {results['total_processing_time']:.2f} seconds")

        # Display task-specific results
        for task_type, task_results in results.get('task_results', {}).items():
            app.analyze_results.append(f"\n{task_type.capitalize()} Results:")
            app.analyze_results.append(f"Total: {len(task_results)}")

            # Display pattern matches
            if task_type == 'find_patterns':
                total_patterns = sum(result.get('patterns_found', 0) for result in task_results if result.get('success', False))
                app.analyze_results.append(f"Total patterns found: {total_patterns}")

                # Display some matches
                matches_displayed = 0
                for result in task_results:
                    if result.get('success', False) and 'matches' in result:
                        for match in result['matches'][:5]:  # Display up to 5 matches per result
                            pattern_str = match.get('pattern', '')
                            if isinstance(pattern_str, bytes):
                                pattern_str = pattern_str.decode('utf-8', errors='replace')
                            app.analyze_results.append(f"  0x{match.get('position', 0):x}: {pattern_str}")
                            matches_displayed += 1
                            if matches_displayed >= 20:  # Display at most 20 matches total
                                break
                        if matches_displayed >= 20:
                            break

            # Display entropy analysis results
            elif task_type == 'analyze_entropy':
                high_entropy_regions = []
                chunk_entropies = []

                for result in task_results:
                    if result.get('success', False):
                        if 'high_entropy_regions' in result:
                            high_entropy_regions.extend(result['high_entropy_regions'])
                        if 'chunk_entropy' in result and 'chunk_size' in result:
                            chunk_entropies.append((result['chunk_entropy'], result['chunk_size']))

                # Calculate overall entropy (weighted by chunk size)
                total_size = sum(size for _, size in chunk_entropies)
                overall_entropy = sum(entropy * size for entropy, size in chunk_entropies) / total_size if total_size > 0 else 0

                app.analyze_results.append(f"Overall entropy: {overall_entropy:.2f}")
                app.analyze_results.append(f"High entropy regions: {len(high_entropy_regions)}")

                # Show some high entropy regions
                app.analyze_results.append("\nSelected high entropy regions:")
                for region in sorted(high_entropy_regions, key=lambda x: x.get('entropy', 0), reverse=True)[:10]:
                    app.analyze_results.append(f"  0x{region.get('offset', 0):x} - Size: {region.get('size', 0)} - Entropy: {region.get('entropy', 0):.2f}")

            # Display symbolic execution results
            elif task_type == 'symbolic_execution':
                total_paths = sum(result.get('paths_explored', 0) for result in task_results if result.get('success', False))
                total_vulns = sum(result.get('vulnerabilities_found', 0) for result in task_results if result.get('success', False))
                app.analyze_results.append(f"Total paths explored: {total_paths}")
                app.analyze_results.append(f"Total vulnerabilities found: {total_vulns}")

    else:
        # Results from convenience methods

        # Process pattern search results
        pattern_results = results.get('pattern_search', [])
        if pattern_results:
            app.analyze_results.append(f"\nPattern Search Results:")
            app.analyze_results.append(f"Total patterns found: {len(pattern_results)}")

            # Group by pattern
            pattern_groups = {}
            for match in pattern_results:
                pattern = match.get('pattern', '')
                if isinstance(pattern, bytes):
                    pattern = pattern.decode('utf-8', errors='replace')
                if pattern not in pattern_groups:
                    pattern_groups[pattern] = []
                pattern_groups[pattern].append(match)

            # Display summary by pattern
            app.analyze_results.append("\nMatches by pattern:")
            for pattern, matches in pattern_groups.items():
                app.analyze_results.append(f"  {pattern}: {len(matches)} matches")

            # Display some sample matches
            app.analyze_results.append("\nSample matches:")
            for match in sorted(pattern_results, key=lambda x: x.get('position', 0))[:20]:
                pattern = match.get('pattern', '')
                if isinstance(pattern, bytes):
                    pattern = pattern.decode('utf-8', errors='replace')
                match_content = match.get('match', '')
                if isinstance(match_content, bytes):
                    match_content = match_content.decode('utf-8', errors='replace')
                app.analyze_results.append(f"  0x{match.get('position', 0):x}: {pattern} - {match_content}")

        # Process entropy analysis results
        entropy_results = results.get('entropy_analysis', {})
        if entropy_results:
            overall_entropy = entropy_results.get('overall_entropy', 0)
            windows = entropy_results.get('windows', [])
            high_entropy_regions = entropy_results.get('high_entropy_regions', [])

            app.analyze_results.append(f"\nEntropy Analysis Results:")
            app.analyze_results.append(f"Overall entropy: {overall_entropy:.2f}")
            app.analyze_results.append(f"Windows analyzed: {len(windows)}")
            app.analyze_results.append(f"High entropy regions: {len(high_entropy_regions)}")

            # Display some high entropy regions
            if high_entropy_regions:
                app.analyze_results.append("\nTop high entropy regions:")
                for region in sorted(high_entropy_regions, key=lambda x: x.get('entropy', 0), reverse=True)[:10]:
                    app.analyze_results.append(f"  0x{region.get('offset', 0):x} - Size: {region.get('size', 0)} - Entropy: {region.get('entropy', 0):.2f}")

    # Ask if user wants to generate a report
    generate_report = QMessageBox.question(
        app,
        "Generate Report",
        "Do you want to generate a report of the distributed processing results?",
        QMessageBox.Yes | QMessageBox.No
    ) == QMessageBox.Yes

    if generate_report:
        # Ask for report filename
        filename, _ = QFileDialog.getSaveFileName(
            app,
            "Save Report",
            "",
            "HTML Files (*.html);;All Files (*)"
        )

        if filename:
            if not filename.endswith('.html'):
                filename += '.html'

            report_path = manager.generate_report(filename)
            if report_path:
                app.update_output.emit(log_message(f"[Distributed Processing] Report saved to {report_path}"))

                # Try to open the report in the default browser
                try:
                    webbrowser.open(f"file://{os.path.abspath(report_path)}")
                except:
                    pass  # Ignore errors opening browser
            else:
                app.update_output.emit(log_message("[Distributed Processing] Failed to generate report"))


class DistributedProcessingConfigDialog(QDialog):
    """Configuration dialog for distributed processing"""

    def __init__(self, binary_path, parent=None):
        """
        Initialize the distributed processing configuration dialog.

        Args:
            binary_path: Path to the binary for distributed processing.
            parent: Optional parent widget.
        """
        super().__init__(parent)
        self.binary_path = binary_path
        self.setWindowTitle("Distributed Processing Configuration")
        self.setup_ui()

    def setup_ui(self):
        """Set up the dialog UI"""
        layout = QVBoxLayout()

        # Processing options
        processing_group = QGroupBox("Processing Options")
        processing_layout = QFormLayout()

        # Workers
        self.workers_spin = QSpinBox()
        self.workers_spin.setRange(1, 32)
        self.workers_spin.setValue(multiprocessing.cpu_count())
        self.workers_spin.setToolTip("Number of worker processes to use")
        processing_layout.addRow("Workers:", self.workers_spin)

        # Chunk size
        self.chunk_size_spin = QSpinBox()
        self.chunk_size_spin.setRange(1, 100)
        self.chunk_size_spin.setValue(1)
        self.chunk_size_spin.setSuffix(" MB")
        self.chunk_size_spin.setToolTip("Size of chunks for processing")
        processing_layout.addRow("Chunk size:", self.chunk_size_spin)

        # Window size for entropy analysis
        self.window_size_spin = QSpinBox()
        self.window_size_spin.setRange(1, 1024)
        self.window_size_spin.setValue(64)
        self.window_size_spin.setSuffix(" KB")
        self.window_size_spin.setToolTip("Size of sliding window for entropy analysis")
        processing_layout.addRow("Window size:", self.window_size_spin)

        # Timeout
        self.timeout_spin = QSpinBox()
        self.timeout_spin.setRange(10, 3600)
        self.timeout_spin.setValue(60)
        self.timeout_spin.setSuffix(" seconds")
        self.timeout_spin.setToolTip("Timeout for processing")
        processing_layout.addRow("Timeout:", self.timeout_spin)

        # Backend selection
        self.backend_combo = QComboBox()
        self.backend_combo.addItem("Auto (select best available)")
        self.backend_combo.addItem("Ray")
        self.backend_combo.addItem("Dask")
        self.backend_combo.addItem("Multiprocessing")
        self.backend_combo.setToolTip("Processing backend to use")
        processing_layout.addRow("Backend:", self.backend_combo)

        # Convenience methods
        self.convenience_check = QCheckBox("Use convenience methods")
        self.convenience_check.setChecked(True)
        self.convenience_check.setToolTip("Use built-in convenience methods instead of task queue for common operations")
        processing_layout.addRow("", self.convenience_check)

        processing_group.setLayout(processing_layout)
        layout.addWidget(processing_group)

        # Analysis options
        analysis_group = QGroupBox("Analysis Options")
        analysis_layout = QVBoxLayout()

        # Section analysis
        self.section_check = QCheckBox("Analyze sections")
        self.section_check.setChecked(True)
        self.section_check.setToolTip("Analyze binary sections")
        analysis_layout.addWidget(self.section_check)

        # Pattern search
        self.pattern_check = QCheckBox("Search for patterns")
        self.pattern_check.setChecked(True)
        self.pattern_check.setToolTip("Search for patterns in the binary")
        analysis_layout.addWidget(self.pattern_check)

        # Entropy analysis
        self.entropy_check = QCheckBox("Analyze entropy")
        self.entropy_check.setChecked(True)
        self.entropy_check.setToolTip("Analyze entropy distribution")
        analysis_layout.addWidget(self.entropy_check)

        # Symbolic execution
        self.symbolic_check = QCheckBox("Run symbolic execution (experimental)")
        self.symbolic_check.setChecked(False)
        self.symbolic_check.setToolTip("Run symbolic execution on selected functions")
        analysis_layout.addWidget(self.symbolic_check)

        # Pattern types
        pattern_group = QGroupBox("Pattern Types")
        pattern_layout = QVBoxLayout()

        self.license_check = QCheckBox("License/registration patterns")
        self.license_check.setChecked(True)
        pattern_layout.addWidget(self.license_check)

        self.hardware_check = QCheckBox("Hardware ID patterns")
        self.hardware_check.setChecked(True)
        pattern_layout.addWidget(self.hardware_check)

        self.crypto_check = QCheckBox("Cryptography patterns")
        self.crypto_check.setChecked(True)
        pattern_layout.addWidget(self.crypto_check)

        self.custom_patterns_edit = QLineEdit()
        self.custom_patterns_edit.setPlaceholder("Custom patterns (comma-separated)")
        self.custom_patterns_edit.setToolTip("Enter custom patterns to search for, separated by commas")
        pattern_layout.addWidget(self.custom_patterns_edit)

        pattern_group.setLayout(pattern_layout)
        analysis_layout.addWidget(pattern_group)

        analysis_group.setLayout(analysis_layout)
        layout.addWidget(analysis_group)

        # Buttons
        buttons = QDialogButtonBox(QDialogButtonBox.Ok | QDialogButtonBox.Cancel)
        buttons.accepted.connect(self.accept)
        buttons.rejected.connect(self.reject)
        layout.addWidget(buttons)

        self.setLayout(layout)

    def get_config(self):
        """Get the configuration from the dialog"""
        # Parse any custom patterns
        custom_patterns = []
        if self.custom_patterns_edit.text().strip():
            custom_patterns = [p.strip() for p in self.custom_patterns_edit.text().split(',')]

        # Map backend selection to value
        backend_map = {
            0: "auto",
            1: "ray",
            2: "dask",
            3: "multiprocessing"
        }
        preferred_backend = backend_map.get(self.backend_combo.currentIndex(), "auto")

        return {
            # Processing options
            'num_workers': self.workers_spin.value(),
            'chunk_size': self.chunk_size_spin.value() * 1024 * 1024,  # Convert MB to bytes
            'window_size_kb': self.window_size_spin.value(),
            'timeout': self.timeout_spin.value(),
            'preferred_backend': preferred_backend,
            'use_convenience_methods': self.convenience_check.isChecked(),

            # Analysis options
            'run_section_analysis': self.section_check.isChecked(),
            'run_pattern_search': self.pattern_check.isChecked(),
            'run_entropy_analysis': self.entropy_check.isChecked(),
            'run_symbolic_execution': self.symbolic_check.isChecked(),

            # Pattern types
            'search_license_patterns': self.license_check.isChecked(),
            'search_hardware_patterns': self.hardware_check.isChecked(),
            'search_crypto_patterns': self.crypto_check.isChecked(),
            'custom_patterns': custom_patterns,
        }

        self.update_output.emit(log_message("[Distributed] Running distributed entropy analysis..."))

        # Ask for window size
        window_size, ok = QInputDialog.getInt(self, "Window Size", "Enter sliding window size (KB):", 64, 1, 1024)
        if not ok:
            self.update_output.emit(log_message("[Distributed] Cancelled"))
            return

        # Run distributed entropy analysis
        start_time = time.time()
        # Get instance of distributed manager
        distributed_manager = DistributedProcessingManager()
        chunk_size = 1024 * 1024  # 1MB chunks
        results = distributed_manager.run_distributed_entropy_analysis(self.binary_path, chunk_size, window_size)
        end_time = time.time()

        # Display results
        self.update_output.emit(log_message(f"[Distributed] Overall entropy: {results['overall_entropy']:.6f}"))
        self.update_output.emit(log_message(f"[Distributed] Found {len(results['high_entropy_regions'])} high entropy regions"))
        self.update_output.emit(log_message(f"[Distributed] Analysis completed in {end_time - start_time:.2f} seconds"))

        # Add to analyze results
        if not hasattr(self, "analyze_results"):
            self.analyze_results = []

        self.analyze_results.append("\n=== DISTRIBUTED ENTROPY ANALYSIS RESULTS ===")
        self.analyze_results.append(f"Analysis time: {end_time - start_time:.2f} seconds")
        self.analyze_results.append(f"Overall entropy: {results['overall_entropy']:.6f}")
        self.analyze_results.append(f"High entropy regions: {len(results['high_entropy_regions'])}")
        self.analyze_results.append(f"Workers used: {distributed_manager.num_workers}")

        if results['high_entropy_regions']:
            self.analyze_results.append("\nTop high entropy regions:")
            for i, region in enumerate(results['high_entropy_regions'][:5]):  # Show up to 5 regions
                self.analyze_results.append(f"{i+1}. Offset: 0x{region['offset']:X}, Size: {region['size']} bytes, Entropy: {region['entropy']:.6f}")

        # Interpret overall entropy
        if results['overall_entropy'] > 7.5:
            self.analyze_results.append("\nVery high overall entropy (>7.5): Likely encrypted or compressed data")
        elif results['overall_entropy'] > 6.5:
            self.analyze_results.append("\nHigh overall entropy (6.5-7.5): Possibly packed or obfuscated code")
        elif results['overall_entropy'] > 5.5:
            self.analyze_results.append("\nModerate overall entropy (5.5-6.5): Typical for compiled code")
        else:
            self.analyze_results.append("\nLow overall entropy (<5.5): Possibly plain text or uncompressed data")

    # -------------------------------
    # GPU Acceleration System
    # -------------------------------

class GPUAccelerator:
    """
    GPU acceleration system for computationally intensive analysis tasks.

    This system leverages GPU computing capabilities to accelerate specific
    analysis tasks such as pattern matching, entropy calculation, and
    cryptographic operations.
    """

    def __init__(self):
        """
        Initialize the GPU accelerator.
        """
        self.logger = logging.getLogger(__name__)
        self.cuda_available = False
        self.opencl_available = False
        self.opencl_memory_registry = {
            'total_allocated': 0,
            'peak_usage': 0,
            'buffers': {}
        }
        self.tensorflow_available = False
        self.pytorch_available = False

        # New attributes for GPU agnostic operation
        self.selected_backend = None
        self.opencl_context = None
        self.opencl_queue = None
        self.opencl_devices = []

        # Multi-GPU support
        self.cuda_devices = []
        self.tensorflow_devices = []
        self.pytorch_devices = []

        # Benchmarking and workload characteristics
        self.backend_benchmarks = {}
        self.workload_characteristics = {
            'pattern_matching': {'compute_intensity': 'medium', 'memory_usage': 'high'},
            'entropy_calculation': {'compute_intensity': 'low', 'memory_usage': 'medium'},
            'hash_calculation': {'compute_intensity': 'high', 'memory_usage': 'low'}
        }

        # Error handling and recovery
        self.error_counts = {'cuda': 0, 'opencl': 0, 'tensorflow': 0, 'pytorch': 0}
        self.max_errors_before_blacklist = 3
        self.blacklisted_backends = set()

        # Check for GPU acceleration libraries
        self._check_available_backends()

        # Select the preferred backend
        self._select_preferred_backend()

        # Run initial benchmarks
        self._run_initial_benchmarks()

    def _check_available_backends(self):
        """
        Check which GPU acceleration backends are available.
        """
        # Check for CUDA
        try:
            if 'cupy' in sys.modules:
                self.cuda_available = True
                self.logger.info("CUDA acceleration available via CuPy")

                # Get available CUDA devices
                try:
                    num_devices = cp.cuda.runtime.getDeviceCount()
                    for i in range(num_devices):
                        device_props = cp.cuda.runtime.getDeviceProperties(i)
                        self.cuda_devices.append({
                            'index': i,
                            'name': device_props['name'].decode('utf-8'),
                            'memory': device_props['totalGlobalMem'],
                            'compute_capability': f"{device_props['major']}.{device_props['minor']}",
                            'multiprocessors': device_props['multiProcessorCount'],
                            'max_threads_per_block': device_props['maxThreadsPerBlock']
                        })
                    self.logger.info(f"Found {len(self.cuda_devices)} CUDA devices")
                except Exception as e:
                    self.logger.warning(f"Error getting CUDA device info: {e}")
            else:
                if cuda is not None and 'pycuda.autoinit' in sys.modules:
                    self.cuda_available = True
                    self.logger.info("CUDA acceleration available via PyCUDA")

                    # Get available CUDA devices for PyCUDA
                    try:
                        drv.init()

                        # Create memory allocation history for tracking
                        if not hasattr(self, 'memory_tracking'):
                            self.memory_tracking = {
                                'history': [],
                                'allocations': {},
                                'peak_usage': 0
                            }

                        for i in range(drv.Device.count()):
                            device = drv.Device(i)

                            # Get device memory information
                            total_memory = device.total_memory()
                            free_memory, _ = drv.mem_get_info()
                            allocated_memory = total_memory - free_memory
                            memory_utilization = (allocated_memory / total_memory) * 100

                            # Use allocated_memory to make decisions about memory management
                            if memory_utilization > 80:
                                # Memory is heavily used - implement aggressive memory management
                                self.logger.warning(f"CUDA device {i} memory utilization high ({memory_utilization:.1f}%)")
                                self.force_memory_cleanup()
                            elif memory_utilization > 50:
                                # Moderate memory usage - implement standard cleanup
                                self.logger.info(f"CUDA device {i} memory utilization moderate ({memory_utilization:.1f}%)")
                                # Schedule cleanup after current operation
                                self.schedule_cleanup = True

                            # Record tracking data
                            timestamp = time.time()
                            self.memory_tracking['history'].append({
                                'device_id': i,
                                'timestamp': timestamp,
                                'allocated_memory': allocated_memory,
                                'total_memory': total_memory,
                                'utilization': memory_utilization
                            })

                            # Update peak usage
                            if allocated_memory > self.memory_tracking['peak_usage']:
                                self.memory_tracking['peak_usage'] = allocated_memory
                                self.logger.info(f"New peak memory usage: {allocated_memory/1024/1024:.1f} MB")

                            # Record device details with memory utilization statistics
                            self.cuda_devices.append({
                                'index': i,
                                'name': device.name(),
                                'memory': total_memory,
                                'free_memory': free_memory,
                                'allocated_memory': allocated_memory,
                                'memory_utilization': memory_utilization,
                                'compute_capability': f"{device.compute_capability()[0]}.{device.compute_capability()[1]}",
                                'multiprocessors': device.get_attribute(drv.device_attribute.MULTIPROCESSOR_COUNT)
                            })

                            # Log memory status for each device
                            self.logger.info(f"CUDA Device {i} ({device.name()}): {total_memory/1024/1024:.1f}MB total, "
                                             f"{allocated_memory/1024/1024:.1f}MB used ({memory_utilization:.1f}%)")

                        self.logger.info(f"Found {len(self.cuda_devices)} CUDA devices (PyCUDA)")

                        # Setup memory monitoring thread if not already running
                        if not hasattr(self, 'memory_monitor_running') or not self.memory_monitor_running:
