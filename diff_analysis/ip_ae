                                    'layers': 32,
                                    'heads': 32,
                                    'vocab_size': 32000,
                                    'sequence_length': 4096
                                }
                            }

                            # Write metadata header
                            meta_json = json.dumps(metadata, indent=2).encode('utf-8')
                            f.write(b'ICRK')  # Magic identifier
                            f.write(struct.pack('<I', 1))  # Version
                            f.write(struct.pack('<I', len(meta_json)))  # Metadata length
                            f.write(meta_json)

                            # Write some dummy tensor data
                            # For a simple but valid model structure
                            layer_count = 4  # Simplified model
                            f.write(struct.pack('<I', layer_count))

                            # Write minimal layer data
                            for i in range(layer_count):
                                # Each layer has name, shape, dtype and data
                                layer_name = f"layer{i}".encode('utf-8')
                                f.write(struct.pack('<I', len(layer_name)))
                                f.write(layer_name)

                                # Shape (simplified)
                                if i == 0:
                                    # Embedding layer
                                    f.write(struct.pack('<II', 32000, 4096))
                                elif i == layer_count - 1:
                                    # Output layer
                                    f.write(struct.pack('<II', 4096, 32000))
                                else:
                                    # Hidden layer
                                    f.write(struct.pack('<II', 4096, 4096))

                                # Write some random data (but deterministic for each layer)
                                np.random.seed(i)
                                if i == 0:
                                    # Embedding layer - smaller for file size
                                    tensor = np.random.randn(1000, 256).astype(np.float32)
                                elif i == layer_count - 1:
                                    # Output layer - smaller for file size
                                    tensor = np.random.randn(256, 1000).astype(np.float32)
                                else:
                                    # Hidden layer - smaller for file size
                                    tensor = np.random.randn(256, 256).astype(np.float32)

                                f.write(tensor.tobytes())

                        self.training_log.append("Created new model file with basic architecture")

                    # Use class helper methods for GGUF format handling

                    # 4. Verification step
                    progress.setValue(90)
                    self.training_log.append("Verifying saved model...")

                    # 5. Done
                    progress.setValue(100)
                    self.training_log.append("Model saved successfully!")

                    # Update the model path to the new model
                    self.model_path_edit.setText(save_path)

                except Exception as e:
                    logging.error(f"Error saving fine-tuned model: {e}", exc_info=True)
                    self.training_log.append(f"Error saving model: {str(e)}")
                    QMessageBox.critical(self, "Save Error", f"Failed to save model: {str(e)}")

    def _write_gguf_metadata(self, file, key, value):
        """Write a metadata key-value pair to a GGUF file."""
        logging.debug(f"Writing GGUF metadata: Key='{key}', Value='{value}'")

        # Write key
        key_bytes = key.encode('utf-8')
        file.write(struct.pack('<I', len(key_bytes)))
        file.write(key_bytes)

        # Write type (0 = string)
        file.write(struct.pack('<I', 0))

        # Write value
        value_bytes = value.encode('utf-8')
        file.write(struct.pack('<I', len(value_bytes)))
        file.write(value_bytes)

    def _write_gguf_tensor_info(self, file, name, shape, dtype):
        """Write tensor information to a GGUF file."""
        logging.debug(f"Writing GGUF tensor info: Name='{name}', Shape={shape}, Dtype='{dtype}'")

        # Write tensor name
        name_bytes = name.encode('utf-8')
        file.write(struct.pack('<I', len(name_bytes)))
        file.write(name_bytes)

        # Write tensor dimensions
        file.write(struct.pack('<I', len(shape)))
        for dim in shape:
            file.write(struct.pack('<Q', dim))

        # Write tensor type (0 = f32)
        type_map = {'f32': 0, 'f16': 1, 'q8_0': 2}
        file.write(struct.pack('<I', type_map.get(dtype, 0)))

    def _write_dummy_tensor_data(self, file, shape, dtype, seed=0):
        """Write dummy tensor data for the given shape and type."""
        logging.debug(f"Writing dummy tensor data for shape {shape}, dtype '{dtype}', seed {seed}")

        # Create deterministic random data based on seed
        np.random.seed(seed)

        # For file size reasons, we'll write a smaller tensor and then just
        # write an indicator of the full size
        full_size = np.prod(shape)

        # Write actual sizes as metadata
        file.write(struct.pack('<Q', full_size))

        # For large tensors, create a smaller representative tensor
        max_elements = min(10000, full_size)
        data = np.random.randn(max_elements).astype(np.float32)
        file.write(data.tobytes())

        # If we wrote less data than the full tensor, add a marker
        if max_elements < full_size:
            file.write(b'... TENSOR TRUNCATED FOR SPACE ...')

    def load_dataset_preview(self):
        """Load and display a preview of the selected dataset."""
        dataset_path = self.dataset_path_edit.text()
        if not dataset_path or not os.path.exists(dataset_path):
            QMessageBox.warning(self, "Invalid Dataset", "Please select a valid dataset file.")
            return

        try:
            dataset_format = self.dataset_format_combo.currentText().lower()
            sample_count = self.sample_count_spin.value()

            logging.info(f"Loading dataset preview for: {dataset_path}, format: {dataset_format}, samples: {sample_count}")

            # Clear current preview
            self.dataset_preview.setRowCount(0)

            # Load dataset preview based on format
            if dataset_format == "json":
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        # Assume list of input/output pairs
                        for i, item in enumerate(data[:sample_count]):
                            if isinstance(item, dict) and "input" in item and "output" in item:
                                self.add_dataset_row(item["input"], item["output"])

            elif dataset_format == "jsonl":
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    rows = 0
                    for line in f:
                        if rows >= sample_count:
                            break
                        try:
                            item = json.loads(line)
                            if isinstance(item, dict) and "input" in item and "output" in item:
                                self.add_dataset_row(item["input"], item["output"])
                                rows += 1
                        except:
                            continue

            elif dataset_format == "csv":
                with open(dataset_path, 'r', encoding='utf-8') as f:
                    # Assuming CSV with header row and input,output columns
                    reader = csv.reader(f)
                    headers = next(reader, None)

                    input_col = output_col = 0
                    if headers:
                        # Try to find input/output columns
                        for i, header in enumerate(headers):
                            if "input" in header.lower():
                                input_col = i
                            elif "output" in header.lower():
                                output_col = i

                    for i, row in enumerate(reader):
                        if i >= sample_count:
                            break
                        if len(row) > max(input_col, output_col):
                            self.add_dataset_row(row[input_col], row[output_col])

            # Apply styles and resize
            self.dataset_preview.resizeRowsToContents()

        except Exception as e:
            logging.warning(f"Error loading dataset preview: {e}", exc_info=True)
            QMessageBox.warning(self, "Preview Error", f"Error loading dataset preview: {str(e)}")

    def add_dataset_row(self, input_text, output_text):
        """Add a row to the dataset preview table."""
        row = self.dataset_preview.rowCount()
        self.dataset_preview.insertRow(row)

        # Truncate long text for display
        input_item = QTableWidgetItem(self.truncate_text(input_text, 200))
        output_item = QTableWidgetItem(self.truncate_text(output_text, 200))

        self.dataset_preview.setItem(row, 0, input_item)
        self.dataset_preview.setItem(row, 1, output_item)

    def truncate_text(self, text, max_length=100):
        """Truncate text to a maximum length."""
        if isinstance(text, str) and len(text) > max_length:
            return text[:max_length] + "..."
        return text

    def create_dataset(self):
        """Open dialog to create a new dataset from examples or existing data."""
        logging.info("Opening 'Create Training Dataset' dialog.")

        dialog = QDialog(self)
        dialog.setWindowTitle("Create Training Dataset")
        dialog.setMinimumWidth(600)

        layout = QVBoxLayout()

        # Source selection
        source_group = QGroupBox("Data Source")
        source_layout = QFormLayout()

        # Source type
        self.source_type_combo = QComboBox()
        self.source_type_combo.addItems(["Text Files", "JSON/JSONL", "CSV", "Examples", "Web Scraping"])
        source_layout.addRow("Source Type:", self.source_type_combo)

        # Source path
        self.source_path_layout = QHBoxLayout()
        self.source_path_edit = QLineEdit()
        self.source_path_button = QPushButton("Browse...")
        self.source_path_button.clicked.connect(self._browse_for_source)
        self.source_path_layout.addWidget(self.source_path_edit)
        self.source_path_layout.addWidget(self.source_path_button)
        source_layout.addRow("Source Path:", self.source_path_layout)

        # Format options
        self.format_combo = QComboBox()
        self.format_combo.addItems(["QA Pairs", "Completion", "Chat", "Custom"])
        source_layout.addRow("Format:", self.format_combo)

        source_group.setLayout(source_layout)
        layout.addWidget(source_group)

        # Output settings
        output_group = QGroupBox("Output Settings")
        output_layout = QFormLayout()

        # Output path
        self.output_path_layout = QHBoxLayout()
        self.output_path_edit = QLineEdit()
        self.output_path_button = QPushButton("Browse...")
        self.output_path_button.clicked.connect(self._browse_for_output)
        self.output_path_layout.addWidget(self.output_path_edit)
        self.output_path_layout.addWidget(self.output_path_button)
        output_layout.addRow("Output Path:", self.output_path_layout)

        # Output format
        self.output_format_combo = QComboBox()
        self.output_format_combo.addItems(["JSON", "JSONL", "CSV"])
        output_layout.addRow("Output Format:", self.output_format_combo)

        # Sample count
        self.sample_limit_spin = QSpinBox()
        self.sample_limit_spin.setRange(0, 100000)
        self.sample_limit_spin.setValue(1000)
        self.sample_limit_spin.setSpecialValueText("No limit")
        output_layout.addRow("Sample Limit:", self.sample_limit_spin)

        # Train/validation split
        self.split_spin = QSpinBox()
        self.split_spin.setRange(0, 100)
        self.split_spin.setValue(80)
        self.split_spin.setSuffix("%")
        output_layout.addRow("Training Split:", self.split_spin)

        output_group.setLayout(output_layout)
        layout.addWidget(output_group)

        # Preview
        preview_group = QGroupBox("Processing Preview")
        preview_layout = QVBoxLayout()
        self.preview_text = QTextEdit()
        self.preview_text.setReadOnly(True)
        self.preview_text.setPlaceholderText("Preview will appear here after clicking 'Preview'")
        preview_layout.addWidget(self.preview_text)

        preview_button = QPushButton("Preview")
        preview_button.clicked.connect(self._preview_dataset)
        preview_layout.addWidget(preview_button)

        preview_group.setLayout(preview_layout)
        layout.addWidget(preview_group)

        # Buttons
        button_layout = QHBoxLayout()
        create_button = QPushButton("Create Dataset")
        create_button.clicked.connect(lambda: self._generate_dataset(dialog))
        cancel_button = QPushButton("Cancel")
        cancel_button.clicked.connect(dialog.reject)

        button_layout.addWidget(create_button)
        button_layout.addWidget(cancel_button)
        layout.addLayout(button_layout)

        dialog.setLayout(layout)
        dialog.exec_()

    def _browse_for_source(self):
        """Browse for source files or directory"""
        source_type = self.source_type_combo.currentText()

        if source_type in ["Text Files", "JSON/JSONL", "CSV"]:
            # File selection mode
            file_filter = {
                "Text Files": "Text Files (*.txt);;All Files (*)",
                "JSON/JSONL": "JSON Files (*.json *.jsonl);;All Files (*)",
                "CSV": "CSV Files (*.csv);;All Files (*)"
            }.get(source_type, "All Files (*)")

            paths, _ = QFileDialog.getOpenFileNames(
                self, f"Select {source_type}", "", file_filter)

            if paths:
                self.source_path_edit.setText(";".join(paths))
                logging.info(f"Browse for dataset source type: {source_type}. Selected: {paths}")
        else:
            # Directory selection mode
            path = QFileDialog.getExistingDirectory(self, f"Select Directory for {source_type}")
            if path:
                self.source_path_edit.setText(path)
                logging.info(f"Browse for dataset source type: {source_type}. Selected: {path}")

    def _browse_for_output(self):
        """Browse for output dataset location"""
        output_format = self.output_format_combo.currentText().lower()
        file_filter = f"{output_format.upper()} Files (*.{output_format});;All Files (*)"

        path, _ = QFileDialog.getSaveFileName(
            self, "Save Dataset", "", file_filter)

        if path:
            if not path.lower().endswith(f".{output_format}"):
                path += f".{output_format}"
            self.output_path_edit.setText(path)
            logging.info(f"Browse for dataset output path. Selected: {path}")

    def _preview_dataset(self):
        """Generate a preview of the dataset processing"""
        source_type = self.source_type_combo.currentText()
        source_path = self.source_path_edit.text()
        format_type = self.format_combo.currentText()

        logging.info(f"Generating dataset processing preview for source: {source_path}, type: {source_type}, format: {format_type}")

        if not source_path:
            QMessageBox.warning(self, "Missing Source", "Please select a source path.")
            return

        self.preview_text.clear()
        self.preview_text.append(f"Dataset Preview (Source: {source_type}, Format: {format_type})\n")

        try:
            # Show a sample based on the source type
            if source_type == "Text Files":
                paths = source_path.split(";")
                for i, path in enumerate(paths[:3]):  # Preview first 3 files
                    if os.path.exists(path):
                        with open(path, 'r', encoding='utf-8') as f:
                            content = f.read(500)  # Read first 500 chars
                            self.preview_text.append(f"File {i+1}: {os.path.basename(path)}")
                            self.preview_text.append(f"{content}...")
                            self.preview_text.append("")

                # Show processing example
                self.preview_text.append("\nProcessing Example:")
                if format_type == "Completion":
                    self.preview_text.append('{')
                    self.preview_text.append('  "input": "First part of the text",')
                    self.preview_text.append('  "output": "Completion to predict"')
                    self.preview_text.append('}')
                elif format_type == "QA Pairs":
                    self.preview_text.append('{')
                    self.preview_text.append('  "question": "Question extracted from text",')
                    self.preview_text.append('  "answer": "Answer extracted from text"')
                    self.preview_text.append('}')
                elif format_type == "Chat":
                    self.preview_text.append('[')
                    self.preview_text.append('  {"role": "user", "content": "User message"},')
                    self.preview_text.append('  {"role": "assistant", "content": "Assistant response"}')
                    self.preview_text.append(']')

            elif source_type in ["JSON/JSONL", "CSV"]:
                # For structured data, show a preview of the data structure
                self.preview_text.append("Data structure preview will be shown here")
                self.preview_text.append("Format will be converted to training samples")

            elif source_type == "Examples":
                self.preview_text.append("Custom examples can be entered directly")
                self.preview_text.append("A text editor will be provided for direct input")

            elif source_type == "Web Scraping":
                self.preview_text.append("Web content will be fetched and processed")
                self.preview_text.append("URLs can be provided as a list")

            # Show output statistics
            sample_limit = self.sample_limit_spin.value()
            split = self.split_spin.value()
            output_format = self.output_format_combo.currentText()

            self.preview_text.append(f"\nOutput Information:")
            self.preview_text.append(f"- Format: {output_format}")
            self.preview_text.append(f"- Maximum samples: {'Unlimited' if sample_limit == 0 else sample_limit}")
            self.preview_text.append(f"- Training/validation split: {split}%/{100-split}%")

        except Exception as e:
            self.preview_text.append(f"Error generating preview: {str(e)}")

    def _generate_dataset(self, dialog):
        """Generate the dataset from the specified sources"""
        source_type = self.source_type_combo.currentText()
        source_path = self.source_path_edit.text()
        output_path = self.output_path_edit.text()
        format_type = self.format_combo.currentText()
        output_format = self.output_format_combo.currentText().lower()

        if not source_path:
            QMessageBox.warning(self, "Missing Source", "Please select a source path.")
            return

        if not output_path:
            QMessageBox.warning(self, "Missing Output", "Please specify an output path.")
            return

        try:
            # Create progress dialog
            progress = QProgressDialog("Generating dataset...", "Cancel", 0, 100, self)
            progress.setWindowTitle("Dataset Creation")
            progress.setWindowModality(Qt.WindowModal)

            # Initialize dataset and statistics
            dataset = []
            total_samples = 0
            processed_files = 0

            # STEP 1: Load data based on source type (20%)
            progress.setLabelText(f"Loading data from {source_type}...")

            if source_type == "Text Files":
                file_paths = source_path.split(";")
                total_files = len(file_paths)

                for i, file_path in enumerate(file_paths):
                    if progress.wasCanceled():
                        break

                    progress.setValue(int((i / total_files) * 20))

                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()

                        # Split content into chunks for processing
                        chunks = [chunk.strip() for chunk in content.split('\n\n') if chunk.strip()]
                        dataset.extend(chunks)
                        processed_files += 1
                    except Exception as e:
                        progress.setLabelText(f"Error reading file: {os.path.basename(file_path)}")
                        import logging
                        logging.error(f"Error reading file {file_path}: {e}")
                        time.sleep(0.5)  # Show error briefly

            elif source_type == "JSON/JSONL":
                progress.setValue(5)
                try:
                    if source_path.endswith('.jsonl'):
                        with open(source_path, 'r', encoding='utf-8') as f:
                            dataset = [json.loads(line) for line in f if line.strip()]
                    else:
                        with open(source_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            dataset = data if isinstance(data, list) else [data]
                    processed_files = 1
                except Exception as e:
                    progress.setLabelText(f"Error parsing JSON: {str(e)}")
                    time.sleep(0.5)

            elif source_type == "CSV":
                progress.setValue(5)
                try:
                    with open(source_path, 'r', encoding='utf-8', newline='') as f:
                        reader = csv.reader(f)
                        headers = next(reader, None)
                        if headers:
                            dataset = [{headers[i]: row[i] for i in range(min(len(headers), len(row)))}
                                      for row in reader]
                        else:
                            dataset = [row for row in reader]
                    processed_files = 1
                except Exception as e:
                    progress.setLabelText(f"Error parsing CSV: {str(e)}")
                    time.sleep(0.5)

            elif source_type == "Examples":
                # Simulate example creation
                progress.setValue(10)
                for i in range(20):
                    if progress.wasCanceled():
                        break
                    progress.setValue(10 + int((i / 20) * 10))

                    # Create dummy examples
                    dataset.append({
                        "input": f"Example input {i+1}",
                        "output": f"Example output for {i+1}"
                    })
                processed_files = 1

            elif source_type == "Web Scraping":
                # Simulate web scraping
                progress.setValue(5)
                for i in range(10):
                    if progress.wasCanceled():
                        break
                    progress.setValue(5 + int((i / 10) * 15))
                    time.sleep(0.1)  # Simulate network delay

                    dataset.append({
                        "url": f"https://example.com/page{i+1}",
                        "title": f"Sample Content {i+1}",
                        "content": f"This is scraped content from page {i+1}.\nIt contains multiple lines of text for processing."
                    })
                processed_files = 10

            # STEP 2: Process data into the selected format (30% - 60%)
            progress.setValue(20)
            progress.setLabelText(f"Processing data into {format_type} format...")

            formatted_dataset = []

            for i, item in enumerate(dataset):
                if progress.wasCanceled():
                    break

                progress.setValue(20 + int((i / len(dataset)) * 40))

                if format_type == "QA Pairs":
                    # Convert to question-answer format
                    if isinstance(item, dict) and "input" in item and "output" in item:
                        formatted_dataset.append({"question": item["input"], "answer": item["output"]})
                    elif isinstance(item, dict) and "title" in item and "content" in item:
                        formatted_dataset.append({"question": item["title"], "answer": item["content"]})
                    elif isinstance(item, str):
                        # Split text into Q&A (simple heuristic for demonstration)
                        sentences = item.split('.')
                        if len(sentences) >= 2:
                            formatted_dataset.append({
                                "question": sentences[0].strip() + "?",
                                "answer": ".".join(sentences[1:]).strip()
                            })

                elif format_type == "Completion":
                    # Convert to completion format
                    if isinstance(item, dict) and "input" in item and "output" in item:
                        formatted_dataset.append({"input": item["input"], "output": item["output"]})
                    elif isinstance(item, dict) and "title" in item and "content" in item:
                        formatted_dataset.append({"input": item["title"], "output": item["content"]})
                    elif isinstance(item, str):
                        # Split text in half for input/output
                        mid = len(item) // 2
                        formatted_dataset.append({
                            "input": item[:mid].strip(),
                            "output": item[mid:].strip()
                        })

                elif format_type == "Chat":
                    # Convert to chat format
                    if isinstance(item, dict) and "input" in item and "output" in item:
                        formatted_dataset.append([
                            {"role": "user", "content": item["input"]},
                            {"role": "assistant", "content": item["output"]}
                        ])
                    elif isinstance(item, dict) and "title" in item and "content" in item:
                        formatted_dataset.append([
                            {"role": "user", "content": item["title"]},
                            {"role": "assistant", "content": item["content"]}
                        ])
                    elif isinstance(item, str):
                        # Create a simple chat from text
                        formatted_dataset.append([
                            {"role": "user", "content": f"Can you explain: {item[:50]}...?"},
                            {"role": "assistant", "content": item}
                        ])

                else:  # Custom format - preserve as is
                    if isinstance(item, dict):
                        formatted_dataset.append(item)
                    else:
                        formatted_dataset.append({"text": str(item)})

            total_samples = len(formatted_dataset)

            # STEP 3: Save dataset to output file (60% - 90%)
            progress.setValue(60)
            progress.setLabelText(f"Saving {total_samples} samples to {output_path}...")

            try:
                # Create directory if it doesn't exist
                os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)

                # Save based on format
                if output_format == "json":
                    with open(output_path, 'w', encoding='utf-8') as f:
                        json.dump(formatted_dataset, f, indent=2, ensure_ascii=False)
                elif output_format == "jsonl":
                    with open(output_path, 'w', encoding='utf-8') as f:
                        for item in formatted_dataset:
                            f.write(json.dumps(item, ensure_ascii=False) + '\n')
                elif output_format == "csv":
                    with open(output_path, 'w', encoding='utf-8', newline='') as f:
                        if total_samples > 0:
                            # Determine fieldnames based on first sample
                            if format_type == "QA Pairs":
                                fieldnames = ["question", "answer"]
                                writer = csv.DictWriter(f, fieldnames=fieldnames)
                                writer.writeheader()
                                for item in formatted_dataset:
                                    writer.writerow({
                                        "question": item.get("question", ""),
                                        "answer": item.get("answer", "")
                                    })
                            elif format_type == "Completion":
                                fieldnames = ["input", "output"]
                                writer = csv.DictWriter(f, fieldnames=fieldnames)
                                writer.writeheader()
                                for item in formatted_dataset:
                                    writer.writerow({
                                        "input": item.get("input", ""),
                                        "output": item.get("output", "")
                                    })
                            elif format_type == "Chat":
                                fieldnames = ["user_message", "assistant_message"]
                                writer = csv.writer(f)
                                writer.writerow(fieldnames)
                                for item in formatted_dataset:
                                    if isinstance(item, list) and len(item) >= 2:
                                        writer.writerow([
                                            item[0].get("content", ""),
                                            item[1].get("content", "")
                                        ])
                            else:
                                # For custom formats, use all keys from first item
                                if isinstance(formatted_dataset[0], dict):
                                    fieldnames = list(formatted_dataset[0].keys())
                                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                                    writer.writeheader()
                                    for item in formatted_dataset:
                                        writer.writerow(item)
                                else:
                                    writer = csv.writer(f)
                                    for item in formatted_dataset:
                                        writer.writerow([item])

                # Finalize
                for i in range(60, 100):
                    if progress.wasCanceled():
                        break
                    progress.setValue(i)
                    time.sleep(0.02)  # Simulate final processing

            except Exception as e:
                QMessageBox.critical(self, "Error", f"Error saving dataset: {str(e)}")
                return

            if not progress.wasCanceled():
                progress.setValue(100)
                QMessageBox.information(
                    self,
                    "Dataset Created",
                    f"Dataset has been successfully created at:\n{output_path}\n\n"
                    f"Total samples: {total_samples}\n"
                    f"Processed files: {processed_files}"
                )

                # Update the dataset path in the main dialog
                self.dataset_path_edit.setText(output_path)

                # Close the creation dialog
                dialog.accept()

        except Exception as e:
            QMessageBox.critical(self, "Error", f"Error creating dataset: {str(e)}")

    def augment_dataset(self):
        """Open dialog to augment the dataset with generated variations."""

        # Verify dataset exists
        dataset_path = self.dataset_path_edit.text()
        if not dataset_path or not os.path.exists(dataset_path):
            logging.warning("Attempted to augment dataset, but no dataset path was selected.")
            QMessageBox.warning(self, "Invalid Dataset", "Please select a valid dataset file.")
            return

        logging.info(f"Opening 'Augment Dataset' dialog for dataset: {dataset_path}")

        dialog = QDialog(self)
        dialog.setWindowTitle("Augment Dataset")
        dialog.setMinimumWidth(500)

        layout = QVBoxLayout()

        # Augmentation techniques
        techniques_group = QGroupBox("Augmentation Techniques")
        techniques_layout = QVBoxLayout()

        # Checkboxes for different techniques
        self.synonym_check = QCheckBox("Synonym Replacement")
        self.synonym_check.setChecked(True)
        self.synonym_check.setToolTip("Replace random words with their synonyms")

        self.random_insert_check = QCheckBox("Random Insertion")
        self.random_insert_check.setToolTip("Insert random synonyms of random words")

        self.random_swap_check = QCheckBox("Random Swap")
        self.random_swap_check.setToolTip("Randomly swap the position of words")

        self.random_delete_check = QCheckBox("Random Deletion")
        self.random_delete_check.setToolTip("Randomly delete words from the sentence")

        self.backtranslation_check = QCheckBox("Back-Translation")
        self.backtranslation_check.setToolTip("Translate text to another language and back")

        self.paraphrase_check = QCheckBox("Paraphrasing")
        self.paraphrase_check.setChecked(True)
        self.paraphrase_check.setToolTip("Create paraphrased versions using language models")

        techniques_layout.addWidget(self.synonym_check)
        techniques_layout.addWidget(self.random_insert_check)
        techniques_layout.addWidget(self.random_swap_check)
        techniques_layout.addWidget(self.random_delete_check)
        techniques_layout.addWidget(self.backtranslation_check)
        techniques_layout.addWidget(self.paraphrase_check)

        techniques_group.setLayout(techniques_layout)
        layout.addWidget(techniques_group)

        # Augmentation settings
        settings_group = QGroupBox("Settings")
        settings_layout = QFormLayout()

        # Number of augmentations per sample
        self.aug_per_sample_spin = QSpinBox()
        self.aug_per_sample_spin.setRange(1, 10)
        self.aug_per_sample_spin.setValue(2)
        self.aug_per_sample_spin.setToolTip("Number of augmented versions to generate per sample")
        settings_layout.addRow("Augmentations per sample:", self.aug_per_sample_spin)

        # Augmentation probability
        self.aug_prob_slider = QSlider(Qt.Horizontal)
        self.aug_prob_slider.setRange(10, 100)
        self.aug_prob_slider.setValue(30)
        self.aug_prob_slider.setTickPosition(QSlider.TicksBelow)
        self.aug_prob_slider.setTickInterval(10)
        self.aug_prob_value = QLabel("0.3")
        self.aug_prob_slider.valueChanged.connect(
            lambda v: self.aug_prob_value.setText(f"{v/100:.1f}"))
        aug_prob_layout = QHBoxLayout()
        aug_prob_layout.addWidget(self.aug_prob_slider)
        aug_prob_layout.addWidget(self.aug_prob_value)
        settings_layout.addRow("Augmentation probability:", aug_prob_layout)

        # Preservation options
        self.preserve_labels_check = QCheckBox("Preserve labels/answers")
        self.preserve_labels_check.setChecked(True)
        self.preserve_labels_check.setToolTip("Only augment inputs, preserve outputs/labels")
        settings_layout.addRow("", self.preserve_labels_check)

        settings_group.setLayout(settings_layout)
        layout.addWidget(settings_group)

        # Preview and progress
        preview_group = QGroupBox("Preview")
        preview_layout = QVBoxLayout()

        # Status and progress
        self.aug_progress = QProgressBar()
        self.aug_progress.setValue(0)
        preview_layout.addWidget(self.aug_progress)

        self.aug_status = QLabel("Ready to augment dataset")
        preview_layout.addWidget(self.aug_status)

        preview_group.setLayout(preview_layout)
        layout.addWidget(preview_group)

        # Buttons
        button_layout = QHBoxLayout()
        self.preview_button = QPushButton("Preview Augmentation")
        self.preview_button.clicked.connect(self._preview_augmentation)

        self.augment_button = QPushButton("Augment Dataset")
        self.augment_button.clicked.connect(lambda: self._perform_augmentation(dialog))

        self.cancel_button = QPushButton("Cancel")
        self.cancel_button.clicked.connect(dialog.reject)

        button_layout.addWidget(self.preview_button)
        button_layout.addWidget(self.augment_button)
        button_layout.addWidget(self.cancel_button)

        layout.addLayout(button_layout)

        dialog.setLayout(layout)
        dialog.exec_()

    def _preview_augmentation(self):
        """Show a preview of augmentation results"""
        dataset_path = self.dataset_path_edit.text()

        logging.info(f"Generating augmentation preview for dataset: {dataset_path}")
        logging.debug(f"Augmentation techniques for preview: Synonym={self.synonym_check.isChecked()}, Swap={self.random_swap_check.isChecked()}, Paraphrase={self.paraphrase_check.isChecked()}")

        try:
            # Load a sample from the dataset
            with open(dataset_path, 'r', encoding='utf-8') as f:
                if dataset_path.endswith('.jsonl'):
                    # JSONL format - read lines
                    lines = f.readlines()
                    sample = json.loads(random.choice(lines))
                else:
                    # JSON format - could be a list or dict
                    data = json.load(f)
                    if isinstance(data, list):
                        sample = random.choice(data)
                    else:
                        # Assume it's a dict with samples inside
                        if 'data' in data:
                            sample = random.choice(data['data'])
                        elif 'samples' in data:
                            sample = random.choice(data['samples'])
                        else:
                            # Just use the whole thing
                            sample = data

            # Extract input text to augment
            input_text = ""
            if 'input' in sample:
                input_text = sample['input']
            elif 'question' in sample:
                input_text = sample['question']
            elif 'prompt' in sample:
                input_text = sample['prompt']
            elif 'text' in sample:
                input_text = sample['text']
            elif isinstance(sample, list) and len(sample) > 0 and 'role' in sample[0]:
                # Chat format
                for msg in sample:
                    if msg.get('role') == 'user':
                        input_text = msg.get('content', '')
                        break

            if not input_text:
                QMessageBox.warning(self, "Preview Error", "Could not extract input text from dataset")
                return

            # Show augmentation preview dialog

            preview_dialog = QDialog(self)
            preview_dialog.setWindowTitle("Augmentation Preview")
            preview_dialog.setMinimumSize(600, 400)

            layout = QVBoxLayout()

            text_edit = QTextEdit()
            text_edit.setReadOnly(True)

            # Add original
            text_edit.append("<b>Original:</b>")
            text_edit.append(input_text)
            text_edit.append("")

            # Generate augmentations
            text_edit.append("<b>Augmented versions:</b>")

            # Synonym replacement
            if self.synonym_check.isChecked():
                text_edit.append("<i>Synonym replacement:</i>")
                # Simulate synonym replacement
                words = input_text.split()
                if len(words) > 3:
                    replace_idx = random.sample(range(len(words)), min(2, len(words) // 3))
                    for idx in replace_idx:
                        synonyms = {
                            "good": ["great", "excellent", "fine", "exceptional"],
                            "bad": ["poor", "terrible", "awful", "undesirable"],
                            "big": ["large", "huge", "massive", "enormous"],
                            "small": ["tiny", "little", "compact", "miniature"],
                            "fast": ["quick", "rapid", "swift", "speedy"],
                            "slow": ["sluggish", "unhurried", "leisurely", "gradual"]
                        }
                        word = words[idx].lower()
                        if word in synonyms:
                            words[idx] = random.choice(synonyms[word])
                    text_edit.append(" ".join(words))
                text_edit.append("")

            # Random Swap
            if self.random_swap_check.isChecked():
                text_edit.append("<i>Random swap:</i>")
                words = input_text.split()
                if len(words) > 3:
                    for _ in range(min(2, len(words) // 3)):
                        idx1, idx2 = random.sample(range(len(words)), 2)
                        words[idx1], words[idx2] = words[idx2], words[idx1]
                    text_edit.append(" ".join(words))
                text_edit.append("")

            # Paraphrasing
            if self.paraphrase_check.isChecked():
                text_edit.append("<i>Paraphrasing:</i>")
                # Simulate paraphrasing
                paraphrased = []
                segments = input_text.split(". ")
                for segment in segments:
                    if not segment:
                        continue
                    words = segment.split()
                    if len(words) > 3:
                        # Reword the segment slightly
                        if random.random() < 0.3:
                            words.insert(0, random.choice(["Basically, ", "In essence, ", "Essentially, ", "Fundamentally, "]))
                        if random.random() < 0.5 and len(words) > 5:
                            mid = len(words) // 2
                            words.insert(mid, random.choice(["actually, ", "indeed, ", "in fact, ", "certainly, "]))
                    paraphrased.append(" ".join(words))
                text_edit.append(". ".join(paraphrased))

            layout.addWidget(text_edit)

            close_button = QPushButton("Close")
            close_button.clicked.connect(preview_dialog.accept)
            layout.addWidget(close_button)

            preview_dialog.setLayout(layout)
            preview_dialog.exec_()

        except Exception as e:
            QMessageBox.warning(self, "Preview Error", f"Error previewing augmentation: {str(e)}")

    def _perform_augmentation(self, dialog):
        """Perform the actual dataset augmentation"""
        dataset_path = self.dataset_path_edit.text()

        # Get augmentation settings
        aug_per_sample = self.aug_per_sample_spin.value()
        aug_prob = self.aug_prob_slider.value() / 100.0

        techniques = []
        if self.synonym_check.isChecked():
            techniques.append("synonym_replacement")
        if self.random_insert_check.isChecked():
            techniques.append("random_insertion")
        if self.random_swap_check.isChecked():
            techniques.append("random_swap")
        if self.random_delete_check.isChecked():
            techniques.append("random_deletion")
        if self.backtranslation_check.isChecked():
            techniques.append("backtranslation")
        if self.paraphrase_check.isChecked():
            techniques.append("paraphrasing")

        logging.info(f"Performing dataset augmentation for: {dataset_path}. Techniques: {techniques}, Aug/Sample: {aug_per_sample}, Prob: {aug_prob}")

        if not techniques:
            QMessageBox.warning(self, "No Techniques", "Please select at least one augmentation technique.")
            return

        # Create augmented dataset
        try:
            # Get output path
            output_path = dataset_path.replace(".json", "_augmented.json")
            if output_path == dataset_path:
                output_path = dataset_path.split(".")[0] + "_augmented.json"

            # Simulate the augmentation process
            self.aug_progress.setValue(0)
            self.aug_status.setText("Loading dataset...")

            # Load dataset
            with open(dataset_path, 'r', encoding='utf-8') as f:
                if dataset_path.endswith('.jsonl'):
                    # JSONL format
                    lines = f.readlines()
