            encoding='utf-8',
            errors='replace'
        )

        stdout, stderr = process.communicate()

        if process.returncode != 0:
            error_msg = f"[Ghidra Analysis] Ghidra process failed with exit code {
                process.returncode}."
            app.update_output.emit(log_message(error_msg))
            app.update_status.emit(
                f"Error: Ghidra failed (Code {process.returncode})")
            if stderr:
                # Clean up stderr output for better logging
                clean_stderr = "\n".join(
                    line for line in stderr.splitlines() if line.strip())
                if clean_stderr:
                    app.update_output.emit(log_message(
                        f"[Ghidra Error Output]\n{clean_stderr}"))
            # Stop further processing in this thread if Ghidra failed
            return

        # Process stdout if successful
        if stdout:
            for line in stdout.splitlines():
                if line.strip():
                    # Avoid logging overly verbose Ghidra messages if needed
                    if "INFO" not in line or "Decompiling" in line or "Analysis results written" in line:
                        app.update_output.emit(
                            log_message(f"[Ghidra] {line.strip()}"))

        # Log stderr even on success, might contain warnings
        if stderr:
            clean_stderr = "\n".join(line for line in stderr.splitlines(
            ) if line.strip() and "INFO" not in line)  # Filter INFO lines
            if clean_stderr:
                app.update_output.emit(log_message(
                    f"[Ghidra Warnings/Output]\n{clean_stderr}"))

        # Check for output JSON file (only if process succeeded)
        json_path = os.path.join(os.getcwd(), "analysis_results.json")
        if os.path.exists(json_path):
            app.update_output.emit(log_message(
                f"[Ghidra Analysis] Results file found: {json_path}"))
            try:
                # Add a try-except around processing the results file
                process_ghidra_analysis_results(app, json_path)
                # Set status after processing
                app.update_status.emit("Ghidra analysis complete")
            except Exception as json_proc_err:
                app.update_output.emit(
                    log_message(
                        f"[Ghidra Analysis] Error processing results file '{json_path}': {json_proc_err}"))
                app.update_status.emit("Error processing Ghidra results")
        else:
            app.update_output.emit(log_message(
                "[Ghidra Analysis] No results file found."))
            app.update_status.emit(
                "Ghidra analysis complete (no results file)")

    except FileNotFoundError:
        # Handle case where Ghidra command itself is not found
        app.update_output.emit(
            log_message(
                f"[Ghidra Analysis] Error: Command not found. Ensure Ghidra path is correct: {
                    cmd[0]}"))
        app.update_status.emit("Error: Ghidra command not found")
    except Exception as e:
        # General exception handling
        app.update_output.emit(log_message(
            f"[Ghidra Analysis] Error during execution: {e}"))
        app.update_output.emit(log_message(traceback.format_exc()))
        app.update_status.emit(f"Error: {str(e)}")

    finally:
        # Robust cleanup
        try:
            if 'temp_dir' in locals() and os.path.exists(
                    temp_dir):  # Check if temp_dir was defined and exists
                try:
                    shutil.rmtree(temp_dir)
                    app.update_output.emit(log_message(
                        "[Ghidra Analysis] Temporary files cleaned up."))
                except Exception as cleanup_error:
                    app.update_output.emit(
                        log_message(
                            f"[Ghidra Analysis] Warning: Failed to clean up temporary directory '{temp_dir}': {cleanup_error}"))
        except Exception as final_e:
            app.update_output.emit(
                log_message(
                    f"[Ghidra Analysis] Error during final cleanup checks: {final_e}"))


def parse_patch_instructions(text):
    """
    Parses patch instructions from AI-generated text more robustly.
    Handles variations in formatting and logs skipped lines.
    Returns a list of dictionaries with address (int), new_bytes (bytes),
    and description (str) fields.
    """
    instructions = []
    # Regex to find lines starting with "Address:" (case-insensitive), capturing address, bytes, and optional comment
    # It allows flexible spacing and optional '0x' prefixes.
    # Group 1: Address (hex)
    # Group 2: Hex Bytes (hex chars and spaces allowed)
    # Group 3: Optional Comment/Description
    pattern = re.compile(
        r"^\s*Address:\s*(?:0x)?([0-9A-Fa-f]+)\s*NewBytes:\s*([0-9A-Fa-f\s]+)(?:\s*//\s*(.*))?$",
        re.IGNORECASE | re.MULTILINE)

    logger.info(
        f"[AI Parser] Attempting to parse AI response for patch instructions...")
    lines_processed = 0
    potential_matches = 0

    for match in pattern.finditer(text):
        lines_processed += 1  # Counting matches found by regex
        potential_matches += 1
        address_hex = match.group(1)
        new_bytes_hex_raw = match.group(2)
        description = match.group(3).strip() if match.group(
            3) else "AI generated patch"

        # Clean up hex bytes string (remove spaces)
        new_bytes_hex = "".join(new_bytes_hex_raw.split())

        # Validate and convert
        try:
            # Ensure hex bytes string has an even number of characters
            if len(new_bytes_hex) % 2 != 0:
                logger.warning(
                    f"[AI Parser] Skipped line {lines_processed}: Odd number of hex characters in NewBytes: '{new_bytes_hex_raw}'")
                continue

            address = int(address_hex, 16)
            new_bytes = bytes.fromhex(new_bytes_hex)

            if not new_bytes:  # Skip if parsing resulted in empty bytes
                logger.warning(
                    f"[AI Parser] Skipped line {lines_processed}: Parsed NewBytes resulted in empty byte string for '{new_bytes_hex_raw}'")
                continue

            instructions.append({
                "address": address,
                "new_bytes": new_bytes,
                "description": description
            })
            logger.info(
                f"[AI Parser] Successfully parsed instruction: Address=0x{
                    address:X}, Bytes='{
                    new_bytes.hex().upper()}', Desc='{description}'")

        except ValueError as e:
            logger.warning(
                f"[AI Parser] Skipped line {lines_processed}: Error parsing hex values: Address='{address_hex}', Bytes='{new_bytes_hex_raw}'. Error: {e}")
        except Exception as e_parse:
            logger.error(
                f"[AI Parser] Unexpected error parsing line {lines_processed}: {e_parse}")
            # Log the full matched line
            logger.error(f"  Line content: {match.group(0)}")

    # Log summary
    if not potential_matches:
        logger.warning(
            "[AI Parser] No lines matching the expected patch format (Address:... NewBytes:...) were found in the AI response.")
    else:
        logger.info(
            f"[AI Parser] Finished parsing. Found {
                len(instructions)} valid patch instruction(s) out of {potential_matches} potential matches.")

    return instructions

# -------------------------------
# Enhanced Binary Analysis
# -------------------------------


def analyze_binary_internal(binary_path, flags=None):
    """
    Analyzes the binary file structure in detail.

    Performs comprehensive static analysis of a binary executable file,
    examining its PE header, sections, imports, exports, resources, and strings.
    Identifies suspicious characteristics like high-entropy sections,
    dangerous permissions, and license-related imports.

    Args:
        binary_path: Path to the binary file to analyze
        flags: Optional list of analysis flags to control behavior
               (e.g., "stealth" to skip string scanning)

    Returns:
        list: Analysis results as a list of formatted strings
    """
    if flags is None:
        flags = []

    results = []

    try:
        logging.info(f"Starting internal binary analysis for: {binary_path}. Flags: {flags}")
        results.append(f"Analyzing binary: {os.path.basename(binary_path)}")
        results.append(f"File size: {os.path.getsize(binary_path):,} bytes")

        pe = pefile.PE(binary_path)

        # Basic PE header information
        results.append(f"\nPE Header:")
        results.append(
            f"Machine: 0x{
                pe.FILE_HEADER.Machine:04X} ({
                get_machine_type(
                    pe.FILE_HEADER.Machine)})")
        results.append(
            f"Number of sections: {pe.FILE_HEADER.NumberOfSections}")
        results.append(
            f"Time date stamp: {
                hex(
                    pe.FILE_HEADER.TimeDateStamp)} ({
                get_pe_timestamp(
                    pe.FILE_HEADER.TimeDateStamp)})")
        results.append(
            f"Characteristics: 0x{
                pe.FILE_HEADER.Characteristics:04X} ({
                get_characteristics(
                    pe.FILE_HEADER.Characteristics)})")

        # Optional header
        results.append(f"\nOptional Header:")
        results.append(
            f"Magic: 0x{
                pe.OPTIONAL_HEADER.Magic:04X} ({
                get_magic_type(
                    pe.OPTIONAL_HEADER.Magic)})")
        results.append(
            f"Entry point: 0x{pe.OPTIONAL_HEADER.AddressOfEntryPoint:08X}")
        results.append(f"Image base: 0x{pe.OPTIONAL_HEADER.ImageBase:08X}")
        if hasattr(pe.OPTIONAL_HEADER,
                   "CheckSum") and pe.OPTIONAL_HEADER.CheckSum != 0:
            results.append(f"Checksum: 0x{pe.OPTIONAL_HEADER.CheckSum:08X}")

        # Section information
        results.append(f"\nSections:")
        suspicious_sections = []

        for section in pe.sections:
            name = section.Name.decode('utf-8', errors='ignore').rstrip('\0')
            results.append(f"  {name}:")
            results.append(
                f"    Virtual Address: 0x{section.VirtualAddress:08X}")
            results.append(
                f"    Virtual Size: 0x{section.Misc_VirtualSize:08X} ({section.Misc_VirtualSize:,} bytes)")
            results.append(
                f"    Raw Data Size: 0x{section.SizeOfRawData:08X} ({section.SizeOfRawData:,} bytes)")

            # Check for suspicious sections
            entropy = calculate_entropy(section.get_data())
            results.append(f"    Entropy: {entropy:.2f}")

            if entropy > 7.0:
                results.append(
                    f"    WARNING: High entropy, possible encryption/compression")
                suspicious_sections.append(name)

            # Check section permissions
            characteristics = section.Characteristics
            is_executable = (characteristics & 0x20000000) != 0
            is_writable = (characteristics & 0x80000000) != 0
            perms = []
            if is_executable:
                perms.append("executable")
            if is_writable:
                perms.append("writable")
            if characteristics & 0x40000000:
                perms.append("readable")

            results.append(f"    Permissions: {', '.join(perms)}")

            logging.debug(f"Section '{name}': Entropy={entropy:.2f}, Executable={is_executable}, Writable={is_writable}")

            if is_executable and is_writable:
                results.append(
                    f"    WARNING: Section is both executable and writable (suspicious)")
                suspicious_sections.append(name)

        if suspicious_sections:
            results.append(
                f"\nSuspicious sections: {', '.join(suspicious_sections)}")

        # Import information
        if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
            results.append(f"\nImports:")
            license_related_imports = []

            for entry in pe.DIRECTORY_ENTRY_IMPORT:
                dll_name = entry.dll.decode('utf-8', errors='ignore')
                results.append(f"  {dll_name}:")

                # Limit to first 10 per DLL for brevity
                for imp in entry.imports[:10]:
                    if imp.name:
                        imp_name = imp.name.decode('utf-8', errors='ignore')
                        results.append(f"    {imp_name}")

                        # Check for license-related imports
                        license_keywords = [
                            "license", "activ", "regist", "key", "serial",
                            "valid", "expire", "check", "verify", "crypt"
                        ]

                        for keyword in license_keywords:
                            if keyword in imp_name.lower():
                                license_related_imports.append(
                                    f"{dll_name}:{imp_name}")
                                break

                if len(entry.imports) > 10:
                    results.append(
                        f"    ... and {len(entry.imports) - 10} more imports")

            if license_related_imports:
                logging.info(f"Found {len(license_related_imports)} license-related imports.")
                results.append(f"\nLicense-related imports:")
                for imp in license_related_imports:
                    results.append(f"  {imp}")

        # Export information
        if hasattr(pe, 'DIRECTORY_ENTRY_EXPORT'):
            results.append(f"\nExports:")
            # Limit to first 10 for brevity
            for exp in pe.DIRECTORY_ENTRY_EXPORT.symbols[:10]:
                if exp.name:
                    results.append(
                        f"  {exp.name.decode('utf-8', errors='ignore')}")

            if len(pe.DIRECTORY_ENTRY_EXPORT.symbols) > 10:
                results.append(
                    f"  ... and {len(pe.DIRECTORY_ENTRY_EXPORT.symbols) - 10} more exports")

        # Resources
        if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):
            results.append(f"\nResources:")
            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:
                try:
                    resource_type_str = get_resource_type(resource_type.id)
                    results.append(f"  {resource_type_str}:")

                    for resource_id in resource_type.directory.entries:
                        results.append(f"    ID: {resource_id.id}")
                except Exception:
                    pass

        # String scanning (simple)
        if "stealth" not in flags:  # Only do string scanning if not in stealth mode
            results.append(f"\nInteresting strings:")

            with open(binary_path, 'rb') as f:
                content = f.read()

            # License-related strings
            license_strings = []

            # UTF-16 patterns
            utf16_patterns = [
                b'L\x00i\x00c\x00e\x00n\x00s\x00e\x00',
                b'R\x00e\x00g\x00i\x00s\x00t\x00e\x00r\x00',
                b'A\x00c\x00t\x00i\x00v\x00a\x00t\x00i\x00o\x00n\x00',
                b'T\x00r\x00i\x00a\x00l\x00',
                b'E\x00x\x00p\x00i\x00r\x00e\x00'
            ]

            # UTF-8 patterns
            utf8_patterns = [
                b'License', b'Register', b'Activation',
                b'Serial', b'Trial', b'Expire', b'HWID'
            ]

            # Search UTF-16 patterns
            for pattern in utf16_patterns:
                for match in re.finditer(pattern, content):
                    # Try to extract the full string
                    pos = match.start()
                    end_pos = content.find(b'\x00\x00', pos)
                    if end_pos > pos:
                        # Extract and convert to UTF-16
                        try:
                            s = content[pos:end_pos + 2].decode('utf-16')
                            license_strings.append(s)
                        except BaseException:
                            pass

            # Search UTF-8 patterns
            for pattern in utf8_patterns:
                for match in re.finditer(pattern, content):
                    # Try to extract the full string
                    pos = match.start()
                    # Look for a null byte or a non-printable character
                    end_pos = pos
                    while end_pos < len(
                            content) and content[end_pos] >= 32 and content[end_pos] < 127:
                        end_pos += 1

                    if end_pos > pos:
                        try:
                            s = content[pos:end_pos].decode('utf-8')
                            license_strings.append(s)
                        except BaseException:
                            pass

            # Remove duplicates and show results
            license_strings = list(set(license_strings))
            for s in license_strings[:20]:  # Limit to 20 for brevity
                if len(s) > 3:  # Filter out too short strings
                    results.append(f"  {s}")

            if len(license_strings) > 20:
                results.append(
                    f"  ... and {len(license_strings) - 20} more strings")

            if not license_strings:
                results.append(
                    "  No license-related strings found (may be obfuscated)")

        # Protection detection if flags specify it
        if "auto" in flags:
            results.append("\nRunning protection detection...")

            # Check for commercial protectors with more details
            protection_results = detect_commercial_protections(binary_path)
            for line in protection_results:
                if not line.startswith("Scanning for"):  # Skip intro line
                    results.append(f"  {line}")

        logging.info(f"Internal binary analysis complete for {binary_path}.")

    except ImportError:
        results.append(
            "Error: pefile module not available. Please install with 'pip install pefile'")
        logging.error(f"Error in analyze_binary_internal for {binary_path}: pefile module not available", exc_info=True)
    except Exception as e:
        results.append(f"Error analyzing binary: {e}")
        results.append(traceback.format_exc())
        logging.error(f"Error in analyze_binary_internal for {binary_path}: {e}", exc_info=True)

    return results


def get_machine_type(machine_value):
    """Returns a readable machine type from the Machine value."""
    machine_types = {
        0x0: "UNKNOWN",
        0x1d3: "AM33",
        0x8664: "AMD64",
        0x1c0: "ARM",
        0xaa64: "ARM64",
        0x1c4: "ARMNT",
        0xebc: "EBC",
        0x14c: "I386",
        0x200: "IA64",
        0x9041: "M32R",
        0x266: "MIPS16",
        0x366: "MIPSFPU",
        0x466: "MIPSFPU16",
        0x1f0: "POWERPC",
        0x1f1: "POWERPCFP",
        0x166: "R4000",
        0x5032: "RISCV32",
        0x5064: "RISCV64",
        0x5128: "RISCV128",
        0x1a2: "SH3",
        0x1a3: "SH3DSP",
        0x1a6: "SH4",
        0x1a8: "SH5",
        0x1c2: "THUMB",
        0x169: "WCEMIPSV2"
    }
    return machine_types.get(machine_value, f"UNKNOWN (0x{machine_value:04X})")


def get_magic_type(magic_value):
    """
    Returns a readable magic type.

    Converts the numeric magic type value from a PE file header into a
    human-readable format description. Identifies PE32, PE32+ (64-bit),
    and other executable formats.

    Args:
        magic_value: Numeric magic type value from PE header

    Returns:
        str: Human-readable format description or "Unknown" if not recognized
    """
    magic_types = {
        0x10b: "PE32",
        0x20b: "PE32+",
        0x107: "ROM Image"
    }
    return magic_types.get(magic_value, f"UNKNOWN (0x{magic_value:04X})")


def get_characteristics(characteristics):
    """
    Converts PE file characteristics flags to human-readable descriptions.

    Interprets the characteristics bitfield from a PE file header and
    returns a list of the enabled characteristics as human-readable strings.
    Identifies properties like whether the file is executable, a DLL,
    system file, etc.

    Args:
        characteristics: Numeric characteristics bitfield from PE header

    Returns:
        list: Human-readable descriptions of the enabled characteristics
    """
    flags = []
    if characteristics & 0x0001:
        flags.append("RELOCS_STRIPPED")
    if characteristics & 0x0002:
        flags.append("EXECUTABLE_IMAGE")
    if characteristics & 0x0004:
        flags.append("LINE_NUMS_STRIPPED")
    if characteristics & 0x0008:
        flags.append("LOCAL_SYMS_STRIPPED")
    if characteristics & 0x0010:
        flags.append("AGGRESIVE_WS_TRIM")
    if characteristics & 0x0020:
        flags.append("LARGE_ADDRESS_AWARE")
    if characteristics & 0x0080:
        flags.append("BYTES_REVERSED_LO")
    if characteristics & 0x0100:
        flags.append("32BIT_MACHINE")
    if characteristics & 0x0200:
        flags.append("DEBUG_STRIPPED")
    if characteristics & 0x0400:
        flags.append("REMOVABLE_RUN_FROM_SWAP")
    if characteristics & 0x0800:
        flags.append("NET_RUN_FROM_SWAP")
    if characteristics & 0x1000:
        flags.append("SYSTEM")
    if characteristics & 0x2000:
        flags.append("DLL")
    if characteristics & 0x4000:
        flags.append("UP_SYSTEM_ONLY")
    if characteristics & 0x8000:
        flags.append("BYTES_REVERSED_HI")

    return ", ".join(flags)


def get_pe_timestamp(timestamp):
    """
    Converts a PE timestamp to a readable date string.

    Transforms the Unix timestamp from a PE file header into a formatted
    date and time string, showing when the executable was compiled.

    Args:
        timestamp: Unix timestamp from PE header

    Returns:
        str: Formatted date and time string
    """
    try:
        dt = datetime.datetime.fromtimestamp(timestamp)
        return dt.strftime("%Y-%m-%d %H:%M:%S")
    except BaseException:
        return "Invalid timestamp"


def get_file_icon(path):
    """
    Get the icon for a file.

    Extracts the primary icon from a Windows executable file and
    converts it to a QIcon object for display in the UI. Handles
    extraction of icons at different resolutions.

    Args:
        path: Path to the executable file

    Returns:
        QIcon: Icon extracted from the executable, or an empty QPixmap if extraction fails
    """
    # Create a standalone logger if not in class context
    icon_logger = logging.getLogger("IconExtractor")

    if not sys.platform == "win32":
        # Early return with warning for non-Windows platforms
        icon_logger.warning("Icon extraction is only supported on Windows platforms")
        return QPixmap()

    # Windows-specific code
    try:
        # Extract the icon from the file
        large, small = win32gui.ExtractIconEx(path, 0)
        if large:
            try:
                # Convert icon to bitmap
                hdc = win32ui.CreateDCFromHandle(win32gui.GetDC(0))
                hbmp = win32ui.CreateBitmap()
                hbmp.CreateCompatibleBitmap(hdc, 32, 32)
                hdc = hdc.CreateCompatibleDC()
                hdc.SelectObject(hbmp)
                hdc.DrawIcon((0, 0), large[0])

                # Convert to QPixmap
                bmpstr = hbmp.GetBitmapBits(True)
                img = QImage(
                    bmpstr, 32, 32, QImage.Format_ARGB32_Premultiplied)
                pixmap = QPixmap.fromImage(img)
                return pixmap
            finally:
                # Always clean up handles
                for handle in large:
                    win32gui.DestroyIcon(handle)
                for handle in small:
                    win32gui.DestroyIcon(handle)

    except Exception as e:
        # Detailed error logging
        icon_logger.error(f"Error extracting icon from '{path}': {e}")
        icon_logger.debug(f"Exception details: {traceback.format_exc()}")

    # Return a default icon if extraction fails
    return QPixmap()


def get_resource_type(type_id):
    """
    Returns a readable resource type.

    Converts the numeric resource type ID from a PE file's resource section
    into a human-readable resource type name. Identifies common resource
    types like icons, cursors, bitmaps, menus, dialogs, etc.

    Args:
        type_id: Numeric resource type ID

    Returns:
        str: Human-readable resource type name or "Unknown" if not recognized
    """
    resource_types = {
        1: "RT_CURSOR",
        2: "RT_BITMAP",
        3: "RT_ICON",
        4: "RT_MENU",
        5: "RT_DIALOG",
        6: "RT_STRING",
        7: "RT_FONTDIR",
        8: "RT_FONT",
        9: "RT_ACCELERATOR",
        10: "RT_RCDATA",
        11: "RT_MESSAGETABLE",
        12: "RT_GROUP_CURSOR",
        14: "RT_GROUP_ICON",
        16: "RT_VERSION",
        17: "RT_DLGINCLUDE",
        19: "RT_PLUGPLAY",
        20: "RT_VXD",
        21: "RT_ANICURSOR",
        22: "RT_ANIICON",
        23: "RT_HTML",
        24: "RT_MANIFEST"
    }

    if isinstance(type_id, int):
        return resource_types.get(type_id, f"Unknown ({type_id})")
    else:
        return f"Custom ({type_id})"


def apply_parsed_patch_instructions_with_validation(app, instructions):
    """
    Applies parsed patch instructions to a copy of the binary.

    Takes a list of patch instructions (typically parsed from AI output)
    and applies them to a copy of the target binary. Includes comprehensive
    validation, error handling, and backup creation for safety.

    Each instruction contains an address, new bytes to write, and a description.
    The function verifies each patch can be applied safely before making changes.

    Args:
        app: Application instance containing UI elements and binary path
        instructions: List of patch instructions with addresses and byte values

    Returns:
        bool: True if patching was successful, False otherwise
    """
    if not app.binary_path:
        app.update_output.emit(log_message(
            "[Patch] Error: No binary selected."))
        return
    if not instructions:
        app.update_output.emit(log_message(
            "[Patch] Error: No patch instructions provided."))
        return

    # Create backup (using timestamp for uniqueness)
    backup_path = app.binary_path + f".backup_{int(time.time())}"
    try:
        shutil.copy2(app.binary_path, backup_path)
        app.update_output.emit(log_message(
            f"[Patch] Created backup: {backup_path}"))
    except Exception as e:
        app.update_output.emit(log_message(
            f"[Patch] CRITICAL ERROR: Failed to create backup: {e}"))
        app.update_output.emit(log_message(
            "[Patch] Aborting patching process."))
        # Optionally show a critical error dialog to the user
        # QMessageBox.critical(app, "Backup Failed", f"Could not create backup file. Patching aborted.\nError: {e}")
        return  # Stop patching if backup fails

    # Create patched file path
    base_name, ext = os.path.splitext(app.binary_path)
    patched_path = f"{base_name}_patched{ext}"

    try:
        # Copy original to patched path
        shutil.copy2(app.binary_path, patched_path)
        app.update_output.emit(log_message(
            f"[Patch] Created temporary patched file: {patched_path}"))

        # Load PE structure of the *patched* file for offset calculations
        try:
            pe = pefile.PE(patched_path)
            image_base = pe.OPTIONAL_HEADER.ImageBase
        except pefile.PEFormatError as e:
            app.update_output.emit(
                log_message(
                    f"[Patch] Error: Cannot parse PE structure of '{patched_path}': {e}"))
            app.update_output.emit(log_message("[Patch] Aborting patching."))
            # Clean up the potentially corrupted patched file
            try:
                os.remove(patched_path)
            except Exception:
                pass
            return

        applied_count = 0
        error_count = 0

        # Apply patches to the copy
        with open(patched_path, "r+b") as f:
            for i, patch in enumerate(instructions):
                patch_num = i + 1
                address = patch.get("address")
                new_bytes = patch.get("new_bytes")
                desc = patch.get("description", "No description")

                if address is None or new_bytes is None:
                    app.update_output.emit(
                        log_message(
                            f"[Patch {patch_num}] Skipped: Invalid instruction data."))
                    error_count += 1
                    continue

                try:
                    # Calculate file offset from RVA (relative to image base)
                    # Ensure address is treated as RVA if it's above image
                    # base, otherwise assume direct file offset (less common)
                    if address >= image_base:
                        rva = address - image_base
                        try:
                            offset = pe.get_offset_from_rva(rva)
                        except Exception as e_rva:
                            app.update_output.emit(
                                log_message(
                                    f"[Patch {patch_num}] ERROR: Failed to get offset for RVA 0x{rva:X}: {e_rva}"))
                            error_count += 1
                            continue  # Skip this patch entirely rather than using risky fallback
                    else:
                        # Assuming address might be a direct file offset if
                        # smaller than image base (use with caution)
                        offset = address
                        app.update_output.emit(
                            log_message(
                                f"[Patch {patch_num}] Warning: Address 0x{
                                    address:X} seems low, treating as direct file offset 0x{
                                    offset:X}."))

                    # Apply patch
                    app.update_output.emit(
                        log_message(
                            f"[Patch {patch_num}] Applying at address 0x{
                                address:X} (offset 0x{
                                offset:X}): {
                                len(new_bytes)} bytes for '{desc}'"))
                    f.seek(offset)
                    f.write(new_bytes)
                    applied_count += 1

                except pefile.PEFormatError as e_offset:
                    app.update_output.emit(
                        log_message(
                            f"[Patch {patch_num}] Skipped: Error getting offset for address 0x{
                                address:X}: {e_offset}"))
                    error_count += 1
                except IOError as e_io:
                    app.update_output.emit(log_message(
                        f"[Patch {patch_num}] Skipped: File I/O error applying patch at offset 0x{offset:X}: {e_io}"))
                    error_count += 1
                except Exception as e_apply:
                    app.update_output.emit(
                        log_message(
                            f"[Patch {patch_num}] Skipped: Unexpected error applying patch: {e_apply}"))
                    app.update_output.emit(log_message(traceback.format_exc()))
                    error_count += 1

        # Close the PE file handle before verification
        pe.close()

        app.update_output.emit(
            log_message(
                f"[Patch] Applied {applied_count} patches with {error_count} errors/skips."))

        if applied_count > 0 and error_count == 0:
            app.update_output.emit(log_message(
                f"[Patch] Verifying patched file integrity: {patched_path}"))

            # --- Post-Patch Validation ---
            validation_passed = False
            try:
                # 1. Basic PE Load Check
                verify_pe = pefile.PE(patched_path)
                # 2. (Optional) Recalculate Checksum if needed (though often ignored)
                # checksum = verify_pe.generate_checksum()
                # app.update_output.emit(log_message(f"[Verify] New checksum: 0x{checksum:08X}"))
                verify_pe.close()  # Close handle after check
                validation_passed = True
                app.update_output.emit(log_message(
                    "[Verify] Patched file is still a valid PE executable."))
            except pefile.PEFormatError as e_verify:
                app.update_output.emit(
                    log_message(
                        f"[Verify] CRITICAL ERROR: Patched file '{patched_path}' failed PE validation: {e_verify}"))
                app.update_output.emit(
                    log_message("[Verify] The patch might have corrupted the file structure."))
                app.update_output.emit(
                    log_message(
                        f"[Verify] Please examine the file or restore from backup: {backup_path}"))
                # Consider showing a critical error message box here
                # QMessageBox.critical(app, "Patch Verification Failed", f"Patched file failed validation and might be corrupt.\nError: {e_verify}\n\nPlease restore from {backup_path}")

            # --- Detailed Byte Verification ---
            if validation_passed:
                verification_results = verify_patches(
                    app, patched_path, instructions)  # Use the existing verify function
                for line in verification_results:
                    app.update_output.emit(log_message(f"[Verify] {line}"))

                # Check if all patches verified successfully
                if all(
                        "verified successfully" in line or "Invalid patch" in line for line in verification_results):
                    app.update_output.emit(
                        log_message(
                            f"[Patch] Successfully created and verified patched file: {patched_path}"))
                else:
                    app.update_output.emit(
                        log_message("[Patch] Warning: Some patches could not be verified. Review logs."))
            # No need to call verify_patches again if validation failed

        elif applied_count == 0:
            app.update_output.emit(log_message(
                "[Patch] No patches were applied. Original file remains unchanged."))
            # Clean up the copied file if no patches applied
            try:
                os.remove(patched_path)
            except Exception:
                pass
        else:  # Errors occurred during patching
            app.update_output.emit(log_message(
                "[Patch] Errors occurred during patching. Patched file may be incomplete or corrupt."))
            app.update_output.emit(
                log_message(
                    f"[Patch] Please review logs and consider restoring from backup: {backup_path}"))

    except IOError as e_io:
        app.update_output.emit(
            log_message(
                f"[Patch] CRITICAL FILE ERROR: Could not read/write patch file '{patched_path}': {e_io}"))
    except Exception as e_main:
        app.update_output.emit(log_message(
            f"[Patch] Unexpected error during patching process: {e_main}"))
        app.update_output.emit(log_message(traceback.format_exc()))

    # Return a dictionary with success status and patched file path
    if 'patched_path' in locals() and applied_count > 0 and error_count == 0:
        return {
            "status": "success",
            "patched_path": patched_path,
            "backup_path": backup_path
        }
    # Return just a dict with error status if patching failed
    return {
        "status": "error",
        "message": "Patching failed or no patches were applied"
    }


def rewrite_license_functions_with_parsing(app):
    """
    Attempts to find and rewrite license checking functions using various methods.
    Includes enhanced logging and basic safety checks for code size.
    """
    if not app.binary_path:
        app.update_output.emit(log_message(
            "[License Rewrite] No binary selected."))
        return

    app.update_output.emit(log_message(
        "[License Rewrite] Starting license function rewriting analysis..."))
    app.analyze_status.setText("Rewriting license functions...")
    patches = []
    strategy_used = "None"

    # --- Strategy 1: Deep License Analysis ---
    app.update_output.emit(log_message(
        "[License Rewrite] Running deep license analysis to find candidates..."))
    candidates = enhanced_deep_license_analysis(app.binary_path)

    if candidates:
        app.update_output.emit(
            log_message(
                f"[License Rewrite] Deep analysis found {
                    len(candidates)} candidates. Processing top candidates..."))
        strategy_used = "Deep Analysis"
        # Sort by confidence and take top ones
        candidates.sort(key=lambda x: x.get("confidence", 0), reverse=True)
        top_candidates = candidates[:5]  # Limit number of candidates to patch

        try:
            pe = pefile.PE(app.binary_path)
            is_64bit = pe.FILE_HEADER.Machine == 0x8664
            mode = CS_MODE_64 if is_64bit else CS_MODE_32
            arch = keystone.KS_ARCH_X86
            ks_mode = keystone.KS_MODE_64 if is_64bit else keystone.KS_MODE_32
            cs_mode = CS_MODE_64 if is_64bit else CS_MODE_32

            ks = keystone.Ks(arch, ks_mode)
            md = Cs(arch, cs_mode)
            md.detail = True  # Enable detail for instruction size

            # Get .text section for code analysis
            text_section = next(
                (s for s in pe.sections if b'.text' in s.Name.lower()), None)
            if not text_section:
                app.update_output.emit(log_message(
                    "[License Rewrite] Error: Cannot find .text section."))
                raise Exception(".text section not found")

            code_data = text_section.get_data()
            code_base_addr = pe.OPTIONAL_HEADER.ImageBase + text_section.VirtualAddress

            for candidate in top_candidates:
                start_addr = candidate["start"]
                keywords = candidate.get("keywords", [])
                patch_generated = False

                app.update_output.emit(
                    log_message(
                        f"[License Rewrite] Processing candidate at 0x{
                            start_addr:X} (Keywords: {
                            ', '.join(keywords)})"))

                # Determine the patch bytes (e.g., return 1)
                if is_64bit:
                    # mov rax, 1; ret => 48 C7 C0 01 00 00 00 C3
                    patch_asm = "mov rax, 1; ret"
                    patch_bytes, _ = ks.asm(patch_asm)
                    patch_bytes = bytes(patch_bytes)
                else:
                    # mov eax, 1; ret => B8 01 00 00 00 C3
                    patch_asm = "mov eax, 1; ret"
                    patch_bytes, _ = ks.asm(patch_asm)
                    patch_bytes = bytes(patch_bytes)

                # --- Safety Check: Prologue Size ---
                try:
                    # Calculate offset within code_data
                    code_offset = start_addr - code_base_addr
                    if 0 <= code_offset < len(code_data):
                        # Disassemble first few bytes of the function
                        # Disassemble enough bytes to cover common prologues +
                        # patch size
                        bytes_to_disassemble = max(len(patch_bytes), 15)
                        # Disassemble up to 5 instructions
                        instructions = list(md.disasm(
                            code_data[code_offset: code_offset + bytes_to_disassemble], start_addr, count=5))

                        bytes_at_addr = code_data[code_offset: code_offset + bytes_to_disassemble] if 0 <= code_offset < len(code_data) else None
                        disasm_at_addr = '; '.join([f"{i.mnemonic} {i.op_str}" for i in instructions]) if instructions else None
                        min_patch_size = len(patch_bytes) if 'patch_bytes' in locals() else 6

                        if instructions:
                            prologue_size = 0
                            # More conservative prologue size estimation
                            # Only count simple stack and register setup instructions
