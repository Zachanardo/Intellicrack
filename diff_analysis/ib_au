
                This function is decorated with @ray.remote to enable distributed execution
                across multiple processes or nodes. It reads a specific chunk of the binary
                file based on the provided index, applies the processing function, and
                returns the results.

                Args:
                    chunk_idx (int): Index of the chunk to process

                Returns:
                    Any: Result of applying the process_func to the chunk data and offset

                Note:
                    This function is executed in separate Ray worker processes and
                    communicates results back to the main process.
                """
                offset = chunk_idx * self.chunk_size
                with open(self.binary_path, 'rb') as f:
                    f.seek(offset)
                    chunk_data = f.read(self.chunk_size)
                return process_func(chunk_data, offset)

            # Submit tasks
            tasks = []
            for i in range(num_chunks):
                tasks.append(process_chunk.remote(i))

            # Get results with progress tracking
            results = []
            completed = 0
            for result in ray.get(tasks):
                results.append(result)
                completed += 1
                if completed % max(1, num_chunks // 10) == 0:  # Report every 10%
                    self.logger.info(f"Progress: {completed}/{num_chunks} chunks processed ({completed/num_chunks*100:.1f}%)")

            return results

        except Exception as e:
            self.logger.error(f"Error in Ray processing: {e}")
            self.logger.info("Falling back to multiprocessing")
            return self._process_with_multiprocessing(process_func, num_chunks)

    def _process_with_dask(self, process_func, num_chunks):
        """
        Process binary chunks using Dask.

        Args:
            process_func: Function to process each chunk
            num_chunks: Number of chunks to process

        Returns:
            list: Results from all chunks
        """
        try:
            # Create client
            client = Client(n_workers=self.num_workers)

            # Define function to read and process chunk
            def read_and_process_chunk(chunk_idx):
                """
                Read and process a specific chunk of data from the binary file.

                This function handles the file I/O operations needed to read a specific
                chunk of the binary file based on the provided index. It calculates the
                file offset, reads the binary data, and applies the provided processing
                function to that data.

                Args:
                    chunk_idx (int): Index of the chunk to process

                Returns:
                    Any: Result of applying the process_func to the chunk data and offset

                Note:
                    This function is called by Dask's distributed compute system for
                    parallel processing of file chunks.
                """
                offset = chunk_idx * self.chunk_size
                with open(self.binary_path, 'rb') as f:
                    f.seek(offset)
                    chunk_data = f.read(self.chunk_size)
                return process_func(chunk_data, offset)

            # Create tasks
            futures = []
            for i in range(num_chunks):
                future = client.submit(read_and_process_chunk, i)
                futures.append(future)

            # Show progress
            progress(futures)

            # Compute results
            results = client.gather(futures)

            # Close client
            client.close()

            return list(results)

        except Exception as e:
            self.logger.error(f"Error in Dask processing: {e}")
            self.logger.info("Falling back to multiprocessing")
            return self._process_with_multiprocessing(process_func, num_chunks)

    def _process_with_multiprocessing(self, process_func, num_chunks):
        """
        Process binary chunks using multiprocessing.

        Args:
            process_func: Function to process each chunk
            num_chunks: Number of chunks to process

        Returns:
            list: Results from all chunks
        """
        # Define function to read and process chunk
        def read_and_process_chunk(chunk_idx):
            """
            Read a chunk from the binary file and process it.

            Args:
                chunk_idx: Index of the chunk to read.

            Returns:
                The result of process_func on the chunk data, or an error dict.
            """
            try:
                offset = chunk_idx * self.chunk_size
                with open(self.binary_path, 'rb') as f:
                    f.seek(offset)
                    chunk_data = f.read(self.chunk_size)
                return process_func(chunk_data, offset)
            except Exception as e:
                return {"error": str(e), "offset": offset, "chunk_idx": chunk_idx}

        # Create pool
        with multiprocessing.Pool(processes=self.num_workers) as pool:
            # Process chunks with progress tracking
            results = []
            for i, result in enumerate(pool.imap_unordered(read_and_process_chunk, range(num_chunks))):
                results.append(result)
                if (i + 1) % max(1, num_chunks // 10) == 0:  # Report every 10%
                    self.logger.info(f"Progress: {i+1}/{num_chunks} chunks processed ({(i+1)/num_chunks*100:.1f}%)")

        return results

    def start_processing(self):
        """
        Start distributed processing of tasks using queue-based approach.

        Returns:
            bool: True if processing started successfully, False otherwise
        """
        if not self.binary_path:
            self.logger.error("No binary set")
            return False

        if not self.tasks:
            self.logger.warning("No tasks specified")
            return False

        if self.running:
            self.logger.warning("Already running")
            return False

        # Clear previous results
        self.results = {
            'tasks_completed': 0,
            'tasks_failed': 0,
            'total_processing_time': 0.0,
            'task_results': {}
        }

        try:
            # Initialize multiprocessing queues
            self.task_queue = multiprocessing.Queue()
            self.result_queue = multiprocessing.Queue()

            # Add tasks to queue
            for task in self.tasks:
                self.task_queue.put(task)

            # Add sentinel tasks to signal workers to exit
            for _ in range(self.num_workers):
                self.task_queue.put(None)

            # Start workers
            self.workers = []
            for i in range(self.num_workers):
                worker = multiprocessing.Process(
                    target=self._worker_process,
                    args=(i, self.task_queue, self.result_queue, self.binary_path, self.chunk_size)
                )
                worker.daemon = True
                worker.start()
                self.workers.append(worker)

            self.running = True
            self.logger.info(f"Started {self.num_workers} workers for task-based processing")

            return True

        except Exception as e:
            self.logger.error(f"Error starting processing: {e}")
            self.stop_processing()
            return False

    def _worker_process(self, worker_id, task_queue, result_queue, binary_path, chunk_size):
        """
        Worker process function for task-based processing.

        Args:
            worker_id: ID of the worker
            task_queue: Queue for tasks
            result_queue: Queue for results
            binary_path: Path to the binary file
            chunk_size: Size of chunks for file processing
        """
        try:
            # Set up worker
            logger = logging.getLogger(f"IntellicrackLogger.Worker{worker_id}")
            logger.info(f"Worker {worker_id} started")

            # Process tasks
            while True:
                # Get task from queue
                task = task_queue.get()

                # Check for sentinel
                if task is None:
                    logger.info(f"Worker {worker_id} shutting down")
                    break

                # Process task
                start_time = time.time()
                logger.info(f"Worker {worker_id} processing task: {task['type']} (ID: {task['id']})")

                try:
                    # Process task based on type
                    if task['type'] == 'find_patterns':
                        result = self._task_find_patterns(worker_id, task, binary_path, chunk_size)
                    elif task['type'] == 'analyze_entropy':
                        result = self._task_analyze_entropy(worker_id, task, binary_path, chunk_size)
                    elif task['type'] == 'analyze_section':
                        result = self._task_analyze_section(worker_id, task, binary_path, chunk_size)
                    elif task['type'] == 'symbolic_execution':
                        result = self._task_symbolic_execution(worker_id, task, binary_path, chunk_size)
                    else:
                        # Generic task - process a chunk
                        result = self._task_generic(worker_id, task, binary_path, chunk_size)

                    # Add processing time
                    processing_time = time.time() - start_time
                    result['processing_time'] = processing_time
                    result['worker_id'] = worker_id
                    result['task_id'] = task['id']
                    result['success'] = True

                    # Put result in result queue
                    result_queue.put((worker_id, task, result))

                except Exception as e:
                    logger.error(f"Error processing task {task['id']}: {e}")
                    processing_time = time.time() - start_time
                    error_result = {
                        'error': str(e),
                        'traceback': traceback.format_exc(),
                        'worker_id': worker_id,
                        'task_id': task['id'],
                        'processing_time': processing_time,
                        'success': False
                    }
                    result_queue.put((worker_id, task, error_result))

        except Exception as e:
            logger.error(f"Worker {worker_id} error: {e}")

    def _task_find_patterns(self, worker_id, task, binary_path, chunk_size):
        """Process a pattern-finding task."""
        logger = logging.getLogger(f"IntellicrackLogger.Worker{worker_id}")
        patterns = task['params'].get('patterns', [])
        chunk_start = task['params'].get('chunk_start', 0)
        chunk_end = task['params'].get('chunk_end', None)

        if not patterns:
            return {'error': "No patterns specified", 'matches': []}

        # Read specified chunk of file
        with open(binary_path, 'rb') as f:
            f.seek(chunk_start)
            if chunk_end is not None:
                chunk_data = f.read(chunk_end - chunk_start)
            else:
                chunk_data = f.read(chunk_size)

        # Search for patterns
        matches = []
        for pattern in patterns:
            pattern_bytes = pattern.encode() if isinstance(pattern, str) else pattern
            for match in re.finditer(pattern_bytes, chunk_data):
                matches.append({
                    'pattern': pattern,
                    'position': chunk_start + match.start(),
                    'match': match.group()
                })

        logger.info(f"Found {len(matches)} pattern matches in chunk at offset {chunk_start}")
        return {'matches': matches, 'patterns_found': len(matches)}

    def _task_analyze_entropy(self, worker_id, task, binary_path, chunk_size):
        """Process an entropy analysis task."""
        logger = logging.getLogger(f"IntellicrackLogger.Worker{worker_id}")
        chunk_start = task['params'].get('chunk_start', 0)
        chunk_end = task['params'].get('chunk_end', None)
        window_size = task['params'].get('window_size', 1024)  # Default 1KB windows

        # Read specified chunk of file
        with open(binary_path, 'rb') as f:
            f.seek(chunk_start)
            if chunk_end is not None:
                chunk_data = f.read(chunk_end - chunk_start)
            else:
                chunk_data = f.read(chunk_size)

        # Calculate overall entropy for the chunk
        chunk_entropy = 0
        try:
            chunk_entropy = AdvancedVulnerabilityEngine.calculate_entropy(chunk_data)
        except:
            # Fallback entropy calculation if AdvancedVulnerabilityEngine not available
            counts = Counter(chunk_data)
            total = len(chunk_data)
            chunk_entropy = -sum((count/total) * math.log2(count/total) for count in counts.values())

        # Calculate entropy for sliding windows
        window_results = []
        for i in range(0, len(chunk_data) - window_size + 1, window_size // 2):  # 50% overlap
            window_data = chunk_data[i:i+window_size]
            try:
                window_entropy = AdvancedVulnerabilityEngine.calculate_entropy(window_data)
            except:
                # Fallback entropy calculation
                counts = Counter(window_data)
                total = len(window_data)
                window_entropy = -sum((count/total) * math.log2(count/total) for count in counts.values())

            window_results.append({
                'offset': chunk_start + i,
                'size': len(window_data),
                'entropy': window_entropy
            })

        # Find high entropy regions
        high_entropy_regions = [w for w in window_results if w['entropy'] > 7.0]  # High entropy threshold

        logger.info(f"Analyzed entropy in chunk at offset {chunk_start}: {chunk_entropy:.2f}")
        return {
            'chunk_offset': chunk_start,
            'chunk_size': len(chunk_data),
            'chunk_entropy': chunk_entropy,
            'windows': window_results,
            'high_entropy_regions': high_entropy_regions,
            'high_entropy_count': len(high_entropy_regions)
        }

    def _task_analyze_section(self, worker_id, task, binary_path, chunk_size):
        """Process a section analysis task."""
        # Initialize worker tracking system if not exists
        if not hasattr(self, 'worker_performance'):
            self.worker_performance = {}

        # Create or update worker performance metrics
        if worker_id not in self.worker_performance:
            self.worker_performance[worker_id] = {
                'tasks_completed': 0,
                'total_processing_time': 0,
                'avg_processing_time': 0,
                'sections_analyzed': set(),
                'last_activity': time.time()
            }

        section_start_time = time.time()
        logger = logging.getLogger(f"IntellicrackLogger.Worker{worker_id}")
        section_name = task['params'].get('section_name', None)

        if not section_name:
            return {'error': "No section name specified"}

        # Update worker activity timestamp
        self.worker_performance[worker_id]['last_activity'] = time.time()

        try:
            # Worker-specific resource allocation based on worker_id
            # Higher priority workers get more resources
            if worker_id < 2:  # Priority workers (0, 1)
                logger.info(f"Priority worker {worker_id} analyzing section {section_name} with enhanced resources")
                # Set thread priority higher for these workers
                if hasattr(os, 'sched_setaffinity') and platform.system() == 'Linux':
                    # Pin to specific CPU cores for better performance
                    os.sched_setaffinity(0, {worker_id % os.cpu_count()})
                elif platform.system() == 'Windows':
                    # Set high priority on Windows
                    import psutil
                    p = psutil.Process()
                    p.nice(psutil.HIGH_PRIORITY_CLASS)

            pe = pefile.PE(binary_path)

            section = next((s for s in pe.sections if s.Name.decode().strip('\x00') == section_name), None)
            if not section:
                return {'error': f"Section {section_name} not found"}

            section_data = section.get_data()

            # Track section analysis by this worker
            self.worker_performance[worker_id]['sections_analyzed'].add(section_name)

            # Track active analysis tasks across all workers
            if not hasattr(self, 'active_tasks'):
                self.active_tasks = {}

            self.active_tasks[f"{worker_id}_{section_name}"] = {
                'worker_id': worker_id,
                'section': section_name,
                'start_time': section_start_time,
                'status': 'processing'
            }

            # Dynamically optimize analysis based on worker load
            num_active = len(self.active_tasks)
            analysis_depth = "full"

            if num_active > 10:  # Many active tasks, reduce analysis depth
                analysis_depth = "medium"
                logger.debug(f"Reducing analysis depth due to high load ({num_active} active tasks)")
            elif num_active > 20:  # Very high load, minimal analysis
                analysis_depth = "minimal"
                logger.debug(f"Minimal analysis due to very high load ({num_active} active tasks)")

            entropy = 0
            try:
                entropy = AdvancedVulnerabilityEngine.calculate_entropy(section_data)
            except:
                # Fallback entropy calculation
                counts = Counter(section_data)
                total = len(section_data)
                entropy = -sum((count/total) * math.log2(count/total) for count in counts.values())

            # String extraction (simple)
            strings = []
            current_string = b""
            min_length = 4  # Minimum string length

            for byte in section_data:
                if byte >= 32 and byte <= 126:  # Printable ASCII
                    current_string += bytes([byte])
                else:
                    if len(current_string) >= min_length:
                        strings.append(current_string.decode('ascii'))
                    current_string = b""

            # Add last string if needed
            if len(current_string) >= min_length:
                strings.append(current_string.decode('ascii'))

            # Log analysis results with worker information
            logger.info(f"Worker {worker_id} analyzed section {section_name}: size={len(section_data)}, entropy={entropy:.2f}, strings={len(strings)}")

            # Include worker identification in results for load balancing and diagnostics
            return {
                'section_name': section_name,
                'section_size': len(section_data),
                'entropy': entropy,
                'strings_found': len(strings),
                'strings': strings[:100],  # Limit to first 100 strings
                'characteristics': section.Characteristics,
                'virtual_address': section.VirtualAddress,
                'pointer_to_raw_data': section.PointerToRawData,
                'size_of_raw_data': section.SizeOfRawData,
                'worker_id': worker_id,  # Include worker ID for tracking work distribution
                'processing_time': time.time() - section_start_time  # Track processing efficiency
            }

        except Exception as e:
            logger.error(f"Error analyzing section {section_name}: {e}")
            return {'error': str(e), 'section_name': section_name}

    def _task_symbolic_execution(self, worker_id, task, binary_path, chunk_size):
        """Process a symbolic execution task.

        Uses angr to perform symbolic execution on the target function within the binary.
        Identifies potential vulnerabilities and explores execution paths.

        Args:
            worker_id: ID of the worker process
            task: Dictionary containing task parameters
            binary_path: Path to the binary file
            chunk_size: Size of binary chunks for distributed analysis

        Returns:
            Dictionary containing results of symbolic execution
        """
        logger = logging.getLogger(f"IntellicrackLogger.Worker{worker_id}")
        target_function = task['params'].get('target_function', None)
        max_states = task['params'].get('max_states', 100)
        max_time = task['params'].get('max_time', 300)  # 5 minutes timeout by default

        if not target_function:
            return {'error': "No target function specified"}

        logger.info(f"Starting symbolic execution of {target_function}")

        try:
            # Load the binary with angr
            proj = angr.Project(binary_path, auto_load_libs=False)

            # Get function address
            target_address = None
            try:
                # Try to resolve function by name in symbols
                for sym in proj.loader.main_object.symbols:
                    if sym.name == target_function and sym.type == 'function':
                        target_address = sym.rebased_addr
                        break

                # If not found in symbols, try CFG recovery to find functions
                if target_address is None:
                    logger.info("Function not found in symbols, recovering CFG to find function...")
                    cfg = proj.analyses.CFGFast()
                    for func in cfg.functions.values():
                        if func.name == target_function:
                            target_address = func.addr
                            break
            except Exception as e:
                logger.error(f"Error resolving function address: {str(e)}")

            if target_address is None:
                return {'error': f"Could not resolve address for function {target_function}"}

            logger.info(f"Resolved {target_function} to address 0x{target_address:x}")

            # Create a starting state at the function
            initial_state = proj.factory.call_state(target_address)

            # Create a simulation manager
            simgr = proj.factory.simulation_manager(initial_state)

            # Define vulnerability detection hooks
            vulnerabilities = []

            # Hook for detecting buffer overflow vulnerabilities
            def check_buffer_overflow(state):
                """
                Detects potential buffer overflow vulnerabilities during symbolic execution.

                This function is registered as a hook on memory write operations and checks
                if the target address is symbolic, which could indicate manipulation.

                Args:
                    state (angr.SimState): The current simulation state containing
                                           information about the memory operation

                Note:
                    Detected vulnerabilities are added to the global vulnerabilities list
                    with details about the vulnerability type, address, and severity.
                """
                if state.inspect.mem_write_address is not None:
                    # Check if writing beyond buffer boundaries
                    # This is simplified - real detection would be more complex
                    try:
                        # Check if any symbolic values in the address
                        if state.inspect.mem_write_address.symbolic:
                            # Get possible values
                            possible_values = state.solver.eval_upto(state.inspect.mem_write_address, 10)
                            if len(possible_values) > 1:
                                # Address is not concrete, could be manipulated
                                vulnerabilities.append({
                                    'type': 'buffer_overflow',
                                    'address': f"0x{state.addr:x}",
                                    'description': 'Potential buffer overflow detected - write to symbolic address',
                                    'severity': 'high'
                                })
                    except Exception as e:
                        logger.debug(f"Error in buffer overflow check: {str(e)}")

            # Register the hooks
            initial_state.inspect.b('mem_write', when=angr.BP_AFTER, action=check_buffer_overflow)

            # Track active states and adapt strategy based on count
            num_active = len(simgr.active)

            # Dynamically adjust exploration strategy based on number of active states
            if num_active > 10:
                # If we have many active states, use a more aggressive pruning strategy
                simgr.use_technique(angr.exploration_techniques.LoopSeer(bound=5))  # More aggressive loop bound
                simgr.use_technique(angr.exploration_techniques.LengthLimiter(max_length=500))  # Shorter paths
            else:
                # With fewer states, we can be more thorough
                simgr.use_technique(angr.exploration_techniques.LoopSeer(bound=10))
                simgr.use_technique(angr.exploration_techniques.LengthLimiter(max_length=1000))

            # Use Explorer to find paths to interesting points
            # For example, if there are certain addresses we want to reach or avoid
            # This would be configured based on task parameters
            target_addresses = task['params'].get('target_addresses', [])
            avoid_addresses = task['params'].get('avoid_addresses', [])

            if target_addresses:
                simgr.use_technique(angr.exploration_techniques.Explorer(
                    find=target_addresses,
                    avoid=avoid_addresses
                ))

            # Explore with timeout
            start_time = time.time()
            num_deadended = 0
            num_active = 1  # Start with 1 active state
            num_constraints_solved = 0

            # Track exploration progress
            max_states_seen = 0

            logger.info(f"Beginning path exploration (timeout: {max_time}s)")

            while simgr.active and time.time() - start_time < max_time:
                # Step the simulation manager forward
                simgr.step()

                # Update tracking metrics
                num_active = len(simgr.active)
                max_states_seen = max(max_states_seen, num_active)

                # Log progress periodically and make exploration decisions based on num_active
                if num_active > 0:
                    if num_active % 10 == 0:
                        logger.info(f"Currently exploring {num_active} active states (max seen: {max_states_seen})")

                    # Dynamically adjust exploration strategy based on number of active states
                    if num_active > 50:
                        # Too many states - switch to a more focused strategy
                        logger.info(f"Too many active states ({num_active}), pruning exploration tree")
                        # Keep only the most promising states to avoid state explosion
                        simgr.active = simgr.active[:20]  # Keep only the first 20 states
                        num_active = len(simgr.active)
                        logger.info(f"Pruned to {num_active} active states")
                    elif num_active < 3 and time.time() - start_time > max_time / 2:
                        # Very few paths but time is running out - try alternative strategies
                        logger.info(f"Few active states ({num_active}) and time running out, trying alternative exploration")
                        simgr.use_technique(angr.exploration_techniques.DFS())  # Switch to depth-first search

                # Count constraints solved
                constraint_count = 0
                for state in simgr.active:
                    state_constraints = len(state.solver.constraints)
                    constraint_count += state_constraints

                num_constraints_solved = constraint_count  # Update global constraint count

                # Update progress
                max_states_seen = max(max_states_seen, len(simgr.active))
                num_deadended = len(simgr.deadended) if hasattr(simgr, 'deadended') else 0

                # Check if we've hit max states
                if max_states_seen >= max_states:
                    logger.info(f"Reached maximum states limit ({max_states})")
                    break

                # Periodic logging
                if simgr.active and len(simgr.active) % 10 == 0:
                    logger.info(f"Active states: {len(simgr.active)}, Deadended: {num_deadended}")

            total_time = time.time() - start_time

            # Perform vulnerability analysis on each found path
            for state in simgr.deadended + simgr.active:
                # Check for integer overflow
                for expr in state.solver.constraints:
                    if any(op in str(expr) for op in ['__add__', '__mul__']):
                        try:
                            # Check if there are constraints that could cause overflow
                            for var in expr.variables:
                                var_name = var.decode('utf-8') if isinstance(var, bytes) else str(var)
                                if 'int' in var_name.lower():
                                    max_val = state.solver.max(expr)
                                    min_val = state.solver.min(expr)

                                    # Check for potential overflows (simplified check)
                                    if max_val > 2**31 - 1 or min_val < -2**31:
                                        vulnerabilities.append({
                                            'type': 'integer_overflow',
                                            'address': f"0x{state.addr:x}",
                                            'expression': str(expr),
                                            'description': 'Potential integer overflow detected',
                                            'severity': 'medium'
                                        })
                        except Exception as e:
                            logger.debug(f"Error checking for integer overflow: {str(e)}")

            # Categorize vulnerabilities
            vuln_by_type = {}
            for vuln in vulnerabilities:
                vuln_type = vuln['type']
                if vuln_type not in vuln_by_type:
                    vuln_by_type[vuln_type] = []
                vuln_by_type[vuln_type].append(vuln)

            # Prepare result
            result = {
                'target_function': target_function,
                'target_address': f"0x{target_address:x}",
                'paths_explored': num_deadended + len(simgr.active),
                'constraints_solved': num_constraints_solved,
                'max_active_states': max_states_seen,
                'execution_time': total_time,
                'vulnerabilities_found': len(vulnerabilities),
                'vulnerabilities': vulnerabilities,
                'vulnerability_summary': {vtype: len(vulns) for vtype, vulns in vuln_by_type.items()}
            }

            logger.info(f"Symbolic execution completed: {result['paths_explored']} paths explored, "
                        f"{result['vulnerabilities_found']} vulnerabilities found")

            return result

        except Exception as e:
            error_msg = f"Error during symbolic execution: {str(e)}"
            logger.error(error_msg)
            logger.error(traceback.format_exc())
            return {
                'error': error_msg,
                'target_function': target_function,
                'paths_explored': 0,
                'constraints_solved': 0,
                'vulnerabilities_found': 0
            }

    def _task_generic(self, worker_id, task, binary_path, chunk_size):
        """Process a generic task."""
        logger = logging.getLogger(f"IntellicrackLogger.Worker{worker_id}")
        chunk_start = task['params'].get('chunk_start', 0)

        # Read chunk of file
        with open(binary_path, 'rb') as f:
            f.seek(chunk_start)
            chunk_data = f.read(chunk_size)

        # Priority workers (lower IDs) might process chunks more thoroughly
        processing_level = "standard"
        if worker_id < 3:  # Workers 0, 1, 2 are priority workers
            processing_level = "deep"
            # Perform additional analysis on the chunk for priority workers
            entropy = sum(byte / 255.0 * math.log2(1.0 / (byte / 255.0))
                        for byte in chunk_data if byte > 0) / len(chunk_data) if len(chunk_data) > 0 else 0

        logger.info(f"Worker {worker_id} processed task on chunk at offset {chunk_start}")
        return {
            'worker_id': worker_id,  # Include worker ID in results
            'chunk_offset': chunk_start,
            'chunk_size': len(chunk_data),
            'task_type': task['type'],
            'processing_level': processing_level,
            'entropy': entropy if processing_level == "deep" else None,  # Include entropy for deep analysis
            'worker_load': self.worker_loads.get(worker_id, 1.0) if hasattr(self, 'worker_loads') else 1.0
        }

    def collect_results(self, timeout=None):
        """
        Collect results from workers.

        Args:
            timeout: Timeout in seconds (None for no timeout)

        Returns:
            bool: True if results collected successfully, False otherwise
        """
        if not self.running:
            self.logger.warning("Not running")
            return False

        try:
            # Initialize results
            self.results = {
                'tasks_completed': 0,
                'tasks_failed': 0,
                'total_processing_time': 0.0,
                'task_results': {}
            }

            # Collect results
            tasks_remaining = len(self.tasks)
            start_time = time.time()

            while tasks_remaining > 0:
                # Check timeout
                if timeout is not None and time.time() - start_time > timeout:
                    self.logger.warning(f"Timeout after {timeout} seconds")
                    break

                # Get result from queue
                try:
                    worker_id, task, result = self.result_queue.get(timeout=1.0)
                except queue.Empty:
                    # Check if all workers are still alive
                    if not any(worker.is_alive() for worker in self.workers):
                        self.logger.error("All workers have died")
                        break
                    continue

                # Process result
                task_type = task['type']
                self.logger.info(f"Processing result from worker {worker_id} for task {task_type}")

                # Initialize task type in results if not already present
                if task_type not in self.results['task_results']:
                    self.results['task_results'][task_type] = []

                # Add result to results
                self.results['task_results'][task_type].append(result)

                # Update statistics
                if result.get('success', False):
                    self.results['tasks_completed'] += 1
                else:
                    self.results['tasks_failed'] += 1

                self.results['total_processing_time'] += result.get('processing_time', 0.0)

                # Log progress
                total_tasks = self.results['tasks_completed'] + self.results['tasks_failed']
                self.logger.info(f"Progress: {total_tasks}/{len(self.tasks)} tasks processed")

                # Decrement tasks remaining
                tasks_remaining -= 1

            # Wait for workers to finish
            for worker in self.workers:
                worker.join(timeout=1.0)

            self.running = False
            self.logger.info("Collected results")

            return True

        except Exception as e:
            self.logger.error(f"Error collecting results: {e}")
            return False

    def stop_processing(self):
        """
        Stop distributed processing.

        Returns:
            bool: True if processing stopped successfully, False otherwise
        """
        if not self.running:
            return True

        try:
            # Terminate workers
            for worker in self.workers:
                worker.terminate()

            # Wait for workers to terminate
            for worker in self.workers:
                worker.join(timeout=1.0)

            # Clear queues
            while not self.task_queue.empty():
                self.task_queue.get()

            while not self.result_queue.empty():
                self.result_queue.get()

            self.running = False
            self.logger.info("Stopped processing")

            return True

        except Exception as e:
            self.logger.error(f"Error stopping processing: {e}")
            return False

    def get_results(self):
        """
        Get the distributed processing results.

        Returns:
            dict: Processing results
        """
        return self.results

    # === Convenience Methods for Common Analysis Tasks ===

    def run_distributed_pattern_search(self, patterns, chunk_size_mb=10):
        """
        Search for patterns in a binary file using distributed processing.

        Args:
            patterns: List of patterns to search for (bytes or regex strings)
            chunk_size_mb: Size of each chunk in MB

        Returns:
            list: List of matches with their positions
        """
        if not self.binary_path:
            self.logger.error("No binary set")
            return None

        # Calculate chunk size
        chunk_size = chunk_size_mb * 1024 * 1024

        # Get file size
        file_size = os.path.getsize(self.binary_path)

        # Add tasks for each chunk
        self.tasks = []
        for offset in range(0, file_size, chunk_size):
            task = {
                'id': len(self.tasks),
                'type': 'find_patterns',
                'params': {
                    'patterns': patterns,
                    'chunk_start': offset,
                    'chunk_end': min(offset + chunk_size, file_size)
                },
                'description': f"Pattern search in chunk at offset {offset}"
            }
            self.tasks.append(task)

        # Start processing
        self.start_processing()

        # Collect results
        self.collect_results()

        # Process and combine results
        all_matches = []
        if 'find_patterns' in self.results['task_results']:
            for result in self.results['task_results']['find_patterns']:
                if result.get('success', False) and 'matches' in result:
                    all_matches.extend(result['matches'])

        # Sort by position
        all_matches.sort(key=lambda x: x['position'])

        return all_matches

    def run_distributed_entropy_analysis(self, window_size_kb=64, chunk_size_mb=10):
        """
        Calculate entropy of a binary file using distributed processing.

        Args:
            window_size_kb: Size of sliding window in KB
            chunk_size_mb: Size of each chunk in MB

        Returns:
            dict: Entropy analysis results
        """
        if not self.binary_path:
            self.logger.error("No binary set")
            return None

        # Calculate sizes
        window_size = window_size_kb * 1024
        chunk_size = chunk_size_mb * 1024 * 1024

        # Get file size
        file_size = os.path.getsize(self.binary_path)

        # Add tasks for each chunk
        self.tasks = []
        for offset in range(0, file_size, chunk_size):
            task = {
                'id': len(self.tasks),
                'type': 'analyze_entropy',
                'params': {
                    'window_size': window_size,
                    'chunk_start': offset,
                    'chunk_end': min(offset + chunk_size, file_size)
                },
                'description': f"Entropy analysis of chunk at offset {offset}"
            }
            self.tasks.append(task)

        # Start processing
        self.start_processing()

        # Collect results
        self.collect_results()

        # Process and combine results
        all_windows = []
        chunk_entropies = []

        if 'analyze_entropy' in self.results['task_results']:
            for result in self.results['task_results']['analyze_entropy']:
                if result.get('success', False):
                    chunk_entropies.append((result['chunk_entropy'], result['chunk_size']))
