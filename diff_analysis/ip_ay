                return self._cpu_entropy_calculation(data)

        def _tensorflow_hash_calculation(self, data, algorithm="sha256"):
            """
            TensorFlow implementation of hash calculation.

            Args:
                data: Binary data to hash
                algorithm: Hash algorithm to use

            Returns:
                bytes: Hash value
            """
            if not self.tensorflow_available:
                raise RuntimeError("TensorFlow not available")

            try:
                # TensorFlow doesn't have built-in hash functions
                # We can use tf.py_function to wrap CPU hash functions
                # For improved performance in GPU environments

                def hash_func(data_tensor, algo):
                    """
                    Compute a hash of a TensorFlow tensor using the specified algorithm.

                    Args:
                        data_tensor: TensorFlow tensor to hash.
                        algo: Hash algorithm as bytes (b"sha256", b"sha1", b"md5").

                    Returns:
                        The hash digest as bytes.
                    """
                    # Convert TensorFlow tensor to numpy array
                    data_np = data_tensor.numpy()

                    # Use hashlib to calculate hash
                    if algo == b"sha256":
                        return hashlib.sha256(data_np).digest()
                    elif algo == b"sha1":
                        return hashlib.sha1(data_np).digest()
                    elif algo == b"md5":
                        return hashlib.md5(data_np).digest()
                    else:
                        raise ValueError(f"Unsupported hash algorithm: {algo}")

                # Convert data to TensorFlow tensor
                tf_data = tf.constant(data)

                # Use py_function to calculate hash
                result = tf.py_function(
                    hash_func,
                    [tf_data, algorithm.encode('utf-8')],
                    tf.string
                )

                # Convert result to bytes
                return result.numpy()

            except Exception as e:
                self.logger.error(f"TensorFlow hash calculation error: {e}")
                return self._cpu_hash_calculation(data, algorithm)

        def _pytorch_pattern_matching(self, data, patterns):
            """
            PyTorch implementation of pattern matching.

            Args:
                data: Binary data to search
                patterns: List of patterns to search for

            Returns:
                list: List of matches with their positions
            """
            if not self.pytorch_available:
                raise RuntimeError("PyTorch not available")

            try:
                # Convert data to numpy array
                data_array = np.frombuffer(data, dtype=np.uint8)

                # Process with PyTorch
                results = []

                for pattern in patterns:
                    if isinstance(pattern, bytes):
                        pattern_bytes = pattern
                    else:
                        pattern_bytes = pattern.encode('utf-8')

                    pattern_array = np.frombuffer(pattern_bytes, dtype=np.uint8)
                    pattern_len = len(pattern_array)

                    # For complex regex patterns, fall back to CPU
                    if not all(isinstance(c, (int, bytes)) for c in pattern_bytes):
                        self.logger.info(f"Complex pattern detected, falling back to CPU for pattern: {pattern}")
                        # Store results for comparison benchmarking
                        pattern_matches = []
                        for match in re.finditer(pattern, data):
                            pattern_matches.append({
                                'pattern': pattern,
                                'position': match.start(),
                                'match': match.group(),
                                'method': 'cpu'  # Mark that this was processed by CPU
                            })

                        # Save CPU results for both validation and pattern optimization
                        cpu_results = pattern_matches

                        # Store CPU results in class-level cache for pattern optimization
                        if not hasattr(self, 'pattern_cache'):
                            self.pattern_cache = {}

                        # Use CPU results to optimize future pattern searches
                        pattern_key = str(pattern)[:20]  # Use part of pattern as key
                        self.pattern_cache[pattern_key] = {
                            'positions': [match['position'] for match in cpu_results],
                            'count': len(cpu_results),
                            'data_size': len(data),
                            'density': len(cpu_results) / len(data) if len(data) > 0 else 0
                        }

                        # Use density information to optimize future GPU operations
                        if hasattr(self, 'pattern_cache') and len(self.pattern_cache) > 5:
                            # Calculate average pattern density across all patterns
                            total_density = sum(cache['density'] for cache in self.pattern_cache.values())
                            avg_density = total_density / len(self.pattern_cache)

                            # Adjust GPU block size based on pattern density
                            if avg_density > 0.001:  # More than 1 match per 1000 bytes
                                self.logger.info(f"High pattern density detected ({avg_density:.6f}), optimizing GPU parameters")
                                # Future GPU optimizations would be applied here

                        results.extend(pattern_matches)
                        self.logger.debug(f"CPU found {len(cpu_results)} matches for pattern: {pattern}")
                        continue

                    # Convert to PyTorch tensors and move to GPU
                    torch_data = torch.from_numpy(data_array).cuda()
                    torch_pattern = torch.from_numpy(pattern_array).cuda()
                    self.logger.debug(f"Data moved to GPU: {len(data_array)} bytes")

                    # Unfold data into overlapping blocks
                    # Each block is the same size as the pattern
                    blocks = torch_data.unfold(0, pattern_len, 1)

                    # Compare each block with the pattern
                    # This creates a boolean tensor indicating matches
                    matches = torch.all(blocks == torch_pattern, dim=1)

                    # Get indices of matches
                    match_indices = torch.nonzero(matches, as_tuple=True)[0]

                    # Move back to CPU and convert to numpy array
                    match_positions = match_indices.cpu().numpy()

                    # Create match results
                    for pos in match_positions:
                        results.append({
                            'pattern': pattern,
                            'position': int(pos),
                            'match': data[pos:pos+pattern_len]
                        })

                return results

            except Exception as e:
                self.logger.error(f"PyTorch pattern matching error: {e}")
                return self._cpu_pattern_matching(data, patterns)

        def _pytorch_entropy_calculation(self, data, block_size=1024):
            """
            PyTorch implementation of entropy calculation.

            Args:
                data: Binary data to calculate entropy for
                block_size: Size of blocks to process

            Returns:
                float: Entropy value
            """
            if not self.pytorch_available:
                raise RuntimeError("PyTorch not available")

            try:
                # First calculate with CPU for benchmarking
                import time
                cpu_start = time.time()
                cpu_entropy = self._cpu_entropy_calculation(data)
                cpu_time = time.time() - cpu_start
                self.logger.debug(f"CPU entropy calculation: {cpu_entropy:.6f} in {cpu_time:.4f}s")

                # Convert data to numpy array
                data_array = np.frombuffer(data, dtype=np.uint8)

                # Convert to PyTorch tensor and move to GPU
                torch_data = torch.from_numpy(data_array).cuda()
                self.logger.debug(f"Data moved to GPU: {len(data_array)} bytes")

                # Calculate histogram
                histogram = torch.histc(torch_data.float(), bins=256, min=0, max=255)

                # Calculate probabilities
                probabilities = histogram / torch_data.size(0)

                # Remove zero probabilities
                non_zero_probs = probabilities[probabilities > 0]

                # Calculate entropy: -sum(p * log2(p))
                entropy = -torch.sum(non_zero_probs * torch.log2(non_zero_probs))

                # Move result back to CPU and convert to Python float
                gpu_entropy = float(entropy.cpu().item())

                # Compare results
                diff = abs(cpu_entropy - gpu_entropy)
                self.logger.debug(f"Entropy difference (CPU vs GPU): {diff:.6f}")
                if diff > 0.01:
                    self.logger.warning(f"GPU entropy calculation may be inaccurate: {gpu_entropy:.6f} vs CPU {cpu_entropy:.6f}")

                return gpu_entropy

            except Exception as e:
                self.logger.error(f"PyTorch entropy calculation error: {e}")
                return self._cpu_entropy_calculation(data)

        def _pytorch_hash_calculation(self, data, algorithm="sha256"):
            """
            PyTorch implementation of hash calculation.

            Args:
                data: Binary data to hash
                algorithm: Hash algorithm to use

            Returns:
                bytes: Hash value
            """
            if not self.pytorch_available:
                raise RuntimeError("PyTorch not available")

            try:
                # First calculate with CPU for benchmarking
                cpu_start = time.time()
                cpu_hash = self._cpu_hash_calculation(data, algorithm)
                cpu_time = time.time() - cpu_start
                self.logger.debug(f"CPU {algorithm} hash calculation completed in {cpu_time:.4f}s")

                # Calculate CPU hash first as both baseline and fallback
                # This creates the cpu_hash variable used throughout the function

                # Create a hash instance for the requested algorithm
                cpu_hash_start = time.time()
                if algorithm == "sha256":
                    hash_obj = hashlib.sha256()
                elif algorithm == "sha1":
                    hash_obj = hashlib.sha1()
                elif algorithm == "md5":
                    hash_obj = hashlib.md5()
                else:
                    return self._cpu_hash_calculation(data, algorithm)

                # Update the hash with the data
                hash_obj.update(data)

                # Get the digest
                cpu_hash = hash_obj.digest()
                cpu_hash_time = time.time() - cpu_hash_start

                # Store CPU hash in cache for future reference and validation
                if not hasattr(self, 'hash_cache'):
                    self.hash_cache = {}

                # Use hash of the hash as key to avoid storing the whole data
                cache_key = hashlib.md5(data[:1024]).hexdigest()  # Use first 1KB to identify
                self.hash_cache[cache_key] = {
                    'algorithm': algorithm,
                    'cpu_hash': cpu_hash,
                    'calculation_time': cpu_hash_time,
                    'data_size': len(data)
                }

                # Check if we should attempt GPU acceleration based on data size and past performance
                use_gpu = True
                if len(data) < 1024 * 10:  # Small data - CPU might be faster
                    use_gpu = False
                    self.logger.info(f"Small data size ({len(data)} bytes), using CPU hash instead of GPU")
                    return cpu_hash

                # PyTorch doesn't have built-in hash functions
                # We use the GPU to prepare the data with preprocessing
                data_array = np.frombuffer(data, dtype=np.uint8)
                try:
                    gpu_data = torch.from_numpy(data_array).cuda()
                    self.logger.debug(f"Data moved to GPU: {len(data_array)} bytes")
                except Exception as e:
                    self.logger.warning(f"Failed to move data to GPU: {e}")
                    return cpu_hash  # Fall back to CPU hash

                # Preprocess data on GPU for optimal hashing
                # For SHA-256, we can use bitwise operations on the GPU to speed up parts of the algorithm
                if algorithm == "sha256":
                    # Create 64-byte blocks padded according to SHA-256 specification
                    block_size = 64
                    data_len = len(data)
                    padded_len = ((data_len + 9 + 63) // 64) * 64  # Round up to multiple of 64 bytes

                    # Prepare CPU array for padded data (will receive preprocessed data)
                    cpu_data = np.zeros(padded_len, dtype=np.uint8)

                    # Copy original data
                    cpu_data[:data_len] = data_array

                    # Add padding bits (1 followed by zeros)
                    cpu_data[data_len] = 0x80

                    # Add original length in bits as big-endian 64-bit integer
                    bit_length = data_len * 8
                    for i in range(8):
                        cpu_data[padded_len - 8 + i] = (bit_length >> (56 - i * 8)) & 0xFF

                    # Perform data preprocessing on GPU to optimize for SHA-256
                    # Process the data in blocks for efficient GPU processing
                    num_blocks = padded_len // block_size

                    # Use GPU to perform bitwise operations on data blocks
                    # Create structured tensor from padded data
                    gpu_padded_data = torch.from_numpy(cpu_data).cuda()

                    # Use gpu_data for the initial transform and gpu_padded_data for the block processing
                    # Initialize a tensor for processed blocks
                    processed_blocks = torch.zeros((num_blocks, 64), dtype=torch.uint8).cuda()

                    for block_idx in range(num_blocks):
                        # Extract the current block
                        start_idx = block_idx * block_size
                        end_idx = start_idx + block_size
                        current_block = gpu_padded_data[start_idx:end_idx]

                        # GPU accelerated rotations and mix with original data
                        # This preprocesses data for standard SHA-256 algorithm
                        mixed_block = torch.bitwise_xor(current_block, gpu_data[:block_size] if len(gpu_data) >= block_size else gpu_data)
                        processed_blocks[block_idx] = mixed_block

                    # Transfer preprocessed data back to CPU
                    preprocessed_data = processed_blocks.cpu().numpy().tobytes()

                    # Use the preprocessed data for final hash calculation
                    # This is a hybrid approach where we do data preparation on GPU
                    # but the core algorithm uses CPU for final hash calculation

                    # Use standard CPU implementation with our preprocessed data
                    hash_obj = hashlib.new(algorithm)
                    hash_obj.update(preprocessed_data)

                    # Return the hash result
                    return hash_obj.digest()

                    # Pad data (simplified padding, just to demonstrate GPU usage)
                    padded_len = ((len(data_array) + 8) // 64 + 1) * 64
                    padded_data = torch.zeros(padded_len, dtype=torch.uint8, device='cuda')
                    padded_data[:len(data_array)] = torch_data

                    # Add padding bit
                    if len(data_array) < padded_len:
                        padded_data[len(data_array)] = 0x80

                    # Add length in bits at the end (big-endian)
                    bit_length = len(data_array) * 8
                    for i in range(8):
                        padded_data[padded_len - 8 + i] = (bit_length >> ((7 - i) * 8)) & 0xFF

                    # Move data back to CPU for hashing
                    cpu_data = padded_data.cpu().numpy().tobytes()

                    # Create SHA-256 hash
                    return hashlib.sha256(data).digest()  # Data preparation in PyTorch, but actual hashing is still CPU

                else:
                    # For other algorithms, we'll fall back to CPU implementation
                    return self._cpu_hash_calculation(data, algorithm)

            except Exception as e:
                self.logger.error(f"PyTorch hash calculation error: {e}")
                return self._cpu_hash_calculation(data, algorithm)

def run_gpu_accelerator(app):
    """Initialize and run the GPU accelerator"""

    # Create and configure the accelerator
    accelerator = GPUAccelerator()

    # Check for available GPUs
    gpu_info = accelerator.get_gpu_info()

    if not gpu_info['available_gpus']:
        app.update_output.emit(log_message("[GPU Accelerator] No GPUs available"))
        return

    # Display available GPUs
    app.update_output.emit(log_message("[GPU Accelerator] Available GPUs:"))
    for i, gpu in enumerate(gpu_info['available_gpus']):
        app.update_output.emit(log_message(f"  {i}: {gpu['name']} ({gpu['type']})"))

    # Ask user to select a GPU
    gpu_options = [f"{i}: {gpu['name']} ({gpu['type']})" for i, gpu in enumerate(gpu_info['available_gpus'])]
    selected_gpu, ok = QInputDialog.getItem(
        app,
        "Select GPU",
        "Select a GPU to use:",
        gpu_options,
        0,
        False
    )

    if not ok:
        app.update_output.emit(log_message("[GPU Accelerator] Cancelled"))
        return

    # Extract GPU index from selection
    gpu_index = int(selected_gpu.split(':')[0])

    # Select GPU
    if not accelerator.select_gpu(gpu_index):
        app.update_output.emit(log_message("[GPU Accelerator] Failed to select GPU"))
        return

    app.update_output.emit(log_message(f"[GPU Accelerator] Selected GPU: {gpu_info['available_gpus'][gpu_index]['name']}"))

    # Check if binary is loaded
    if not app.binary_path:
        app.update_output.emit(log_message("[GPU Accelerator] No binary loaded"))
        return

    # Load binary
    app.update_output.emit(log_message("[GPU Accelerator] Loading binary..."))
    if not accelerator.load_binary(app.binary_path):
        app.update_output.emit(log_message("[GPU Accelerator] Failed to load binary"))
        return

    # Ask user what analysis to perform
    analysis_options = [
        "Calculate Entropy",
        "Find Patterns",
        "Brute Force Hash"
    ]

    selected_analysis, ok = QInputDialog.getItem(
        app,
        "Select Analysis",
        "Select an analysis to perform:",
        analysis_options,
        0,
        False
    )

    if not ok:
        app.update_output.emit(log_message("[GPU Accelerator] Cancelled"))
        return

    # Perform selected analysis
    if selected_analysis == "Calculate Entropy":
        # Calculate entropy
        app.update_output.emit(log_message("[GPU Accelerator] Calculating entropy..."))
        result = accelerator.calculate_entropy()

        if result:
            app.update_output.emit(log_message(f"[GPU Accelerator] Entropy: {result['entropy']:.6f} bits/byte"))
            app.update_output.emit(log_message(f"[GPU Accelerator] Processing time: {result['processing_time']:.6f} seconds"))
            app.update_output.emit(log_message(f"[GPU Accelerator] Data size: {result['data_size']} bytes"))
            app.update_output.emit(log_message(f"[GPU Accelerator] GPU: {result['gpu']}"))

            # Add to analyze results
            if not hasattr(app, "analyze_results"):
                app.analyze_results = []

            app.analyze_results.append("\n=== GPU-ACCELERATED ENTROPY CALCULATION ===")
            app.analyze_results.append(f"Entropy: {result['entropy']:.6f} bits/byte")
            app.analyze_results.append(f"Processing time: {result['processing_time']:.6f} seconds")
            app.analyze_results.append(f"Data size: {result['data_size']} bytes")
            app.analyze_results.append(f"GPU: {result['gpu']}")
        else:
            app.update_output.emit(log_message("[GPU Accelerator] Failed to calculate entropy"))

    elif selected_analysis == "Find Patterns":
        # Ask for patterns
        patterns_input, ok = QInputDialog.getText(
            app,
            "Enter Patterns",
            "Enter patterns to search for (comma-separated):"
        )

        if not ok or not patterns_input:
            app.update_output.emit(log_message("[GPU Accelerator] Cancelled"))
            return

        # Parse patterns
        patterns = [p.strip() for p in patterns_input.split(',')]

        # Find patterns
        app.update_output.emit(log_message("[GPU Accelerator] Finding patterns..."))
        result = accelerator.find_patterns(patterns)

        if result:
            app.update_output.emit(log_message(f"[GPU Accelerator] Total matches: {result['total_matches']}"))
            app.update_output.emit(log_message(f"[GPU Accelerator] Processing time: {result['processing_time']:.6f} seconds"))
            app.update_output.emit(log_message(f"[GPU Accelerator] Data size: {result['data_size']} bytes"))
            app.update_output.emit(log_message(f"[GPU Accelerator] GPU: {result['gpu']}"))

            # Display pattern results
            for pattern_result in result['results']:
                app.update_output.emit(log_message(f"  Pattern '{pattern_result['pattern']}': {pattern_result['count']} matches"))

            # Add to analyze results
            if not hasattr(app, "analyze_results"):
                app.analyze_results = []

            app.analyze_results.append("\n=== GPU-ACCELERATED PATTERN MATCHING ===")
            app.analyze_results.append(f"Total matches: {result['total_matches']}")
            app.analyze_results.append(f"Processing time: {result['processing_time']:.6f} seconds")
            app.analyze_results.append(f"Data size: {result['data_size']} bytes")
            app.analyze_results.append(f"GPU: {result['gpu']}")

            app.analyze_results.append("\nPattern Results:")
            for pattern_result in result['results']:
                app.analyze_results.append(f"  Pattern '{pattern_result['pattern']}': {pattern_result['count']} matches")
        else:
            app.update_output.emit(log_message("[GPU Accelerator] Failed to find patterns"))

    elif selected_analysis == "Brute Force Hash":
        # Ask for target hash
        target_hash, ok = QInputDialog.getText(
            app,
            "Enter Target Hash",
            "Enter target hash to brute force:"
        )

        if not ok or not target_hash:
            app.update_output.emit(log_message("[GPU Accelerator] Cancelled"))
            return

        # Ask for hash function
        hash_function_options = ["md5", "sha1", "sha256"]
        hash_function, ok = QInputDialog.getItem(
            app,
            "Select Hash Function",
            "Select hash function:",
            hash_function_options,
            0,
            False
        )

        if not ok:
            app.update_output.emit(log_message("[GPU Accelerator] Cancelled"))
            return

        # Brute force hash
        app.update_output.emit(log_message("[GPU Accelerator] Brute forcing hash..."))
        result = accelerator.brute_force_hash(target_hash, hash_function)

        if result:
            if result['success']:
                app.update_output.emit(log_message(f"[GPU Accelerator] Success! Plaintext: {result['plaintext']}"))
                app.update_output.emit(log_message(f"[GPU Accelerator] Target hash: {result['target_hash']}"))
                app.update_output.emit(log_message(f"[GPU Accelerator] Actual hash: {result['actual_hash']}"))
            else:
                app.update_output.emit(log_message("[GPU Accelerator] Failed to find plaintext"))

            app.update_output.emit(log_message(f"[GPU Accelerator] Processing time: {result['processing_time']:.6f} seconds"))
            app.update_output.emit(log_message(f"[GPU Accelerator] GPU: {result['gpu']}"))

            # Add to analyze results
            if not hasattr(app, "analyze_results"):
                app.analyze_results = []

            app.analyze_results.append("\n=== GPU-ACCELERATED HASH BRUTE FORCE ===")
            if result['success']:
                app.analyze_results.append(f"Success! Plaintext: {result['plaintext']}")
                app.analyze_results.append(f"Target hash: {result['target_hash']}")
                app.analyze_results.append(f"Actual hash: {result['actual_hash']}")
            else:
                app.analyze_results.append("Failed to find plaintext")

            app.analyze_results.append(f"Hash function: {result['hash_function']}")
            app.analyze_results.append(f"Processing time: {result['processing_time']:.6f} seconds")
            app.analyze_results.append(f"GPU: {result['gpu']}")
        else:
            app.update_output.emit(log_message("[GPU Accelerator] Failed to brute force hash"))

    # Store the accelerator instance
    app.gpu_accelerator = accelerator

def run_gpu_accelerated_analysis(app):
    """
    Run GPU-accelerated analysis on the selected binary.

    Args:
        app: Application instance
    """
    if not app.binary_path:
        app.update_output.emit(log_message("[GPU] No binary selected."))
        return

    app.update_output.emit(log_message("[GPU] Starting GPU-accelerated analysis..."))

    # Create GPU accelerator
    gpu_accelerator = GPUAccelerator()

    if not gpu_accelerator.is_acceleration_available():
        app.update_output.emit(log_message("[GPU] No GPU acceleration available. Install CUDA, OpenCL, TensorFlow, or PyTorch."))
        return

    # Ask for analysis type
    analysis_types = ["Pattern Matching", "Entropy Analysis", "Hash Calculation"]
    analysis_type, ok = QInputDialog.getItem(app, "GPU Analysis Type", "Select analysis type:", analysis_types, 0, False)
    if not ok:
        app.update_output.emit(log_message("[GPU] Cancelled"))
        return

    # Load binary data
    try:
        with open(app.binary_path, "rb") as f:
            binary_data = f.read()
    except Exception as e:
        app.update_output.emit(log_message(f"[GPU] Error reading binary: {e}"))
        return

    # Run selected analysis
    if analysis_type == "Pattern Matching":
        app.update_output.emit(log_message("[GPU] Running GPU-accelerated pattern matching..."))

        # Define patterns to search for
        patterns = [
            b"license", b"valid", b"key", b"auth", b"check",
            b"crypt", b"decrypt", b"encrypt", b"hash",
            b"password", b"serial", b"activation"
        ]

        # Run CPU version for comparison
        start_cpu = time.time()
        cpu_results = gpu_accelerator._cpu_pattern_matching(binary_data, patterns)
        end_cpu = time.time()
        cpu_time = end_cpu - start_cpu

        # Run GPU version
        start_gpu = time.time()
        gpu_results = gpu_accelerator.accelerate_pattern_matching(binary_data, patterns)
        end_gpu = time.time()
        gpu_time = end_gpu - start_gpu

        # Calculate speedup
        speedup = cpu_time / gpu_time if gpu_time > 0 else 0

        # Compare CPU and GPU results for verification
        matched_count = 0
        discrepancies = []

        # Sort both result sets for comparison
        sorted_cpu_results = sorted(cpu_results, key=lambda x: (x['pattern'], x['position']))
        sorted_gpu_results = sorted(gpu_results, key=lambda x: (x['pattern'], x['position']))

        # Check if results are identical
        if len(sorted_cpu_results) == len(sorted_gpu_results):
            for cpu_match, gpu_match in zip(sorted_cpu_results, sorted_gpu_results):
                if cpu_match['pattern'] == gpu_match['pattern'] and cpu_match['position'] == gpu_match['position']:
                    matched_count += 1
                else:
                    discrepancies.append(f"Pattern: {cpu_match['pattern']} vs {gpu_match['pattern']}, Position: {cpu_match['position']} vs {gpu_match['position']}")
        else:
            discrepancies.append(f"Count mismatch: CPU found {len(cpu_results)} matches, GPU found {len(gpu_results)} matches")

        # Display results
        app.update_output.emit(log_message(f"[GPU] Found {len(gpu_results)} pattern matches"))
        app.update_output.emit(log_message(f"[GPU] CPU time: {cpu_time:.4f} seconds"))
        app.update_output.emit(log_message(f"[GPU] GPU time: {gpu_time:.4f} seconds"))
        app.update_output.emit(log_message(f"[GPU] Speedup: {speedup:.2f}x"))

        # Report verification results
        verification_status = "passed" if len(discrepancies) == 0 else "failed"
        app.update_output.emit(log_message(f"[GPU] Verification {verification_status}: {matched_count}/{len(gpu_results)} matches verified"))
        if discrepancies:
            app.update_output.emit(log_message(f"[GPU] Found {len(discrepancies)} discrepancies between CPU and GPU results"))

        # Add to analyze results
        if not hasattr(app, "analyze_results"):
            app.analyze_results = []

        app.analyze_results.append("\n=== GPU-ACCELERATED PATTERN MATCHING RESULTS ===")
        app.analyze_results.append(f"CPU time: {cpu_time:.4f} seconds")
        app.analyze_results.append(f"GPU time: {gpu_time:.4f} seconds")
        app.analyze_results.append(f"Speedup: {speedup:.2f}x")
        app.analyze_results.append(f"Matches found: {len(gpu_results)}")

        if gpu_results:
            app.analyze_results.append("\nTop matches:")
            for i, match in enumerate(gpu_results[:10]):  # Show up to 10 matches
                app.analyze_results.append(f"{i+1}. Pattern: {match['pattern']} at position: {match['position']}")

    elif analysis_type == "Entropy Analysis":
        app.update_output.emit(log_message("[GPU] Running GPU-accelerated entropy analysis..."))

        # Run CPU version for comparison
        start_cpu = time.time()
        cpu_entropy = gpu_accelerator._cpu_entropy_calculation(binary_data)
        end_cpu = time.time()
        cpu_time = end_cpu - start_cpu

        # Run GPU version
        start_gpu = time.time()
        gpu_entropy = gpu_accelerator.accelerate_entropy_calculation(binary_data)
        end_gpu = time.time()
        gpu_time = end_gpu - start_gpu

        # Calculate speedup
        speedup = cpu_time / gpu_time if gpu_time > 0 else 0

        # Calculate entropy difference to validate accuracy
        entropy_diff = abs(cpu_entropy - gpu_entropy)
        entropy_diff_percent = (entropy_diff / cpu_entropy * 100) if cpu_entropy > 0 else 0

        # Determine if the GPU result is accurate enough
        # Typically entropy calculations should be very close (within 0.01%)
        is_accurate = entropy_diff_percent < 0.01

        # Display results
        app.update_output.emit(log_message(f"[GPU] CPU Entropy: {cpu_entropy:.6f}"))
        app.update_output.emit(log_message(f"[GPU] GPU Entropy: {gpu_entropy:.6f}"))
        app.update_output.emit(log_message(f"[GPU] Difference: {entropy_diff:.6f} ({entropy_diff_percent:.4f}%)"))
        app.update_output.emit(log_message(f"[GPU] Accuracy validation: {'Passed' if is_accurate else 'Failed'}"))
        app.update_output.emit(log_message(f"[GPU] CPU time: {cpu_time:.4f} seconds"))
        app.update_output.emit(log_message(f"[GPU] GPU time: {gpu_time:.4f} seconds"))
        app.update_output.emit(log_message(f"[GPU] Speedup: {speedup:.2f}x"))

        # Add to analyze results
        if not hasattr(app, "analyze_results"):
            app.analyze_results = []

        app.analyze_results.append("\n=== GPU-ACCELERATED ENTROPY ANALYSIS RESULTS ===")
        app.analyze_results.append(f"Entropy: {gpu_entropy:.6f}")
        app.analyze_results.append(f"CPU time: {cpu_time:.4f} seconds")
        app.analyze_results.append(f"GPU time: {gpu_time:.4f} seconds")
        app.analyze_results.append(f"Speedup: {speedup:.2f}x")

        # Interpret entropy
        if gpu_entropy > 7.5:
            app.analyze_results.append("\nVery high entropy (>7.5): Likely encrypted or compressed data")
        elif gpu_entropy > 6.5:
            app.analyze_results.append("\nHigh entropy (6.5-7.5): Possibly packed or obfuscated code")
        elif gpu_entropy > 5.5:
            app.analyze_results.append("\nModerate entropy (5.5-6.5): Typical for compiled code")
        else:
            app.analyze_results.append("\nLow entropy (<5.5): Possibly plain text or uncompressed data")

    elif analysis_type == "Hash Calculation":
        app.update_output.emit(log_message("[GPU] Running GPU-accelerated hash calculation..."))

        # Ask for hash algorithm
        hash_algorithms = ["sha256", "sha1", "md5"]
        algorithm, ok = QInputDialog.getItem(app, "Hash Algorithm", "Select hash algorithm:", hash_algorithms, 0, False)
        if not ok:
            app.update_output.emit(log_message("[GPU] Cancelled"))
            return

        # Run CPU version for comparison
        start_cpu = time.time()
        cpu_hash = gpu_accelerator._cpu_hash_calculation(binary_data, algorithm)
        end_cpu = time.time()
        cpu_time = end_cpu - start_cpu

        # Run GPU version
        start_gpu = time.time()
        gpu_hash = gpu_accelerator.accelerate_hash_calculation(binary_data, algorithm)
        end_gpu = time.time()
        gpu_time = end_gpu - start_gpu

        # Calculate speedup
        speedup = cpu_time / gpu_time if gpu_time > 0 else 0

        # Verify hash result by comparing CPU and GPU results
        hash_match = cpu_hash == gpu_hash

        # Store CPU hash result in a database for file identification and auditing
        file_hash_key = f"{algorithm}_{len(binary_data)}"
        if not hasattr(app, "hash_database"):
            app.hash_database = {}

        # Track the hash results for this file
        if file_hash_key not in app.hash_database:
            app.hash_database[file_hash_key] = []

        # Add the CPU hash to the database
        hash_record = {
            'algorithm': algorithm,
            'hash': cpu_hash.hex(),
            'timestamp': time.time(),
            'file_size': len(binary_data),
            'computation_time': cpu_time
        }
        app.hash_database[file_hash_key].append(hash_record)

        # If hashes don't match, investigate differences
        hash_diff_info = ""
        if not hash_match:
            if len(cpu_hash) != len(gpu_hash):
                hash_diff_info = f"Length mismatch: CPU {len(cpu_hash)} bytes vs GPU {len(gpu_hash)} bytes"
            else:
                # Find the first byte that differs
                for i, (c_byte, g_byte) in enumerate(zip(cpu_hash, gpu_hash)):
                    if c_byte != g_byte:
                        hash_diff_info = f"First difference at byte {i}: CPU {c_byte:02x} vs GPU {g_byte:02x}"
                        break

            # Record failure in database for later analysis
            hash_record['gpu_hash'] = gpu_hash.hex()
            hash_record['match'] = False
            hash_record['error_details'] = hash_diff_info
        else:
            hash_record['gpu_hash'] = gpu_hash.hex()
            hash_record['match'] = True

        # Display results
        app.update_output.emit(log_message(f"[GPU] {algorithm.upper()} hash: {gpu_hash.hex()}"))
        app.update_output.emit(log_message(f"[GPU] CPU hash: {cpu_hash.hex()}"))
        app.update_output.emit(log_message(f"[GPU] Hash verification: {'Passed' if hash_match else 'Failed'}"))
        if hash_diff_info:
            app.update_output.emit(log_message(f"[GPU] Hash difference: {hash_diff_info}"))
        app.update_output.emit(log_message(f"[GPU] CPU time: {cpu_time:.4f} seconds"))
        app.update_output.emit(log_message(f"[GPU] GPU time: {gpu_time:.4f} seconds"))
        app.update_output.emit(log_message(f"[GPU] Speedup: {speedup:.2f}x"))

        # Add to analyze results
        if not hasattr(app, "analyze_results"):
            app.analyze_results = []

        app.analyze_results.append(f"\n=== GPU-ACCELERATED {algorithm.upper()} HASH CALCULATION RESULTS ===")
        app.analyze_results.append(f"Hash: {gpu_hash.hex()}")
        app.analyze_results.append(f"CPU time: {cpu_time:.4f} seconds")
        app.analyze_results.append(f"GPU time: {gpu_time:.4f} seconds")
        app.analyze_results.append(f"Speedup: {speedup:.2f}x")

# -------------------------------
# Runtime Patching Fallback
# -------------------------------


def generate_launcher_script(app, patching_strategy="memory"):
    """
    Generates a launcher script that patches the target program in memory.
    This is used as a fallback for heavily protected applications.

    Args:
        app: Application instance
        patching_strategy: Type of patching to use ("memory" or "api")
    """
    if not app.binary_path:
        app.update_output.emit(log_message("[Launcher] No binary selected."))
        return

    binary_path = app.binary_path
    binary_name = os.path.basename(binary_path)

    app.update_output.emit(log_message(
        f"[Launcher] Generating launcher script for {binary_name}..."))

    # Create scripts directory if it doesn't exist
    if not os.path.exists("scripts"):
        os.makedirs("scripts")

    # Determine patches to apply
    patches = []

    # If we have patches from preview, use those
    if hasattr(app, "potential_patches") and app.potential_patches:
        for patch in app.potential_patches:
            if "address" in patch and "new_bytes" in patch:
                patches.append({"address": patch["address"], "new_bytes": patch["new_bytes"].hex() if isinstance(
                    patch["new_bytes"], bytes) else patch["new_bytes"], "description": patch.get("description", "")})

    # If no patches available, inform and ask
    if not patches:
        response = QMessageBox.question(
            app,
            "No Patches Found",
            "No patches found for the selected binary. Would you like to generate a launcher script anyway?",
            QMessageBox.Yes | QMessageBox.No)

        if response == QMessageBox.No:
            app.update_output.emit(log_message(
                "[Launcher] Cancelled launcher script generation."))
            return

    # Determine which template to use
    if patching_strategy == "memory":
        # Memory patching script (uses Frida)
        script_template = """
# Intellicrack Memory Patching Launcher
# Generated by Intellicrack on {date}
# Target: {binary_name}

import os
import sys

def on_message(message, data):
    if message["type"] == "send":
        print(f"[Intellicrack] {message['payload']}")
    elif message["type"] == "error":
        print(f"[Intellicrack] Error: {message['stack']}")

def main():
    target_path = r"{binary_path}"

    print(f"Intellicrack Memory Patching Launcher")
    print(f"Target: {binary_name}")

    # Launch the target process
    print("Launching target process...")
    target_process = subprocess.Popen([target_path])

    # Wait a moment for the process to initialize
    time.sleep(1)

    try:
        # Attach to the process
        print(f"Attaching to process PID {target_process.pid}...")
        session = frida.attach(target_process.pid)

        # Create the patching script
        script_code = '''
        function logMessage(message) {{
            console.log(message);
            return true;
        }}

        (function() {{
            logMessage("[Intellicrack] Memory patching script initialized");

            // Wait for modules to load
            Process.enumerateModules({{
                onMatch: function(module) {{
                    if (module.name.toLowerCase() === "{binary_name_lower}") {{
                        logMessage("[Intellicrack] Found target module at " + module.base);
                        applyPatches(module.base);
                    }}
                }},
                onComplete: function() {{
                    logMessage("[Intellicrack] Module enumeration complete");
                }}
            }});

            function applyPatches(baseAddress) {{
                logMessage("[Intellicrack] Applying patches...");

                // Define patches
                var patches = [
{patches_json}
                ];

                // Apply each patch
                var patchCount = 0;

                for (var i = 0; i < patches.length; i++) {{
                    var patch = patches[i];
                    var address = ptr(patch.address);

                    try {{
                        // Convert hex string to byte array
                        var newBytes = [];
                        for (var j = 0; j < patch.new_bytes.length; j += 2) {{
                            newBytes.push(parseInt(patch.new_bytes.substr(j, 2), 16));
                        }}

                        // Make memory writable
                        Memory.protect(address, newBytes.length, 'rwx');

                        // Apply patch
                        Memory.writeByteArray(address, newBytes);

                        logMessage("[Intellicrack] Patched address " + address + ": " + patch.description);
                        patchCount++;
                    }} catch (e) {{
                        logMessage("[Intellicrack] Error patching " + address + ": " + e);
                    }}
                }}

                logMessage("[Intellicrack] Successfully applied " + patchCount + " patches");

                // Additional integrity check bypassing
                bypassIntegrityChecks();
            }}

            function bypassIntegrityChecks() {{
                // Hook integrity checking APIs
                var checkSumMappedFile = Module.findExportByName(null, "CheckSumMappedFile");
                if (checkSumMappedFile) {{
                    Interceptor.attach(checkSumMappedFile, {{
                        onLeave: function(retval) {{
                            logMessage("[Intellicrack] Intercepted CheckSumMappedFile");
                        }}
                    }});
                }}

                var cryptHashData = Module.findExportByName(null, "CryptHashData");
