
            # Modify response based on content type
            content_type = flow.response.headers.get('Content-Type', '')

            if 'application/json' in content_type:
                try:
                    data = json.loads(flow.response.content.decode('utf-8', errors='ignore'))

                    # Modify license-related fields
                    if isinstance(data, dict):
                        if 'license' in data:
                            data['license'] = 'valid'
                        if 'status' in data:
                            data['status'] = 'success'
                        if 'valid' in data:
                            data['valid'] = True
                        if 'expires' in data:
                            data['expires'] = '2099-12-31'
                        if 'activated' in data:
                            data['activated'] = True

                    # Replace response
                    flow.response.content = json.dumps(data).encode('utf-8')
                    self.stats['modified_responses'] += 1
                    ctx.log.info(f"Modified JSON response for {url}")

                except Exception as e:
                    ctx.log.error(f"Error modifying JSON response: {e}")

            elif 'application/xml' in content_type or 'text/xml' in content_type:
                # Simple string replacement for XML
                content = flow.response.content.decode('utf-8', errors='ignore')

                # Replace common license status indicators
                content = re.sub(r'<license[^>]*valid=["\']false["\'][^>]*>', '<license valid="true">', content, flags=re.IGNORECASE)
                content = re.sub(r'<status>(?:invalid|expired|error)</status>', '<status>valid</status>', content, flags=re.IGNORECASE)
                content = re.sub(r'<activated>false</activated>', '<activated>true</activated>', content, flags=re.IGNORECASE)

                flow.response.content = content.encode('utf-8')
                self.stats['modified_responses'] += 1
                ctx.log.info(f"Modified XML response for {url}")

            else:
                # For other content types, try simple string replacement
                content = flow.response.content.decode('utf-8', errors='ignore')

                # Replace common license status indicators
                content = re.sub(r'(license|status|valid)[ \t\n\r\f\v]*[:=][ \t\n\r\f\v]*(invalid|false|0|expired|error)', r'\1=valid', content, flags=re.IGNORECASE)

                flow.response.content = content.encode('utf-8')
                self.stats['modified_responses'] += 1
                ctx.log.info(f"Modified generic response for {url}")

addons = [LicenseInterceptor()]
        """

        self.logger.debug("mitmproxy script generation complete.")
        return script

    def _find_executable(self, executable):
        """Find the path to an executable"""
        self.logger.debug(f"Searching for executable: {executable}")
        if os.name == 'nt':  # Windows
            executable += '.exe'

        # Check if executable is in PATH
        for path in os.environ["PATH"].split(os.pathsep):
            exe_path = os.path.join(path, executable)
            if os.path.isfile(exe_path) and os.access(exe_path, os.X_OK):
                self.logger.debug(f"Found executable at: {exe_path}")
                return exe_path

        # If not found, try common locations
        self.logger.debug(f"{executable} not in PATH, checking common locations.")
        common_locations = [
            os.path.join(os.path.expanduser("~"), ".local", "bin"),
            os.path.join(os.path.expanduser("~"), "AppData", "Local", "Programs", "Python"),
            "/usr/local/bin",
            "/opt/local/bin"
        ]

        for location in common_locations:
            if os.path.exists(location):
                for root, dirs, files in os.walk(location):
                    # Filter out hidden and system directories
                    dirs[:] = [d for d in dirs if not d.startswith('.') and d.lower() not in ('windows', 'program files', 'program files (x86)', 'system32')]
                    if executable in files:
                        exe_path = os.path.join(root, executable)
                        if os.access(exe_path, os.X_OK):
                            return exe_path

        # If still not found, return just the executable name and hope it's in PATH
        self.logger.warning(f"Executable '{executable}' not found. Returning original name.")
        return executable

def run_ssl_tls_interceptor(app):
    """Initialize and run the SSL/TLS interceptor"""

    # If interceptor is already running, stop it
    if hasattr(app, "ssl_interceptor_instance") and app.ssl_interceptor_instance:
        app.update_output.emit(log_message("[SSL Interceptor] Stopping existing interceptor..."))
        app.ssl_interceptor_instance.stop()
        app.ssl_interceptor_instance = None
        app.update_output.emit(log_message("[SSL Interceptor] Interceptor stopped"))
        return

    # Ask for port
    port, ok = QInputDialog.getInt(app, "SSL/TLS Interceptor Port", "Enter port number:", 8443, 1024, 65535)
    if not ok:
        app.update_output.emit(log_message("[SSL Interceptor] Cancelled"))
        return

    # Ask for target host (optional)
    target_host, ok = QInputDialog.getText(app, "Target Host", "Enter target host (leave empty for transparent mode):")
    if not ok:
        app.update_output.emit(log_message("[SSL Interceptor] Cancelled"))
        return

    target_port = 443
    if target_host:
        # Ask for target port
        target_port, ok = QInputDialog.getInt(app, "Target Port", "Enter target port:", 443, 1, 65535)
        if not ok:
            app.update_output.emit(log_message("[SSL Interceptor] Cancelled"))
            return

    # Create and configure the interceptor
    interceptor = SSLTLSInterceptor({
        'intercept_port': port,
        'target_host': target_host if target_host else None,
        'target_port': target_port,
        'ca_cert_path': os.path.join(os.getcwd(), "certs", "intellicrack_ca.pem"),
        'ca_key_path': os.path.join(os.getcwd(), "certs", "intellicrack_ca.key")
    })

    # Ensure certificate directory exists
    os.makedirs(os.path.join(os.getcwd(), "certs"), exist_ok=True)

    # Start the interceptor
    app.update_output.emit(log_message("[SSL Interceptor] Starting SSL/TLS interceptor..."))
    if interceptor.start():
        app.update_output.emit(log_message(f"[SSL Interceptor] Started on port {port}"))
        app.ssl_interceptor_instance = interceptor

        # Add to analyze results
        if not hasattr(app, "analyze_results"):
            app.analyze_results = []

        app.analyze_results.append("\n=== SSL/TLS INTERCEPTOR ===")
        app.analyze_results.append(f"Interceptor running on port {port}")
        if target_host:
            app.analyze_results.append(f"Target: {target_host}:{target_port}")
        else:
            app.analyze_results.append("Mode: Transparent proxy")
        app.analyze_results.append(f"CA Certificate: {interceptor.ca_cert_path}")
        app.analyze_results.append("\nTo use:")
        app.analyze_results.append(f"1. Configure your application to use localhost:{port} as a proxy")
        app.analyze_results.append(f"2. Import the CA certificate from {interceptor.ca_cert_path}")
        app.analyze_results.append("3. The interceptor will automatically modify license-related responses")
    else:
        app.update_output.emit(log_message("[SSL Interceptor] Failed to start interceptor"))
        app.ssl_interceptor_instance = None

        # If no match, return unknown
        return 'unknown'

    def _calculate_byte_frequency(self, data):
        """Calculate the frequency distribution of bytes in the given data."""
        from collections import Counter
        length = len(data)
        counts = Counter(data)
        freq = {byte: count / length for byte, count in counts.items()}
        return freq

    def _learn_new_signature(self, packet_data, port=None):
        """Learn new protocol signatures from traffic samples"""
        # Add to traffic samples
        self.traffic_samples.append({
            'data': packet_data,
            'port': port,
            'timestamp': datetime.datetime.now().isoformat()
        })

        # If we have enough samples, try to generate a signature
        if len(self.traffic_samples) >= 10:
            # Extract common patterns
            patterns = self._extract_common_patterns(self.traffic_samples)

            if patterns:
                # Create a new signature
                signature = {
                    'patterns': patterns,
                    'ports': [s['port'] for s in self.traffic_samples if s['port']] if port else [],
                    'created': datetime.datetime.now().isoformat(),
                    'sample_count': len(self.traffic_samples)
                }

                # Generate a unique ID for the signature
                signature_id = f"protocol_{len(self.signatures) + 1}"

                # Add to signatures
                self.signatures[signature_id] = signature

                # Save signatures
                self._save_signatures()

                # Clear samples
                self.traffic_samples = []

                self.logger.info(f"Created new protocol signature: {signature_id}")

    def _extract_common_patterns(self, samples):
        """Extract common patterns from traffic samples"""
        # Extract byte sequences that appear in most samples
        common_patterns = []

        # Simplified approach: find common substrings
        if len(samples) < 2:
            return []

        # Convert all samples to string for pattern extraction
        sample_strings = []
        for sample in samples:
            if isinstance(sample['data'], bytes):
                # Convert bytes to hex string for pattern matching
                sample_strings.append(' '.join(f'{b:02x}' for b in sample['data']))
            else:
                sample_strings.append(str(sample['data']))

        # Find common n-grams (sequences of n consecutive bytes)
        for n in range(4, 16):  # Try different n-gram sizes
            ngrams = {}

            for s in sample_strings:
                for i in range(len(s) - n + 1):
                    ngram = s[i:i+n]
                    ngrams[ngram] = ngrams.get(ngram, 0) + 1

            # Keep n-grams that appear in at least 70% of samples
            threshold = 0.7 * len(samples)
            common_ngrams = [ng for ng, count in ngrams.items() if count >= threshold]

            # Add to patterns
            common_patterns.extend(common_ngrams)

            # Limit to 20 patterns
            if len(common_patterns) >= 20:
                break

        return common_patterns[:20]  # Return up to 20 patterns

def run_protocol_fingerprinter(app):
    """Initialize and run the protocol fingerprinter"""

    # Ask if learning mode should be enabled
    learning_mode = QMessageBox.question(
        app,
        "Learning Mode",
        "Enable learning mode? This will allow the fingerprinter to learn from captured traffic.",
        QMessageBox.Yes | QMessageBox.No
    ) == QMessageBox.Yes

    # Create and configure the fingerprinter
    fingerprinter = ProtocolFingerprinter({
        'learning_mode': learning_mode,
        'log_traffic': True
    })

    # Store the fingerprinter instance
    app.protocol_fingerprinter_instance = fingerprinter

    app.update_output.emit(log_message("[Protocol Fingerprinter] Initialized"))

    # If in learning mode, prompt user to capture traffic
    if learning_mode:
        app.update_output.emit(log_message("[Protocol Fingerprinter] Learning mode enabled. Capturing traffic for signature generation."))

        # Explain how to use
        app.update_output.emit(log_message("[Protocol Fingerprinter] To use:"))
        app.update_output.emit(log_message("1. Run the Network Traffic Analyzer to capture traffic"))
        app.update_output.emit(log_message("2. The fingerprinter will automatically learn from captured traffic"))
        app.update_output.emit(log_message("3. After capturing enough samples, new protocol signatures will be generated"))
    else:
        app.update_output.emit(log_message("[Protocol Fingerprinter] Learning mode disabled. Using existing protocol signatures."))

    # Add analysis results
    _add_protocol_fingerprinter_results(app, fingerprinter, learning_mode)

    return fingerprinter
    def _save_patterns(self):
        """Save request patterns to the database"""
        patterns_file = os.path.join(os.path.dirname(__file__), 'data', 'cloud_license_patterns.json')

        # Ensure directory exists
        os.makedirs(os.path.dirname(patterns_file), exist_ok=True)

        try:
            with open(patterns_file, 'w') as f:
                json.dump(self.request_patterns, f, indent=2)
        except Exception as e:
            self.logger.error(f"Error saving request patterns: {e}")

    def _run_proxy(self, port):
        """Run the proxy server"""
        try:
            class ProxyHandler(BaseHTTPRequestHandler):
                """Inner handler class with access to outer instance"""

                def __init__(self, *args, **kwargs):
                    """
                    Initialize the ProxyHandler with a reference to the outer instance.

                    Args:
                        *args: Positional arguments for the superclass.
                        **kwargs: Keyword arguments, must include 'outer_instance'.
                    """
                    self.outer = kwargs.pop('outer_instance')
                    super().__init__(*args, **kwargs)

                def log_message(self, format, *args):
                    """Override to use our logger"""
                    self.outer.logger.info(format % args)

                def do_GET(self):
                    """Handle GET requests"""
                    self.outer.logger.info(f"GET request to {self.path}")
                    self._handle_request()

                def do_POST(self):
                    """Handle POST requests"""
                    content_length = int(self.headers.get('Content-Length', 0))
                    post_data = self.rfile.read(content_length).decode('utf-8')
                    self.outer.logger.info(f"POST request to {self.path} with data: {post_data}")
                    self._handle_request(post_data)

                def _handle_request(self, post_data=None):
                    """Common request handling logic"""
                    # Capture request for learning
                    request_info = {
                        'path': self.path,
                        'method': self.command,
                        'headers': dict(self.headers),
                        'data': post_data,
                        'timestamp': datetime.datetime.now().isoformat()
                    }

                    self.outer.captured_requests.append(request_info)

                    # Auto-generate and save reports periodically if configured
                    if (len(self.outer.captured_requests) % 10 == 0) and hasattr(self.outer, 'auto_report') and self.outer.auto_report:
                        report_path = f"report_{time.strftime('%Y%m%d_%H%M%S')}.html"
                        self.outer.generate_report(report_path)
                        self.outer.logger.info(f"Auto-generated report: {report_path}")

                    # Retrieve captured requests for analysis after every 5 requests
                    if len(self.outer.captured_requests) % 5 == 0:
                        request_data = self.outer.get_captured_requests()
                        self.outer.logger.info(f"Analyzed {request_data['count']} requests with {len(request_data['analysis'].get('common_paths', []))} common paths")

                    # Check if we have a pattern for this request
                    response_data = self.outer.generate_response(request_info)

                    # Send response
                    self.send_response(200)

                    # Set content type based on response data
                    if isinstance(response_data, dict):
                        self.send_header('Content-Type', 'application/json')
                        response_body = json.dumps(response_data).encode('utf-8')
                    elif response_data.startswith('<?xml') or response_data.startswith('<'):
                        self.send_header('Content-Type', 'application/xml')
                        response_body = response_data.encode('utf-8')
                    else:
                        self.send_header('Content-Type', 'text/plain')
                        response_body = response_data.encode('utf-8')

                    self.end_headers()
                    self.wfile.write(response_body)

                    # Learn from this interaction if in learning mode
                    if self.outer.learning_mode:
                        self.outer._learn_pattern(request_info, response_data)

            # Create server with handler that has access to outer instance
            handler = lambda *args: ProxyHandler(*args, outer_instance=self)
            self.proxy_server = HTTPServer(('', port), handler)

            # Set up graceful shutdown handler
            def signal_handler(sig, frame):
                """
                Handle shutdown signals for graceful proxy server termination.

                Logs the received signal and stops the proxy server.
                """
                self.logger.info(f"Received signal {sig}, shutting down...")
                self.stop_proxy()

            # Connect shutdown handler if platform supports it
            try:
                import signal
                signal.signal(signal.SIGINT, signal_handler)
                signal.signal(signal.SIGTERM, signal_handler)
            except (ImportError, AttributeError):
                self.logger.debug("Signal handling not supported on this platform")

            # Initialize server start time
            self.start_time = time.time()

            # Start server in a way that can be interrupted
            self.logger.info(f"Starting proxy server on port {port}")
            self.proxy_server.serve_forever()

        except Exception as e:
            self.logger.error(f"Error running proxy: {e}")
            self.running = False
            # Attempt to clear any partial data on error
            self.clear_data()

    def generate_response(self, request_info):
        """Generate a response for a license request"""
        # Define different response generation strategies
        generate_response = {
            "standard": lambda req: self._standard_response(req),
            "enhanced": lambda req: self._enhanced_response(req),
            "learning": lambda req: self._learning_response(req),
            "offline": lambda req: self._offline_response(req)
        }

        # Choose appropriate response strategy based on configuration
        mode = self.config.get('response_mode', 'standard')
        if mode not in generate_response:
            self.logger.warning(f"Unknown response mode: {mode}, falling back to standard")
            mode = "standard"

        # Generate the response using the selected strategy
        self.logger.debug(f"Using response strategy: {mode} for: {request_info['path']}")
        response = generate_response[mode](request_info)

        # Track statistics
        self.last_request_processed = request_info
        self.response_count = getattr(self, 'response_count', 0) + 1

        return response

        # Check if we have a pattern for this request
        for pattern_id, pattern in self.request_patterns.items():
            if self._match_pattern(request_info, pattern):
                self.logger.info(f"Matched pattern {pattern_id}")
                # Record successful matches for reporting
                self.matched_patterns = getattr(self, 'matched_patterns', {})
                self.matched_patterns[pattern_id] = self.matched_patterns.get(pattern_id, 0) + 1
                return pattern['response']

        # If no pattern matched, generate a generic response
        self.logger.info("No pattern matched, using generic response")
        return self._generate_generic_response(request_info)

    def _match_pattern(self, request_info, pattern):
        """Check if a request matches a pattern"""
        # Check path
        if 'path' in pattern and pattern['path'] != request_info['path']:
            return False

        # Check method
        if 'method' in pattern and pattern['method'] != request_info['method']:
            return False

        # Check headers
        if 'headers' in pattern:
            for header, value in pattern['headers'].items():
                if header not in request_info['headers'] or request_info['headers'][header] != value:
                    return False

        # Check data
        if 'data' in pattern and pattern['data'] and request_info['data']:
            # For JSON data
            try:
                pattern_data = json.loads(pattern['data']) if isinstance(pattern['data'], str) else pattern['data']
                request_data = json.loads(request_info['data']) if isinstance(request_info['data'], str) else request_info['data']

                # Check if pattern data is a subset of request data
                for key, value in pattern_data.items():
                    if key not in request_data or request_data[key] != value:
                        return False
            except:
                # For non-JSON data, do simple string matching
                if pattern['data'] not in request_info['data']:
                    return False

        return True

    def _generate_generic_response(self, request_info):
        """Generate a generic response for a license request"""
        # Check content type to determine response format
        content_type = request_info['headers'].get('Content-Type', '')

        if 'json' in content_type:
            # Generate JSON response
            return {
                "status": "success",
                "license": "valid",
                "activated": True,
                "expires": "2099-12-31",
                "features": ["all"],
                "message": "License is valid"
            }
        elif 'xml' in content_type:
            # Generate XML response
            return """<?xml version="1.0" encoding="UTF-8"?>
<response>
  <status>success</status>
  <license valid="true" expires="2099-12-31">
    <features>
      <feature>all</feature>
    </features>
    <message>License is valid</message>
  </license>
</response>"""
        else:
            # Generate plain text response
            return "STATUS=success\nLICENSE=valid\nEXPIRES=2099-12-31\nFEATURES=all\nMESSAGE=License is valid"

    def _learn_pattern(self, request_info, response_data):
        """Learn a new pattern from a request-response pair"""
        # Create a new pattern
        pattern = {
            'path': request_info['path'],
            'method': request_info['method'],
            'headers': {
                'Content-Type': request_info['headers'].get('Content-Type', '')
            },
            'data': request_info['data'],
            'response': response_data,
            'created': datetime.datetime.now().isoformat()
        }

        # Generate a unique ID for the pattern
        pattern_id = f"pattern_{len(self.request_patterns) + 1}"

        # Add to patterns
        self.request_patterns[pattern_id] = pattern

        # Save patterns
        self._save_patterns()

        self.logger.info(f"Learned new pattern: {pattern_id}")

    def stop_proxy(self):
        """Stop the proxy server"""
        # Define different shutdown strategies based on state
        stop_proxy = {
            "normal": lambda: self._normal_shutdown(),
            "emergency": lambda: self._emergency_shutdown(),
            "archive": lambda: self._archive_and_shutdown(),
            "clean": lambda: self._clean_shutdown()
        }

        # Determine appropriate shutdown mode based on current state
        shutdown_mode = "normal"  # default
        if getattr(self, 'emergency_mode', False):
            shutdown_mode = "emergency"
        elif len(self.captured_requests) > 100:
            shutdown_mode = "archive"
        elif getattr(self, 'clean_on_exit', False):
            shutdown_mode = "clean"

        self.logger.info(f"Using shutdown strategy: {shutdown_mode}")

        # Record statistics before stopping
        stats = {
            'total_runtime': time.time() - getattr(self, 'start_time', time.time()),
            'requests_processed': len(self.captured_requests),
            'responses_generated': getattr(self, 'response_count', 0),
            'pattern_matches': getattr(self, 'matched_patterns', {})
        }

        # Execute the selected shutdown strategy
        stop_proxy[shutdown_mode]()

        # Save usage statistics
        self.usage_history = getattr(self, 'usage_history', [])
        self.usage_history.append({
            'timestamp': datetime.datetime.now().isoformat(),
            'stats': stats
        })

        # Perform actual shutdown
        self.running = False
        if self.proxy_server:
            self.logger.info("Shutting down proxy server...")
            self.proxy_server.shutdown()
            self.proxy_server = None

        if self.proxy_thread:
            self.logger.info("Waiting for proxy thread to terminate...")
            self.proxy_thread.join(timeout=2.0)
            self.proxy_thread = None

        # Archive data before clearing
        self._archive_data()

        # Clear active data
        self.clear_data()

        self.logger.info(f"Stopped cloud license proxy after handling {stats['requests_processed']} requests")
        return stats

    def _archive_data(self):
        """Archive captured data before clearing"""
        if not self.captured_requests:
            return

        # Create archive directory if it doesn't exist
        archive_dir = os.path.join(os.path.dirname(self.data_file), 'archives')
        os.makedirs(archive_dir, exist_ok=True)

        # Save to timestamped file
        archive_file = os.path.join(archive_dir, f"requests_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        try:
            with open(archive_file, 'w') as f:
                json.dump(self.captured_requests, f, indent=2)
            self.logger.info(f"Archived {len(self.captured_requests)} requests to {archive_file}")
        except Exception as e:
            self.logger.error(f"Failed to archive data: {e}")

    def clear_data(self):
        """Clear captured data"""
        # Define what data components should be cleared based on settings
        clear_data = {
            "requests": getattr(self, 'clear_requests', True),
            "statistics": getattr(self, 'clear_statistics', True),
            "patterns": getattr(self, 'clear_patterns', False),
            "history": getattr(self, 'clear_history', False)
        }

        # Log which components will be cleared
        components_to_clear = [k for k, v in clear_data.items() if v]
        self.logger.debug(f"Clearing data components: {', '.join(components_to_clear)}")

        # Track counts for reporting
        cleared_counts = {}

        # Clear requests if configured
        if clear_data["requests"]:
            request_count = len(self.captured_requests)
            cleared_counts["requests"] = request_count
            self.captured_requests = []

        # Clear statistics if configured
        if clear_data["statistics"]:
            cleared_counts["statistics"] = 1 if self.response_count > 0 else 0
            self.response_count = 0
            self.last_request_processed = None

        # Clear learned patterns if configured
        if clear_data["patterns"]:
            pattern_count = len(getattr(self, 'matched_patterns', {}))
            cleared_counts["patterns"] = pattern_count
            self.matched_patterns = {}

        # Clear historical data if configured
        if clear_data["history"]:
            history_count = len(getattr(self, 'usage_history', []))
            cleared_counts["history"] = history_count
            self.usage_history = []

        # Log summary
        total_cleared = sum(cleared_counts.values())
        self.logger.info(f"Cleared {total_cleared} data items across {len(components_to_clear)} components")

        return cleared_counts

    def get_captured_requests(self):
        """Get captured requests with analysis"""
        # Define processors with various filtering capabilities
        get_captured_requests = {
            "all": lambda reqs: reqs,
            "recent": lambda reqs: reqs[-10:] if len(reqs) > 10 else reqs,
            "success_only": lambda reqs: [r for r in reqs if r.get('status_code', 0) < 400],
            "failed_only": lambda reqs: [r for r in reqs if r.get('status_code', 0) >= 400]
        }

        # Choose appropriate processor based on current state
        processor_key = "all"  # default
        if hasattr(self, 'last_filter') and self.last_filter in get_captured_requests:
            processor_key = self.last_filter

        # Apply the selected processor to filter requests
        filtered_requests = get_captured_requests[processor_key](self.captured_requests)
        self.logger.debug(f"Using request processor: {processor_key} - returned {len(filtered_requests)} of {len(self.captured_requests)} requests")

        # Perform basic analysis on filtered requests
        analysis = self._analyze_requests(filtered_requests)

        return {
            'requests': filtered_requests,
            'count': len(filtered_requests),
            'analysis': analysis,
            'processor': processor_key
        }

    def _analyze_requests(self, requests):
        """Analyze captured requests for patterns and statistics"""
        self.logger.debug(f"Analyzing {len(requests)} captured requests.")
        if not requests:
            return {}

        # Extract basic statistics
        paths = {}
        methods = {}
        content_types = {}

        for req in requests:
            # Count paths
            path = req.get('path', 'unknown')
            paths[path] = paths.get(path, 0) + 1

            # Count methods
            method = req.get('method', 'unknown')
            methods[method] = methods.get(method, 0) + 1

            # Count content types
            headers = req.get('headers', {})
            content_type = headers.get('Content-Type', 'unknown')
            content_types[content_type] = content_types.get(content_type, 0) + 1

        result = {
            'common_paths': sorted(paths.items(), key=lambda x: x[1], reverse=True),
            'methods': methods,
            'content_types': content_types
        }

        self.logger.debug(f"Request analysis complete. Common paths: {list(paths.keys())[:5]}, Methods: {methods}, Content-Types: {list(content_types.keys())[:5]}")
        return result

    def generate_report(self, filename=None):
        """Generate a comprehensive report of captured requests with analysis"""
        # Define different report generation formats and their handlers
        generate_report = {
            "pdf": self._generate_pdf_report,
            "html": self._generate_html_report,
            "json": self._generate_json_report,
            "csv": self._generate_csv_report,
            "plaintext": self._generate_text_report
        }

        # Determine format based on filename extension or default to PDF
        report_format = "pdf"
        if filename:
            ext = os.path.splitext(filename)[1].lower().lstrip('.')
            if ext in generate_report:
                report_format = ext

        self.logger.info(f"Generating {report_format} report")

        try:
            # Get request data with analysis
            request_data = self.get_captured_requests()

            # Generate more detailed analysis for the report
            pattern_usage = getattr(self, 'matched_patterns', {})
            pattern_stats = [{'id': k, 'matches': v} for k, v in pattern_usage.items()]

            # Calculate summary statistics from the request data
            total_requests = len(request_data) if isinstance(request_data, list) else 0
            unique_hosts = set()
            request_methods = {}
            status_codes = {}

            # Process request data to generate statistics
            for req in request_data if isinstance(request_data, list) else []:
                if 'host' in req:
                    unique_hosts.add(req['host'])
                if 'method' in req:
                    method = req['method']
                    request_methods[method] = request_methods.get(method, 0) + 1
                if 'status' in req:
                    status = req['status']
                    status_codes[status] = status_codes.get(status, 0) + 1

            # Format pattern stats for display
            pattern_table_rows = ""
            for pattern in pattern_stats:
                pattern_table_rows += f"""
                <tr>
                    <td>{pattern['id']}</td>
                    <td>{pattern['matches']}</td>
                </tr>
                """

            # Format request data for display
            request_table_rows = ""
            for i, req in enumerate(request_data[:20] if isinstance(request_data, list) else []): # Show first 20
                method = req.get('method', 'N/A')
                url = req.get('url', 'N/A')
                status = req.get('status', 'N/A')
                request_table_rows += f"""
                <tr>
                    <td>{i+1}</td>
                    <td>{method}</td>
                    <td>{url[:50]}{"..." if len(url) > 50 else ""}</td>
                    <td>{status}</td>
                </tr>
                """

            # Generate HTML report with enhanced visualization using the collected data
            html = f"""
            <html>
            <head>
                <title>Cloud License Requests Report</title>
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 20px; }}
                    h1, h2, h3 {{ color: #333366; }}
                    table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}
                    th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                    th {{ background-color: #f2f2f2; }}
                    tr:nth-child(even) {{ background-color: #f9f9f9; }}
                    .summary-box {{
                        background-color: #f0f0f0;
                        border-radius: 5px;
                        padding: 15px;
                        margin-bottom: 20px;
                        display: inline-block;
                        margin-right: 15px;
                        min-width: 150px;
                    }}
                    .summary-number {{ font-size: 24px; font-weight: bold; color: #333366; }}
                    .summary-container {{ display: flex; flex-wrap: wrap; }}
                </style>
                <style>
                    body {{ font-family: Arial, sans-serif; margin: 20px; }}
                    h1, h2 {{ color: #333; }}
                    table {{ border-collapse: collapse; width: 100%; margin-bottom: 20px; }}
                    th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                    th {{ background-color: #f2f2f2; }}
                    tr:nth-child(even) {{ background-color: #f9f9f9; }}
                    pre {{ background-color: #f5f5f5; padding: 10px; border-radius: 5px; overflow-x: auto; }}
                </style>
            </head>
            <body>
                <h1>Cloud License Requests Report</h1>

                <h2>Captured Requests</h2>
                <p>Total Requests: {total_requests}</p>
                <p>Unique Hosts: {len(unique_hosts)}</p>
                <p>Request Methods: {", ".join(f"{k}:{v}" for k, v in request_methods.items())}</p>

                <table>
                    <tr>
                        <th>Time</th>
                        <th>Method</th>
                        <th>Path</th>
                        <th>Content Type</th>
                        <th>Data</th>
                    </tr>
            """

            for req in self.captured_requests:
                html += f"""
                    <tr>
                        <td>{req['timestamp']}</td>
                        <td>{req['method']}</td>
                        <td>{req['path']}</td>
                        <td>{req['headers'].get('Content-Type', '')}</td>
                        <td><pre>{req['data'] if req['data'] else ''}</pre></td>
                    </tr>
                    """

                self.clear_data()  # Clear captured requests after generating report

            html += """
                </table>

                <h2>Learned Patterns</h2>
                <p>Total Patterns: {}</p>

                <table>
                    <tr>
                        <th>ID</th>
                        <th>Path</th>
                        <th>Method</th>
                        <th>Created</th>
                    </tr>
            """.format(len(self.request_patterns))

            for pattern_id, pattern in self.request_patterns.items():
                html += f"""
                    <tr>
                        <td>{pattern_id}</td>
                        <td>{pattern['path']}</td>
                        <td>{pattern['method']}</td>
                        <td>{pattern['created']}</td>
                    </tr>
                """

            html += """
                </table>
            </body>
            </html>
            """

            # Save to file if filename provided
            if filename:
                with open(filename, 'w') as f:
                    f.write(html)
                self.logger.info(f"Report saved to {filename}")
                return filename
            else:
                return html

        except Exception as e:
            self.logger.error(f"Error generating report: {e}")
            return None

def run_cloud_license_hooker(app):
    """Initialize and run the cloud license response generator"""

    # If generator is already running, stop it
    if hasattr(app, "cloud_license_instance") and app.cloud_license_instance and app.cloud_license_instance.running:
        app.update_output.emit(log_message("[Cloud License] Stopping proxy..."))
        app.cloud_license_instance.stop_proxy()
        app.update_output.emit(log_message("[Cloud License] Proxy stopped"))

        # Ask if user wants to generate a report
        generate_report = QMessageBox.question(
            app,
            "Generate Report",
            "Do you want to generate a report of the captured license requests?",
            QMessageBox.Yes | QMessageBox.No
        ) == QMessageBox.Yes

        if generate_report:
            # Ask for report filename
            filename, _ = QFileDialog.getSaveFileName(
                app,
                "Save Report",
                "",
                "HTML Files (*.html);;All Files (*)"
            )

            if filename:
                if not filename.endswith('.html'):
                    filename += '.html'

                report_path = app.cloud_license_instance.generate_report(filename)
                if report_path:
                    app.update_output.emit(log_message(f"[Cloud License] Report saved to {report_path}"))

                    # Ask if user wants to open the report
                    open_report = QMessageBox.question(
                        app,
                        "Open Report",
                        "Do you want to open the report?",
                        QMessageBox.Yes | QMessageBox.No
                    ) == QMessageBox.Yes

                    if open_report:
                        webbrowser.open(f"file://{os.path.abspath(report_path)}")
                else:
                    app.update_output.emit(log_message("[Cloud License] Failed to generate report"))

        return

    # Ask for port
    port, ok = QInputDialog.getInt(app, "Cloud License Proxy Port", "Enter port number:", 8080, 1024, 65535)
    if not ok:
        app.update_output.emit(log_message("[Cloud License] Cancelled"))
        return

    # Ask if learning mode should be enabled
    learning_mode = QMessageBox.question(
        app,
        "Learning Mode",
        "Enable learning mode? This will allow the generator to learn from captured traffic.",
        QMessageBox.Yes | QMessageBox.No
    ) == QMessageBox.Yes

    # Create and configure the generator
    generator = CloudLicenseResponseGenerator({
        'learning_mode': learning_mode
    })

    # Start the generator
    app.update_output.emit(log_message("[Cloud License] Starting proxy..."))
    try:
        if generator.start_proxy(port):
            app.update_output.emit(log_message(f"[Cloud License] Started proxy on port {port}"))
            app.cloud_license_instance = generator

            # Add to analyze results
            if not hasattr(app, "analyze_results"):
                app.analyze_results = []

            app.analyze_results.append("\n=== CLOUD LICENSE RESPONSE GENERATOR ===")
            app.analyze_results.append(f"Proxy running on port {port}")
            app.analyze_results.append("Learning mode: " + ("Enabled" if learning_mode else "Disabled"))
            app.analyze_results.append("\nTo use:")
            app.analyze_results.append(f"1. Configure your application to use localhost:{port} as a proxy")
            app.analyze_results.append("2. The generator will automatically respond with valid license data")
            app.analyze_results.append("3. To stop the proxy and generate a report, run the Cloud License Generator again")
