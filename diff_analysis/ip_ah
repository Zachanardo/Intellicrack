
    Args:
        packages: List of package names to install

    Returns:
        tuple: (bool, dict) - Overall success status and detailed results dictionary
    """

    logger.info("\nInstalling dependencies...")

    # Results tracking
    installation_results = {
        "succeeded": [],
        "failed": [],
        "installation_log": {}
    }

    # Base pip command with upgrade flag to ensure latest compatible version
    pip_base_cmd = [
        sys.executable,
        "-m",
        "pip",
        "install",
        "--upgrade"
    ]

    # Special cases for dependencies that need extra options
    special_installs = {
        # llama-cpp-python needs CUDA acceleration options and specific repo
        "llama-cpp-python": {
            "default": pip_base_cmd + [
                "llama-cpp-python",
                "--upgrade",
                "--extra-index-url=https://pypi.fury.io/d/scientific/",
            ],
            # GPU version if CUDA is available
            "cuda": pip_base_cmd + [
                "llama-cpp-python",
                "--upgrade",
                "--extra-index-url=https://pypi.fury.io/d/scientific/",
                "--force-reinstall",
                "--no-cache-dir",
                "--install-option=--use-cuda"
            ]
        },
        # Frida should install both frida and frida-tools
        "frida": {
            "default": pip_base_cmd + [
                "frida",
                "frida-tools"
            ]
        },
        # Capstone sometimes has issues with standard pip
        "capstone": {
            "default": pip_base_cmd + [
                "capstone",
                "--no-cache-dir"
            ]
        },
        # LIEF sometimes needs specific version options
        "lief": {
            "default": pip_base_cmd + [
                "lief>=0.12.0",
                "--no-cache-dir"
            ]
        },
        # PyQt5 may need specific version constraints
        "pyqt5": {
            "default": pip_base_cmd + [
                "pyqt5>=5.15.0",
                "--no-cache-dir"
            ]
        }
    }

    # Determine system details for specialized installs
    is_windows = platform.system() == "Windows"
    is_64bit = platform.architecture()[0] == "64bit"
    has_cuda = False

    # Check for CUDA availability if we're trying to install GPU-dependent packages
    cuda_packages = ["llama-cpp-python", "tensorflow", "pytorch", "cupy"]
    if any(pkg in packages for pkg in cuda_packages):
        try:
            # Try to detect CUDA installation
            if is_windows:
                has_cuda = os.path.exists(os.environ.get("CUDA_PATH", "C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA"))
            else:
                # Simple test for Linux/Mac
                result = subprocess.run(
                    ["nvidia-smi"],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    timeout=5  # Prevent long hang if nvidia-smi stalls
                )
                has_cuda = result.returncode == 0

            if has_cuda:
                logger.info("CUDA detected - will use GPU-accelerated versions where available")
        except (subprocess.SubprocessError, FileNotFoundError, OSError):
            logger.debug("CUDA not detected or not properly configured")
        except Exception as e:
            logger.debug(f"Error detecting CUDA: {e}")

    # Create temp directory for logs
    log_dir = tempfile.mkdtemp(prefix="intellicrack_pip_")

    # Overall success flag
    installation_success = True

    # Install packages with progress indication
    for i, package in enumerate(packages, 1):
        logger.info(f"[{i}/{len(packages)}] Installing {package}...")
        installation_results["installation_log"][package] = {}

        try:
            # Determine command for this package
            if package in special_installs:
                # Check for specialized version (e.g., CUDA)
                if has_cuda and "cuda" in special_installs[package]:
                    cmd = special_installs[package]["cuda"]
                    logger.info(f"Using CUDA-enabled version for {package}")
                elif not is_64bit:
                    # Adjust command for 32-bit systems if needed
                    cmd = special_installs[package].get("x86", special_installs[package]["default"])
                    logger.info(f"Using 32-bit version for {package}")
                else:
                    cmd = special_installs[package]["default"]

                # Log the special handling
                logger.debug(f"Using specialized install for {package}: {' '.join(cmd)}")
            else:
                cmd = pip_base_cmd + [package]

            # Log file for this package
            log_file = os.path.join(log_dir, f"{package}_install.log")

            # Run pip with timeout protection
            with open(log_file, 'w') as log:
                # Start process
                process = subprocess.Popen(
                    cmd,
                    stdout=log,
                    stderr=subprocess.STDOUT,
                    text=True,
                    bufsize=1,  # Line buffered
                )

                # Setup timeout (10 minutes)
                timeout = 600  # seconds
                timer = Timer(timeout, process.kill)

                try:
                    timer.start()
                    returncode = process.wait()
                finally:
                    timer.cancel()

            # Read the log
            with open(log_file, 'r') as log:
                install_log = log.read()
                installation_results["installation_log"][package]["log"] = install_log

            # Process result
            if returncode == 0:
                logger.info(f"✓ Successfully installed {package}")
                installation_results["succeeded"].append(package)

                # Try to get installed version
                try:
                    module = __import__(package.replace("-", "_"))
                    version = getattr(module, "__version__", "unknown version")
                    installation_results["installation_log"][package]["version"] = version
                except (ImportError, AttributeError):
                    pass
            else:
                # Analyze error for common issues
                error_type = "unknown error"

                if "PermissionError" in install_log:
                    error_type = "permission error - try running as administrator"
                elif "connection error" in install_log.lower():
                    error_type = "network error - check internet connection"
                elif "conflicts with the installed package" in install_log:
                    error_type = "dependency conflict"
                elif "command 'cl.exe' failed" in install_log:
                    error_type = "missing Visual C++ compiler"
                elif "Microsoft Visual C++" in install_log:
                    error_type = "missing Visual C++ build tools"

                logger.error(f"✗ Failed to install {package}: {error_type}")
                installation_results["failed"].append((package, error_type))
                installation_success = False

                # Provide helper messages for common errors
                if "Microsoft Visual C++" in install_log:
                    logger.info("To fix this error, install Microsoft Visual C++ Build Tools")
                    logger.info("Download from: https://visualstudio.microsoft.com/visual-cpp-build-tools/")

        except subprocess.TimeoutExpired:
            logger.error(f"✗ Installation of {package} timed out after 10 minutes")
            installation_results["failed"].append((package, "installation timeout"))
            installation_success = False

        except Exception as e:
            error_details = traceback.format_exc()
            logger.error(f"✗ Error installing {package}: {e}")
            logger.debug(f"Detailed installation error:\n{error_details}")
            installation_results["failed"].append((package, str(e)))
            installation_success = False

    # Display summary
    if installation_results["succeeded"]:
        logger.info(f"\nSuccessfully installed {len(installation_results['succeeded'])} package(s):")
        for package in installation_results["succeeded"]:
            version = installation_results["installation_log"][package].get("version", "")
            version_str = f" ({version})" if version else ""
            logger.info(f"  ✓ {package}{version_str}")

    if installation_results["failed"]:
        logger.warning(f"\nFailed to install {len(installation_results['failed'])} package(s):")
        for package, error in installation_results["failed"]:
            logger.warning(f"  ✗ {package} - {error}")

        # Provide troubleshooting advice
        logger.info("\nTroubleshooting tips:")
        logger.info("- Run as administrator if you have permission issues")
        logger.info("- Check your internet connection")
        logger.info("- Some packages require additional system dependencies")
        logger.info("- Manual installation command: pip install <package-name>")

    # Return success status and detailed results
    return installation_success, installation_results

def setup_required_environment():
    """Sets up required directories and environment for Intellicrack."""
    required_directories = [
        "models",        # For AI models
        "logs",          # For log files
        "plugins",       # For plugins
        "backups",       # For binary backups
        "scripts",       # For generated scripts
        "ghidra_scripts",  # For Ghidra scripts
        "keys",          # For generated license keys
        "training_data"  # For AI training data
    ]

    logger.info("Setting up Intellicrack environment...")

    for directory in required_directories:
        if not os.path.exists(directory):
            try:
                os.makedirs(directory)
                logger.info(f"✓ Created {directory} directory")
            except Exception as e:
                logger.error(f"✗ Failed to create {directory} directory: {e}")

    # Create initial plugin directory structure
    plugin_subdirs = ["frida_scripts", "ghidra_scripts", "custom_modules"]

    for subdir in plugin_subdirs:
        path = os.path.join("plugins", subdir)
        if not os.path.exists(path):
            try:
                os.makedirs(path)
                logger.info(f"✓ Created {path} directory")
            except Exception as e:
                logger.error(f"✗ Failed to create {path} directory: {e}")

    # Create sample plugins for each type
    create_sample_plugins()


def create_sample_plugins():
    """Creates sample plugins in the plugins directory."""
    # Sample Frida script
    frida_sample = """
// Sample Frida script: Registry Monitor
// This script hooks Windows Registry functions and logs access to licensing-related keys
Java.perform(function() {
    var registryKeys = [
        "HKEY_LOCAL_MACHINE\\\\SOFTWARE\\\\Microsoft\\\\Windows NT\\\\CurrentVersion",
        "HKEY_CURRENT_USER\\\\Software"
    ];

    // Hook RegOpenKeyExW
    var RegOpenKeyExW = Module.findExportByName("advapi32.dll", "RegOpenKeyExW");
    if (RegOpenKeyExW) {
        Interceptor.attach(RegOpenKeyExW, {
            onEnter: function(args) {
                var keyPath = args[1].readUtf16String();
                if (keyPath && registryKeys.some(key => keyPath.includes(key))) {
                    console.log("[Registry] Opening key: " + keyPath);
                }
            }
        });
    }

    // Hook RegQueryValueExW
    var RegQueryValueExW = Module.findExportByName("advapi32.dll", "RegQueryValueExW");
    if (RegQueryValueExW) {
        Interceptor.attach(RegQueryValueExW, {
            onEnter: function(args) {
                this.valueName = args[1].readUtf16String();
            },
            onLeave: function(retval) {
                if (this.valueName && this.valueName.toLowerCase().includes("licens")) {
                    console.log("[Registry] Querying value: " + this.valueName);
                }
            }
        });
    }
});
"""

    # Sample Ghidra script
    ghidra_sample = """
//Sample Ghidra Script for Intellicrack
//@category SecurityResearch

import ghidra.app.script.GhidraScript;
import ghidra.program.model.symbol.*;
import ghidra.program.model.listing.*;

public class LicensePatternScanner extends GhidraScript {

    @Override
    public void run() throws Exception {
        println("License Pattern Scanner starting...");

        // Search for license-related symbols
        SymbolTable symbolTable = currentProgram.getSymbolTable();
        SymbolIterator symbols = symbolTable.getAllSymbols(true);

        int licenseRelatedCount = 0;

        for (Symbol symbol : symbols) {
            String name = symbol.getName().toLowerCase();
            if (name.contains("licens") || name.contains("serial") ||
                name.contains("activ") || name.contains("valid")) {
                println("Found license-related symbol: " + symbol.getName() +
                       " at " + symbol.getAddress());
                licenseRelatedCount++;
            }
        }

        println("License Pattern Scanner completed. Found " + licenseRelatedCount +
               " license-related symbols.");
    }
}
"""

    # Sample custom module
    custom_module = """
# Sample custom module for Intellicrack
# This module demonstrates how to create a custom plugin

class DemoPlugin:
    \"\"\"
    Demo plugin that shows how to integrate with Intellicrack
    \"\"\"
    def __init__(self):
        self.name = "Demo Plugin"
        self.version = "1.0"
        self.description = "Demonstrates Intellicrack plugin architecture"

    def analyze(self, binary_path):
        \"\"\"Analyze the given binary.\"\"\"
        results = []
        results.append(f"Demo plugin analyzing: {binary_path}")
        results.append("This is where your custom analysis code would run")
        results.append("You can return results as a list of strings")
        return results

    def patch(self, binary_path):
        \"\"\"Patch the given binary.\"\"\"
        results = []
        results.append(f"Demo plugin would patch: {binary_path}")
        results.append("This is where your custom patching code would run")
        return results

# Function to register this plugin with Intellicrack
def register():
    return DemoPlugin()
"""

    # Write sample plugins to files
    try:
        frida_path = os.path.join("plugins", "frida_scripts", "registry_monitor.js")
        with open(frida_path, "w", encoding="utf-8") as f:
            f.write(frida_sample)
        logger.info(f"Sample Frida plugin written to {frida_path}")

        ghidra_path = os.path.join("plugins", "ghidra_scripts", "LicensePatternScanner.java")
        with open(ghidra_path, "w", encoding="utf-8") as f:
            f.write(ghidra_sample)
        logger.info(f"Sample Ghidra plugin written to {ghidra_path}")

        custom_path = os.path.join("plugins", "custom_modules", "demo_plugin.py")
        with open(custom_path, "w", encoding="utf-8") as f:
            f.write(custom_module)
        logger.info(f"Sample custom module written to {custom_path}")

        logger.info("✓ Created all sample plugins successfully")
    except Exception as e:
        logger.error(f"✗ Failed to create sample plugins: {e}")


# Check for first run and initialize environment if needed
if __name__ == "__main__":
    # Check for first run
    if not CONFIG.get("first_run_completed", False):
        print("First run detected. Setting up Intellicrack environment...")
        setup_required_environment()
        if check_and_install_dependencies():
            # Update config to mark first run complete
            CONFIG["first_run_completed"] = True
            with open("intellicrack_config.json", "w", encoding="utf-8") as f:
                json.dump(CONFIG, f, indent=2)
            logger.info("Intellicrack initialization complete")
        else:
            logger.error("Failed to initialize Intellicrack dependencies")

def compute_file_hash(file_path, algorithm='sha256', progress_signal=None):
    """
    Computes the hash of a file using the specified algorithm with optional progress updates via signal.

    Calculates the cryptographic hash of the specified file using the given algorithm, reading it
    in chunks to handle large files efficiently. Can provide progress updates
    through a signal mechanism for UI integration.

    Args:
        file_path: Path to the file to hash
        algorithm (str): The hashing algorithm to use (e.g., 'sha256', 'md5'). Defaults to 'sha256'.
        progress_signal: Optional signal to emit progress updates (0-100%)

    Returns:
        str: Hexadecimal representation of the computed hash
    """
    try:
        hasher = hashlib.new(algorithm.lower())
        file_size = os.path.getsize(file_path)
        chunk_size = 4096 * 1024  # 4MB chunks
        processed = 0

        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(chunk_size), b""):
                hasher.update(chunk)
                processed += len(chunk)
                if progress_signal and file_size > 0:
                    progress_percent = int((processed / file_size) * 100)
                    # Handle both signal objects and function callbacks
                    if hasattr(progress_signal, 'emit'):
                        # If it's a signal object with emit method
                        progress_signal.emit(progress_percent)
                    else:
                        # If it's a function callback
                        progress_signal(progress_percent)

        return hasher.hexdigest()
    except Exception as e:
        error_message = f"Error computing hash for {file_path} with algorithm {algorithm}: {e}"
        error_type = type(e).__name__

        # Add more context to the error message
        if isinstance(e, FileNotFoundError):
            error_message = f"File not found when computing hash: {file_path}"
        elif isinstance(e, PermissionError):
            error_message = f"Permission denied when computing hash: {file_path}"
        elif isinstance(e, IOError):
            error_message = f"IO Error when computing hash: {file_path} - {str(e)}"
        elif isinstance(e, ValueError) and "unsupported hash type" in str(e).lower():
            error_message = f"Unsupported hash algorithm '{algorithm}': {e}"


        # Log the exception if logger is accessible, otherwise print
        try:
            logger.exception(f"{error_type} - {error_message}")
        except NameError:  # logger might not be defined if this is called standalone
            print(f"{error_type} - {error_message}")

        # Include traceback information for debugging
        traceback_info = traceback.format_exc()
        try:
            logger.debug(f"Traceback: {traceback_info}")
        except NameError:
            print(f"Traceback: {traceback_info}")

        return ""

# -------------------------------
# Centralized Model Management
# -------------------------------


def load_ai_model(self):
    """
    Centralizes AI model loading from the user-specified path with proper caching
    and enhanced error handling. Prevents repeated loads.
    """
    global LOADED_AI_MODEL
    if LOADED_AI_MODEL is not None:
        # Check if the model instance is still valid (simple check)
        try:
            # Perform a trivial operation to check if model object is usable
            _ = LOADED_AI_MODEL.n_ctx()
            return LOADED_AI_MODEL
        except Exception as e:
            self.update_output.emit(
                log_message(
                    f"[AI Model] Existing model instance check failed: {e}. Reloading."))
            LOADED_AI_MODEL = None  # Force reload

    # Initialize the model manager if it doesn't exist
    if not hasattr(self, 'model_manager'):
        self.model_manager = ModelManager(CONFIG)
        self.update_output.emit(log_message("[Model] Initialized ModelManager"))

    model_path = self.selected_model_path

    self.update_output.emit(log_message(f"[AI Model] Attempting to load AI model. Current path: {self.selected_model_path}"))

    # Check if a model path is selected and the file exists
    if not model_path or not os.path.exists(model_path):
        self.update_output.emit(log_message("[AI Model] Error: No valid model path selected."))
        # Consider showing a message box here if this is a critical operation
        # QMessageBox.warning(self, "Model Not Found", "No valid AI model file selected or found.")
        return None

    # Determine optimal context size based on available system memory
    available_memory_gb = 8  # Default fallback value
    try:
        available_memory_gb = psutil.virtual_memory().available / (1024 * 1024 * 1024)
        print(f"DEBUG: Detected available memory: {available_memory_gb:.2f} GB")
    except (ImportError, Exception) as e:
        self.update_output.emit(log_message(
            f"[AI Model] Memory detection issue: {str(e)}. Using default context size."))
        print(f"DEBUG: Using fallback memory value of {available_memory_gb} GB due to: {str(e)}")

    context_size = CONFIG.get("context_size", 8192)  # Get default from config

    # Adjust context size based on available memory (example thresholds)
    if available_memory_gb < 8:
        context_size = min(context_size, 4096)  # Reduce if low memory
        self.update_output.emit(
            log_message(
                f"[AI Model] Low memory detected (<8GB). Adjusting context size to {context_size}."))
    elif available_memory_gb >= 16:
        # Increase if high memory, up to a limit if needed
        context_size = max(context_size, 16384)
        self.update_output.emit(
            log_message(
                f"[AI Model] High memory detected (>=16GB). Adjusting context size to {context_size}."))
    else:
        self.update_output.emit(log_message(
            f"[AI Model] Using configured context size: {context_size}"))

    # Load the model using llama-cpp-python
    self.update_output.emit(
        log_message(
            f"[AI Model] Loading model '{
                os.path.basename(model_path)}' with context size {context_size}... (This may take time)"))

    # Enhanced diagnostics for model loading
    try:
        mem = psutil.virtual_memory()
        print(f"DEBUG: Memory before model loading - Total: {mem.total/(1024**3):.2f}GB, Available: {mem.available/(1024**3):.2f}GB, Used: {mem.used/(1024**3):.2f}GB ({mem.percent}%)")
    except ImportError:
        print("DEBUG: psutil not available for memory diagnostics")
        psutil = None # Ensure psutil is None if import fails

    # Check model file details
    print(f"DEBUG: Final model path: {model_path}")
    if os.path.exists(model_path):
        filesize = os.path.getsize(model_path) / (1024**3)  # GB
        print(f"DEBUG: Model file exists, size: {filesize:.2f}GB")
    else:
        print(f"DEBUG: ERROR - Model file not found at {model_path}")

    print("DEBUG: Importing Llama module...")

    # Check Llama.cpp version
    try:
        from llama_cpp import __version__ as llama_version
        print(f"DEBUG: llama_cpp version: {llama_version}")
    except:
        print("DEBUG: Unable to determine llama_cpp version")

    try:
        # n_gpu_layers=-1 tries to offload all possible layers to GPU if available
        # Set verbose=True for loading diagnostics
        print(f"DEBUG: Creating Llama instance with: path={model_path}, n_ctx={context_size}, n_threads=0, n_gpu_layers=-1, verbose=True")
        self.update_output.emit(log_message(f"[AI Model] Loading model '{os.path.basename(model_path)}' with context size {context_size}..."))
        from llama_cpp import Llama
        LOADED_AI_MODEL = Llama(model_path=model_path, n_ctx=context_size,
                                n_threads=0, n_gpu_layers=-1, verbose=True)

        print("DEBUG: Model loaded successfully!")
        self.update_output.emit(log_message(
            "[AI Model] AI model loaded successfully."))
        return LOADED_AI_MODEL
    except ImportError as ie:
        print(f"DEBUG: ImportError while loading model: {str(ie)}")
        self.update_output.emit(log_message(
            f"[AI Model] CRITICAL ERROR: llama-cpp-python library not found. AI features unavailable. Details: {str(ie)}"))
        # QMessageBox.critical(self, "Dependency Error", "llama-cpp-python not found. Please install it.")
        LOADED_AI_MODEL = None
        return None
    except Exception as e:
        # Catch errors during Llama initialization (e.g., model file corruption, incompatibility)
        print(f"DEBUG: Exception during model loading: {type(e).__name__}: {str(e)}")
        print(f"DEBUG: Traceback: {traceback.format_exc()}")

        error_msg = f"[AI Model] CRITICAL ERROR loading model '{os.path.basename(model_path)}': {str(e)}"
        self.update_output.emit(log_message(error_msg))

        # Provide more helpful diagnostics based on error type
        if "Failed to load model from file" in str(e):
            print("DEBUG: Model file loading failed - likely corrupted or incompatible")
            self.update_output.emit(log_message(
                f"[AI Model] Ensure the model file is a valid GGUF file compatible with llama-cpp-python."))
        elif "out of memory" in str(e).lower():
            print("DEBUG: Out of memory error detected")
            self.update_output.emit(log_message(
                f"[AI Model] Not enough memory to load the model. Try closing other applications or using a smaller model."))
        else:
            self.update_output.emit(log_message(
                f"[AI Model] An unexpected error occurred during model loading."))

        # Optionally clear the path if loading fails:
        # self.selected_model_path = None
        # if hasattr(self, 'custom_model_path_label'):
        #     self.custom_model_path_label.setText("None (Loading failed)")
        # self.save_config()

        LOADED_AI_MODEL = None  # Ensure model is not partially loaded
        return None

# -------------------------------
# Simple Dependency Update Manager
# -------------------------------



# -------------------------------
# Advanced License Detection
# -------------------------------


def detect_hardware_dongles(app=None):
    """
    Detects hardware dongle drivers and APIs.
    Supports detection of SafeNet, HASP, CodeMeter, and other common dongles.
    """
    logging.info("Starting hardware dongle detection.")
    results = []

    # Known hardware dongle drivers and DLLs
    dongle_drivers = {
        "SafeNet": ["sentinel.sys", "sentinelkeyW.dll", "hasp_windows_x64_demo.dll"],
        "HASP": ["haspvb32.dll", "haspdos.sys", "haspds_windows.dll", "hasp_windows_demo.dll"],
        "CodeMeter": ["codemeter.exe", "wibukey.dll", "wibusys.dll"],
        "Rainbow": ["rainbow.dll", "rainbow.sys"],
        "ROCKEY": ["rockey.dll", "rockeydrv.sys"],
        "Hardlock": ["hlock.sys", "hlock.dll"],
        "Matrix": ["matrix.sys", "matrix.dll"],
        "Keylok": ["keylok.sys", "keylok3.sys"]
    }

    # Check installed drivers in system directories
    system_dirs = [
        os.environ.get("SystemRoot", "C:\\Windows"),
        os.path.join(os.environ.get("SystemRoot", "C:\\Windows"), "System32"),
        os.path.join(os.environ.get("SystemRoot", "C:\\Windows"), "SysWOW64"),
        os.path.join(os.environ.get("SystemRoot", "C:\\Windows"),
                     "system32\\drivers")
    ]

    results.append("Scanning for hardware dongle drivers...")

    found_dongles = set()

    for dir_path in system_dirs:
        if not os.path.exists(dir_path):
            continue

        logging.debug(f"Scanning directory for dongle drivers: {dir_path}")

        for dongle, files in dongle_drivers.items():
            for file in files:
                if os.path.exists(os.path.join(dir_path, file)):
                    found_dongles.add(dongle)
                    driver_path = os.path.join(dir_path, file)
                    logging.info(f"Found {dongle} driver: {driver_path}")
                    results.append(f"Found {dongle} dongle driver: {driver_path}")

    # Check running processes for dongle service processes
    dongle_processes = {
        "SafeNet": ["hasplmd.exe", "hasplms.exe", "aksmon.exe"],
        "HASP": ["hasplmd.exe", "hasplms.exe"],
        "CodeMeter": ["codemeter.exe", "CodeMeterCC.exe"],
        "Hardlock": ["hldrv.exe"],
        "Matrix": ["matrix.exe"],
        "Keylok": ["keylok.exe", "keylokd.exe"]
    }

    try:
        running_processes = [p.name().lower() for p in psutil.process_iter()]

        for dongle, processes in dongle_processes.items():
            for process in processes:
                if process.lower() in running_processes:
                    found_dongles.add(dongle)
                    logging.info(f"Found {dongle} service process: {process}")
                    results.append(
                        f"Found {dongle} dongle service process: {process}")
    except Exception as e:
        results.append(f"Error scanning for dongle service processes: {e}")

    # Check for imports of dongle-related APIs in the binary
    binary_path = getattr(app, "binary_path", None) if app else None
    if binary_path:
        logging.info(f"Scanning binary {binary_path} for dongle API imports.")
        try:
            pe = pefile.PE(binary_path)

            # Look for imports
            if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
                for entry in pe.DIRECTORY_ENTRY_IMPORT:
                    dll_name = entry.dll.decode().lower()

                    for dongle, files in dongle_drivers.items():
                        for file in files:
                            if file.lower() in dll_name:
                                found_dongles.add(dongle)
                                logging.info(f"Binary imports {dongle} API: {dll_name}")
                                results.append(
                                    f"Binary imports {dongle} dongle API: {dll_name}")
        except Exception as e:
            logging.error(f"Error during dongle detection: {e}", exc_info=True)
            results.append(
                f"Error scanning binary for dongle API imports: {e}")

    # Summary
    if found_dongles:
        summary = f"Detected hardware dongles: {', '.join(found_dongles)}"
    else:
        summary = "No hardware dongles detected"

    logging.info(f"Hardware dongle detection complete. Detected: {', '.join(found_dongles) if found_dongles else 'None'}")
    results.append(summary)

    return results


def detect_tpm_protection(app=None):
    """
    Detects the use of TPM (Trusted Platform Module) for licensing.
    """
    results = []
    results.append("Scanning for TPM-based protection...")

    # Check for TPM related DLLs and drivers
    tpm_components = [
        "tpm.sys",
        "tbs.dll",
        "tbssvc.dll",
        "tpm.dll",
        "TpmCmp.dll",
        "TPMCoreProvisioning.dll"
    ]

    system_dirs = [
        os.environ.get("SystemRoot", "C:\\Windows"),
        os.path.join(os.environ.get("SystemRoot", "C:\\Windows"), "System32"),
        os.path.join(os.environ.get("SystemRoot", "C:\\Windows"),
                     "system32\\drivers")
    ]

    tpm_files_found = []

    for dir_path in system_dirs:
        if not os.path.exists(dir_path):
            continue

        for component in tpm_components:
            component_path = os.path.join(dir_path, component)
            if os.path.exists(component_path):
                tpm_files_found.append(component)

    # Check for TPM services
    tpm_services = ["TPM", "TBS"]
    running_tpm_services = []

    try:
        for service in tpm_services:
            try:
                status = win32serviceutil.QueryServiceStatus(service)[1]
                if status == win32service.SERVICE_RUNNING:
                    running_tpm_services.append(service)
            except (WindowsError, Exception) as e:
                # Service not found or access denied
                logger.debug(f"TPM service check error: {e}")
                pass
    except Exception as e:
        results.append(f"Error checking TPM services: {e}")

    # Look for TPM API calls in the binary
    tpm_api_found = False
    binary_path = getattr(app, "binary_path", None) if app else None

    if binary_path:
        try:
            pe = pefile.PE(binary_path)

            if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
                for entry in pe.DIRECTORY_ENTRY_IMPORT:
                    dll_name = entry.dll.decode().lower()

                    if "tbs" in dll_name or "tpm" in dll_name:
                        for imp in entry.imports:
                            if imp.name:
                                imp_name = imp.name.decode()
                                if "Tbsi" in imp_name or "TPM" in imp_name:
                                    tpm_api_found = True
                                    results.append(
                                        f"Binary calls TPM API: {imp_name}")
        except Exception as e:
            results.append(f"Error scanning binary for TPM API calls: {e}")

    # String scanning for TPM related strings in the binary
    tpm_strings_found = False

    if binary_path:
        try:
            with open(binary_path, 'rb') as f:
                binary_data = f.read()

                tpm_keywords = [b"TPM", b"TrustedPlatformModule",
                                b"Tbsi_", b"TPM_", b"TPM2_"]

                for keyword in tpm_keywords:
                    if keyword in binary_data:
                        tpm_strings_found = True
                        results.append(
                            f"Found TPM reference in binary: {
                                keyword.decode()}")
        except Exception as e:
            results.append(f"Error scanning binary for TPM strings: {e}")

    # Analyze TPM protection level
    if tpm_api_found or (len(running_tpm_services) > 0 and tpm_strings_found):
        results.append("TPM-based protection DETECTED (High confidence)")
    elif tpm_strings_found or len(running_tpm_services) > 0:
        results.append("Possible TPM-based protection (Medium confidence)")
    elif len(tpm_files_found) > 0:
        results.append(
            "TPM components found but protection not detected (Low confidence)")
    else:
        results.append("No TPM-based protection detected")

    return results


def detect_virtualization_protection(app=None):
    """
    Detects anti-virtualization and container detection mechanisms in the binary.

    This function analyzes a binary file for code patterns commonly used to detect virtualization,
    sandboxes, and debugging environments. It checks for VM-related strings, API imports commonly
    used for detection, and instruction patterns like CPUID and RDTSC that are often used to
    detect virtualized environments.

    Args:
        app: Application instance that contains the binary_path attribute, or None

    Returns:
        list: A list of detection results and findings
        int: Detection level (0-7) indicating severity of anti-VM techniques
    """
    results = []
    results.append("Scanning for virtualization detection mechanisms...")

    # Enhanced VM detection patterns with more common checks
    vm_detection_patterns = [
        b"vmware", b"VirtualBox", b"VIRTUAL", b"VBOX",
        b"qemu", b"hyperv", b"xen", b"parallels",
        b"vmcheck", b"sandbox", b"SbieDll", b"Wine",
        b"VBoxService", b"VMTools", b"HGFS", b"vmhgfs",
        b"QEMU", b"KVM", b"BOCHS", b".vbox"
    ]

    # VM detection APIs with categorization
    vm_detection_apis = {
        "debugger": [
            "CheckRemoteDebuggerPresent",
            "IsDebuggerPresent",
            "OutputDebugString",
            "DebugActiveProcess"
        ],
        "timing": [
            "GetTickCount",
            "QueryPerformanceCounter",
            "QueryPerformanceFrequency",
            "timeGetTime"
        ],
        "system_info": [
            "GetSystemFirmwareTable",
            "EnumDeviceDrivers",
            "NtQuerySystemInformation",
            "GetSystemInfo",
            "DeviceIoControl",
            "GetAdaptersInfo"
        ],
        "registry": [
            "RegOpenKeyExA",
            "RegQueryValueExA",
            "RegEnumKeyExA"
        ]
    }

    vm_strings_found = []
    vm_apis_found = {category: [] for category in vm_detection_apis}

    binary_path = getattr(app, "binary_path", None) if app else None
    if not binary_path or not os.path.exists(binary_path):
        results.append("No valid binary path provided or binary not found")
        return results, 0

    # Check for VM detection strings
    try:
        with open(binary_path, 'rb') as f:
            binary_data = f.read()

            for pattern in vm_detection_patterns:
                # Case-insensitive search using lower() on both sides
                pattern_lower = pattern.lower()
                binary_lower = binary_data.lower()

                if pattern_lower in binary_lower:
                    pattern_str = pattern.decode(errors='replace')  # Safely decode with replacement for invalid chars
                    vm_strings_found.append(pattern_str)

                    # Log the offset for better analysis
                    offset = binary_lower.find(pattern_lower)
                    context = binary_data[max(0, offset-10):offset+len(pattern)+10]
                    context_str = repr(context)
                    logger.debug(f"Found VM string '{pattern_str}' at offset 0x{offset:X}, context: {context_str}")

    except FileNotFoundError:
        results.append(f"Binary file not found: {binary_path}")
    except PermissionError:
        results.append(f"Permission denied when accessing binary: {binary_path}")
    except OSError as e:
        results.append(f"OS error when reading binary: {str(e)}")
    except Exception as e:
        error_details = traceback.format_exc()
        results.append(f"Error scanning binary for virtualization detection strings: {str(e)}")
        logger.debug(f"String scanning error details:\n{error_details}")

    # Check for VM detection APIs
    try:
        pe = pefile.PE(binary_path)

        if hasattr(pe, 'DIRECTORY_ENTRY_IMPORT'):
            for entry in pe.DIRECTORY_ENTRY_IMPORT:
                dll_name = entry.dll.decode(errors='replace').lower() if entry.dll else "unknown"

                for imp in entry.imports:
                    if imp.name:
                        try:
                            imp_name = imp.name.decode(errors='replace')
                        except Exception as e:
                            logger.warning(f"Import analysis error: {e}")
                            imp_name = f"Unknown-{imp.ordinal}" if hasattr(imp, 'ordinal') else "Unknown"

                        # Check against all categories of VM detection APIs
                        for category, apis in vm_detection_apis.items():
                            for api in apis:
                                if api.lower() in imp_name.lower():
                                    vm_apis_found[category].append(f"{imp_name} (from {dll_name})")

    except ImportError:
        results.append("Pefile module not available - skipping API detection")
    except pefile.PEFormatError:
        results.append(f"Not a valid PE file: {binary_path}")
    except Exception as e:
        error_details = traceback.format_exc()
        results.append(f"Error scanning for VM detection APIs: {str(e)}")
        logger.debug(f"API scanning error details:\n{error_details}")

    # Check for instruction patterns commonly used for VM detection
    has_cpuid_check = False
    has_rdtsc_check = False
    has_sidt_sgdt_check = False
    has_registry_vm_check = False
    has_in_instruction = False  # I/O port access often used for VM detection

    instruction_patterns = {}  # Store found patterns with their offsets
