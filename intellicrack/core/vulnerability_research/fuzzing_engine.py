"""This file is part of Intellicrack.
Copyright (C) 2025 Zachary Flint

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

"""
Fuzzing Engine

Intelligent fuzzing framework for vulnerability discovery with multiple
fuzzing strategies, coverage-guided fuzzing, and crash analysis.
"""

import logging
import multiprocessing
import os
import random
import re
import struct
import subprocess
import tempfile
import time
from enum import Enum
from typing import Any

from ...models import VulnerabilityLevel

logger = logging.getLogger(__name__)


class FuzzingStrategy(Enum):
    """Fuzzing strategies"""

    RANDOM = "random"
    MUTATION = "mutation"
    GENERATION = "generation"
    GRAMMAR_BASED = "grammar_based"
    COVERAGE_GUIDED = "coverage_guided"
    HYBRID = "hybrid"


# Use VulnerabilityLevel as CrashSeverity for consistency
CrashSeverity = VulnerabilityLevel


class FuzzingEngine:
    """Advanced fuzzing engine with multiple strategies and crash analysis.
    """

    def __init__(self):
        """Initialize the fuzzing engine with mutation strategies and crash detection capabilities."""
        self.logger = logging.getLogger("IntellicrackLogger.FuzzingEngine")

        # Fuzzing configuration
        self.config = {
            "max_iterations": 10000,
            "max_file_size": 1024 * 1024,  # 1MB
            "timeout": 30,  # seconds
            "parallel_workers": multiprocessing.cpu_count(),
            "crash_detection": True,
            "coverage_collection": False,
            "mutation_rate": 0.1,
            "crossover_rate": 0.3,
        }

        # Mutation strategies
        self.mutation_strategies = {
            "bit_flip": self._mutate_bit_flip,
            "byte_flip": self._mutate_byte_flip,
            "arithmetic": self._mutate_arithmetic,
            "insert": self._mutate_insert,
            "delete": self._mutate_delete,
            "duplicate": self._mutate_duplicate,
            "splice": self._mutate_splice,
            "magic_values": self._mutate_magic_values,
            "string_replace": self._mutate_string_replace,
            "format_aware": self._mutate_format_aware,
        }

        # Magic values for mutations
        self.magic_values = {
            "integers": [0, 1, -1, 255, 256, 65535, 65536, 2147483647, -2147483648],
            "strings": [b"A" * 1000, b"\x00" * 100, b"\xff" * 100, b"%s%s%s%s"],
            "format_strings": [b"%n", b"%x", b"%s", b"%d", b"%p"],
            "special_chars": [b"\x00", b"\xff", b"\x7f", b"\x80", b"\x0a", b"\x0d"],
        }

        # File format grammars
        self.file_grammars = {
            "text": self._generate_text_grammar,
            "xml": self._generate_xml_grammar,
            "json": self._generate_json_grammar,
            "http": self._generate_http_grammar,
            "binary": self._generate_binary_grammar,
        }

        # Coverage data
        self.coverage_data = {}
        self.interesting_inputs = []

        # Crash analysis
        self.crashes = {}
        self.unique_crashes = {}

        # Statistics
        self.stats = {
            "total_executions": 0,
            "crashes_found": 0,
            "unique_crashes": 0,
            "hangs_found": 0,
            "coverage_paths": 0,
            "execution_speed": 0.0,
            "start_time": 0,
            "elapsed_time": 0,
        }

    def start_fuzzing(self,
                     target_command: str,
                     seed_inputs: list[str] | None = None,
                     strategy: FuzzingStrategy = FuzzingStrategy.HYBRID,
                     max_iterations: int | None = None) -> dict[str, Any]:
        """Start fuzzing campaign against target.

        Args:
            target_command: Command to execute target (use @@@ for input file placeholder)
            seed_inputs: List of seed input files
            strategy: Fuzzing strategy to use
            max_iterations: Maximum number of iterations

        Returns:
            Fuzzing campaign results

        """
        result = {
            "success": False,
            "strategy": strategy.value,
            "target_command": target_command,
            "iterations_completed": 0,
            "crashes_found": [],
            "unique_crashes": [],
            "statistics": {},
            "campaign_id": self._generate_campaign_id(),
            "error": None,
        }

        try:
            self.logger.info(f"Starting fuzzing campaign: {strategy.value}")

            # Initialize campaign
            self.stats["start_time"] = time.time()
            self.stats["total_executions"] = 0

            # Set configuration
            if max_iterations:
                self.config["max_iterations"] = max_iterations

            # Prepare seed inputs
            if not seed_inputs:
                seed_inputs = self._generate_default_seeds()

            # Validate target command
            if "@@@" not in target_command:
                result["error"] = "Target command must contain @@@ placeholder for input file"
                return result

            # Create output directories
            output_dir = self._create_output_directories(result["campaign_id"])

            # Execute fuzzing strategy
            if strategy == FuzzingStrategy.RANDOM:
                campaign_result = self._random_fuzzing(target_command, seed_inputs, output_dir)
            elif strategy == FuzzingStrategy.MUTATION:
                campaign_result = self._mutation_fuzzing(target_command, seed_inputs, output_dir)
            elif strategy == FuzzingStrategy.GENERATION:
                campaign_result = self._generation_fuzzing(target_command, output_dir)
            elif strategy == FuzzingStrategy.GRAMMAR_BASED:
                campaign_result = self._grammar_based_fuzzing(target_command, seed_inputs, output_dir)
            elif strategy == FuzzingStrategy.COVERAGE_GUIDED:
                campaign_result = self._coverage_guided_fuzzing(target_command, seed_inputs, output_dir)
            elif strategy == FuzzingStrategy.HYBRID:
                campaign_result = self._hybrid_fuzzing(target_command, seed_inputs, output_dir)
            else:
                result["error"] = f"Unknown fuzzing strategy: {strategy.value}"
                return result

            # Finalize results
            self.stats["elapsed_time"] = time.time() - self.stats["start_time"]

            result.update(campaign_result)
            result["statistics"] = self.stats.copy()
            result["success"] = True

            self.logger.info(f"Fuzzing campaign completed: {result['iterations_completed']} iterations")
            return result

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.error(f"Fuzzing campaign failed: {e}")
            result["error"] = str(e)
            return result

    def analyze_crash(self,
                     crash_file: str,
                     target_command: str) -> dict[str, Any]:
        """Analyze crash for exploitability and root cause.

        Args:
            crash_file: Path to input that caused crash
            target_command: Target command used

        Returns:
            Crash analysis results

        """
        result = {
            "success": False,
            "crash_file": crash_file,
            "crash_type": None,
            "severity": CrashSeverity.INFO.value,
            "exploitability": "unknown",
            "root_cause": None,
            "stack_trace": None,
            "registers": {},
            "memory_info": {},
            "recommendations": [],
            "error": None,
        }

        try:
            self.logger.info(f"Analyzing crash: {crash_file}")

            if not os.path.exists(crash_file):
                result["error"] = f"Crash file not found: {crash_file}"
                return result

            # Execute target with crash input under debugger
            debug_info = self._debug_crash(target_command, crash_file)

            if debug_info:
                # Analyze crash information
                crash_analysis = self._analyze_crash_info(debug_info)
                result.update(crash_analysis)

                # Assess exploitability
                exploitability = self._assess_exploitability(debug_info, crash_analysis)
                result["exploitability"] = exploitability

                # Generate recommendations
                recommendations = self._generate_crash_recommendations(crash_analysis)
                result["recommendations"] = recommendations

                result["success"] = True
            else:
                result["error"] = "Failed to reproduce crash under debugger"

            return result

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.error(f"Crash analysis failed: {e}")
            result["error"] = str(e)
            return result

    def minimize_testcase(self,
                         crash_file: str,
                         target_command: str,
                         strategy: str = "binary_search") -> dict[str, Any]:
        """Minimize crash testcase to smallest reproducing input.

        Args:
            crash_file: Path to crash input
            target_command: Target command
            strategy: Minimization strategy

        Returns:
            Minimization results

        """
        result = {
            "success": False,
            "original_file": crash_file,
            "minimized_file": None,
            "original_size": 0,
            "minimized_size": 0,
            "reduction_ratio": 0.0,
            "iterations": 0,
            "error": None,
        }

        try:
            self.logger.info(f"Minimizing testcase: {crash_file}")

            if not os.path.exists(crash_file):
                result["error"] = f"Crash file not found: {crash_file}"
                return result

            # Read original input
            with open(crash_file, "rb") as f:
                original_data = f.read()

            result["original_size"] = len(original_data)

            # Perform minimization
            if strategy == "binary_search":
                minimized_data = self._minimize_binary_search(original_data, target_command)
            elif strategy == "delta_debugging":
                minimized_data = self._minimize_delta_debugging(original_data, target_command)
            else:
                result["error"] = f"Unknown minimization strategy: {strategy}"
                return result

            # Save minimized input
            minimized_file = crash_file.replace(".crash", ".minimized")
            with open(minimized_file, "wb") as f:
                f.write(minimized_data)

            result["minimized_file"] = minimized_file
            result["minimized_size"] = len(minimized_data)
            result["reduction_ratio"] = 1.0 - (len(minimized_data) / len(original_data))
            result["success"] = True

            self.logger.info(f"Testcase minimized: {result['original_size']} -> {result['minimized_size']} bytes")
            return result

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.error(f"Testcase minimization failed: {e}")
            result["error"] = str(e)
            return result

    def _random_fuzzing(self, target_command: str, seed_inputs: list[str], output_dir: str) -> dict[str, Any]:
        """Perform random fuzzing."""
        result = {
            "iterations_completed": 0,
            "crashes_found": [],
            "unique_crashes": [],
        }

        try:
            self.logger.info("Starting random fuzzing")

            for iteration in range(self.config["max_iterations"]):
                # Generate random input
                if seed_inputs and random.random() < 0.5:  # noqa: S311
                    # Use seed as base
                    seed_file = random.choice(seed_inputs)  # noqa: S311
                    with open(seed_file, "rb") as f:
                        base_data = f.read()
                    fuzz_data = self._randomize_data(base_data)
                else:
                    # Generate completely random data
                    size = random.randint(1, self.config["max_file_size"])
                    fuzz_data = bytes([random.randint(0, 255) for _ in range(size)])

                # Execute target
                execution_result = self._execute_target(target_command, fuzz_data, output_dir)

                if execution_result["crashed"]:
                    crash_info = self._process_crash(execution_result, fuzz_data, output_dir, iteration)
                    result["crashes_found"].append(crash_info)

                result["iterations_completed"] += 1
                self.stats["total_executions"] += 1

                # Progress logging
                if iteration % 1000 == 0:
                    self.logger.info(f"Random fuzzing progress: {iteration}/{self.config['max_iterations']}")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Random fuzzing error: {e}")

        return result

    def _mutation_fuzzing(self, target_command: str, seed_inputs: list[str], output_dir: str) -> dict[str, Any]:
        """Perform mutation-based fuzzing."""
        result = {
            "iterations_completed": 0,
            "crashes_found": [],
            "unique_crashes": [],
        }

        try:
            self.logger.info("Starting mutation fuzzing")

            # Load seed inputs
            seed_data = []
            for seed_file in seed_inputs:
                try:
                    with open(seed_file, "rb") as f:
                        seed_data.append(f.read())
                except (OSError, ValueError, RuntimeError) as e:
                    self.logger.error("Error in fuzzing_engine: %s", e)
                    continue

            if not seed_data:
                seed_data = [b"AAAA"]  # Minimal seed

            for iteration in range(self.config["max_iterations"]):
                # Select seed
                base_data = random.choice(seed_data)

                # Apply mutations
                mutated_data = self._apply_mutations(base_data)

                # Execute target
                execution_result = self._execute_target(target_command, mutated_data, output_dir)

                if execution_result["crashed"]:
                    crash_info = self._process_crash(execution_result, mutated_data, output_dir, iteration)
                    result["crashes_found"].append(crash_info)

                # Add interesting inputs to seed pool
                if self._is_interesting_input(execution_result):
                    seed_data.append(mutated_data)

                result["iterations_completed"] += 1
                self.stats["total_executions"] += 1

                # Progress logging
                if iteration % 1000 == 0:
                    self.logger.info(f"Mutation fuzzing progress: {iteration}/{self.config['max_iterations']}")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Mutation fuzzing error: {e}")

        return result
    def _generation_fuzzing(self, target_command: str, output_dir: str) -> dict[str, Any]:
        """Perform generative fuzzing."""
        result = {
            "iterations_completed": 0,
            "crashes_found": [],
            "unique_crashes": [],
        }

        try:
            self.logger.info("Starting generation fuzzing")

            for iteration in range(self.config["max_iterations"]):
                # Generate input based on common patterns
                generated_data = self._generate_input()

                # Execute target
                execution_result = self._execute_target(target_command, generated_data, output_dir)

                if execution_result["crashed"]:
                    crash_info = self._process_crash(execution_result, generated_data, output_dir, iteration)
                    result["crashes_found"].append(crash_info)

                result["iterations_completed"] += 1
                self.stats["total_executions"] += 1

                # Progress logging
                if iteration % 1000 == 0:
                    self.logger.info(f"Generation fuzzing progress: {iteration}/{self.config['max_iterations']}")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Generation fuzzing error: {e}")

        return result

    def _grammar_based_fuzzing(self, target_command: str, seed_inputs: list[str], output_dir: str) -> dict[str, Any]:
        """Perform grammar-based fuzzing."""
        result = {
            "iterations_completed": 0,
            "crashes_found": [],
            "unique_crashes": [],
        }

        try:
            self.logger.info("Starting grammar-based fuzzing")

            # Detect file format from seeds
            file_format = self._detect_file_format(seed_inputs)

            for iteration in range(self.config["max_iterations"]):
                # Generate input using grammar
                if file_format in self.file_grammars:
                    grammar_func = self.file_grammars[file_format]
                    generated_data = grammar_func()
                else:
                    # Default to text generation
                    generated_data = self._generate_text_grammar()

                # Execute target
                execution_result = self._execute_target(target_command, generated_data, output_dir)

                if execution_result["crashed"]:
                    crash_info = self._process_crash(execution_result, generated_data, output_dir, iteration)
                    result["crashes_found"].append(crash_info)

                result["iterations_completed"] += 1
                self.stats["total_executions"] += 1

                # Progress logging
                if iteration % 1000 == 0:
                    self.logger.info(f"Grammar fuzzing progress: {iteration}/{self.config['max_iterations']}")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Grammar-based fuzzing error: {e}")

        return result

    def _coverage_guided_fuzzing(self, target_command: str, seed_inputs: list[str], output_dir: str) -> dict[str, Any]:
        """Perform coverage-guided fuzzing."""
        result = {
            "iterations_completed": 0,
            "crashes_found": [],
            "unique_crashes": [],
        }

        try:
            self.logger.info("Starting coverage-guided fuzzing")

            # Initialize with seeds
            queue = []
            for seed_file in seed_inputs:
                try:
                    with open(seed_file, "rb") as f:
                        data = f.read()
                        queue.append(data)
                except (OSError, ValueError, RuntimeError) as e:
                    self.logger.error("Error in fuzzing_engine: %s", e)
                    continue

            if not queue:
                queue = [b"AAAA"]

            for iteration in range(self.config["max_iterations"]):
                # Select input from queue
                if queue:
                    base_data = random.choice(queue)
                else:
                    base_data = b"AAAA"

                # Mutate
                mutated_data = self._apply_mutations(base_data)

                # Execute with coverage collection
                execution_result = self._execute_target_with_coverage(target_command, mutated_data, output_dir)

                if execution_result["crashed"]:
                    crash_info = self._process_crash(execution_result, mutated_data, output_dir, iteration)
                    result["crashes_found"].append(crash_info)

                # Check for new coverage
                if execution_result.get("new_coverage"):
                    queue.append(mutated_data)
                    self.stats["coverage_paths"] += 1

                result["iterations_completed"] += 1
                self.stats["total_executions"] += 1

                # Progress logging
                if iteration % 1000 == 0:
                    self.logger.info(f"Coverage fuzzing progress: {iteration}/{self.config['max_iterations']}")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Coverage-guided fuzzing error: {e}")

        return result

    def _hybrid_fuzzing(self, target_command: str, seed_inputs: list[str], output_dir: str) -> dict[str, Any]:
        """Perform hybrid fuzzing combining multiple strategies."""
        result = {
            "iterations_completed": 0,
            "crashes_found": [],
            "unique_crashes": [],
        }

        try:
            self.logger.info("Starting hybrid fuzzing")

            strategies = [
                ("mutation", 0.4),
                ("random", 0.2),
                ("generation", 0.2),
                ("grammar", 0.2),
            ]

            # Load seeds
            seed_data = []
            for seed_file in seed_inputs:
                try:
                    with open(seed_file, "rb") as f:
                        seed_data.append(f.read())
                except (OSError, ValueError, RuntimeError) as e:
                    logger.error("Error in fuzzing_engine: %s", e)
                    continue

            if not seed_data:
                seed_data = [b"AAAA"]

            for iteration in range(self.config["max_iterations"]):
                # Select strategy based on probabilities
                strategy = self._select_strategy(strategies)

                # Generate input based on strategy
                if strategy == "mutation":
                    base_data = random.choice(seed_data)
                    fuzz_data = self._apply_mutations(base_data)
                elif strategy == "random":
                    size = random.randint(1, min(1000, self.config["max_file_size"]))
                    fuzz_data = bytes([random.randint(0, 255) for _ in range(size)])
                elif strategy == "generation":
                    fuzz_data = self._generate_input()
                elif strategy == "grammar":
                    file_format = self._detect_file_format(seed_inputs)
                    if file_format in self.file_grammars:
                        fuzz_data = self.file_grammars[file_format]()
                    else:
                        fuzz_data = self._generate_text_grammar()
                else:
                    # Fallback to random data if strategy is unknown
                    fuzz_data = bytes([random.randint(0, 255) for _ in range(100)])

                # Execute target
                execution_result = self._execute_target(target_command, fuzz_data, output_dir)

                if execution_result["crashed"]:
                    crash_info = self._process_crash(execution_result, fuzz_data, output_dir, iteration)
                    result["crashes_found"].append(crash_info)

                # Add interesting inputs to seeds
                if self._is_interesting_input(execution_result):
                    seed_data.append(fuzz_data)
                    if len(seed_data) > 1000:  # Limit seed pool size
                        seed_data = seed_data[-1000:]

                result["iterations_completed"] += 1
                self.stats["total_executions"] += 1

                # Progress logging
                if iteration % 1000 == 0:
                    self.logger.info(f"Hybrid fuzzing progress: {iteration}/{self.config['max_iterations']}")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Hybrid fuzzing error: {e}")

        return result

    def _execute_target(self, target_command: str, input_data: bytes, output_dir: str) -> dict[str, Any]:
        """Execute target with input data."""
        result = {
            "crashed": False,
            "hanged": False,
            "exit_code": 0,
            "stdout": "",
            "stderr": "",
            "execution_time": 0.0,
            "signal": None,
        }

        try:
            # Create temporary input file
            with tempfile.NamedTemporaryFile(mode="wb", delete=False, dir=output_dir) as f:
                f.write(input_data)
                input_file = f.name

            # Replace placeholder in command
            cmd = target_command.replace("@@@", input_file)

            start_time = time.time()

            try:
                # Execute with timeout
                process = subprocess.Popen(
                    cmd.split(),
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                )

                try:
                    stdout, stderr = process.communicate(timeout=self.config["timeout"])
                    result["exit_code"] = process.returncode
                    result["stdout"] = stdout
                    result["stderr"] = stderr

                except subprocess.TimeoutExpired as e:
                    logger.error("Subprocess timeout in fuzzing_engine: %s", e)
                    process.kill()
                    result["hanged"] = True

            except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
                logger.error("Error in fuzzing_engine: %s", e)
                result["crashed"] = True
                result["stderr"] = str(e)

            result["execution_time"] = time.time() - start_time

            # Check for crash indicators
            if result["exit_code"] < 0:
                result["crashed"] = True
                result["signal"] = -result["exit_code"]

            # Cleanup
            try:
                os.unlink(input_file)
            except (OSError, ValueError, RuntimeError) as e:
                logger.error("Error in fuzzing_engine: %s", e)

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Target execution error: {e}")
            result["crashed"] = True

        return result

    def _execute_target_with_coverage(self, target_command: str, input_data: bytes, output_dir: str) -> dict[str, Any]:
        """Execute target with real coverage collection using gcov or AFL++."""
        result = self._execute_target(target_command, input_data, output_dir)

        # Real coverage collection implementation
        coverage_dir = os.path.join(output_dir, "coverage")
        os.makedirs(coverage_dir, exist_ok=True)

        # Check if target is instrumented with AFL++
        if "afl-" in target_command or os.environ.get("AFL_USE_ASAN"):
            # Use AFL++ coverage information
            result["new_coverage"] = self._check_afl_coverage(target_command, coverage_dir)
        else:
            # Try gcov for coverage
            result["new_coverage"] = self._check_gcov_coverage(target_command, coverage_dir)

        # If no coverage tools available, check for new execution paths
        if result["new_coverage"] is None:
            result["new_coverage"] = self._check_execution_paths(result, coverage_dir)

        return result

    def _check_afl_coverage(self, target_command: str, coverage_dir: str) -> bool:
        """Check AFL++ coverage information."""
        try:
            # Check AFL coverage map
            afl_out_dir = os.environ.get("AFL_OUT_DIR", "/tmp/afl_out")
            coverage_map = os.path.join(afl_out_dir, "fuzz_bitmap")

            if os.path.exists(coverage_map):
                # Read current coverage bitmap
                with open(coverage_map, "rb") as f:
                    current_coverage = f.read()

                # Compare with previous coverage
                prev_coverage_file = os.path.join(coverage_dir, "prev_coverage.bin")
                new_coverage = False

                if os.path.exists(prev_coverage_file):
                    with open(prev_coverage_file, "rb") as f:
                        prev_coverage = f.read()

                    # Check if any new bits are set
                    for i in range(min(len(current_coverage), len(prev_coverage))):
                        if current_coverage[i] != prev_coverage[i]:
                            new_coverage = True
                            break
                else:
                    new_coverage = True

                # Save current coverage for next comparison
                with open(prev_coverage_file, "wb") as f:
                    f.write(current_coverage)

                return new_coverage

        except Exception as e:
            self.logger.debug(f"AFL coverage check failed: {e}")

        return False

    def _check_gcov_coverage(self, target_command: str, coverage_dir: str) -> bool:
        """Check gcov coverage information."""
        try:
            # Run gcov to generate coverage data
            target_binary = target_command.split()[0]
            gcov_cmd = ["gcov", "-b", "-c", target_binary]

            result = subprocess.run(
                gcov_cmd,
                check=False, cwd=coverage_dir,
                capture_output=True,
                text=True,
                timeout=10,
            )

            if result.returncode == 0:
                # Parse gcov output for new lines/branches
                output_lines = result.stdout.split("\n")
                for line in output_lines:
                    if "Lines executed:" in line or "Branches executed:" in line:
                        # Extract percentage
                        percent_match = re.search(r"(\d+\.\d+)%", line)
                        if percent_match:
                            coverage_percent = float(percent_match.group(1))

                            # Check if coverage increased
                            prev_coverage_file = os.path.join(coverage_dir, "prev_coverage.txt")

                            if os.path.exists(prev_coverage_file):
                                with open(prev_coverage_file) as f:
                                    prev_coverage = float(f.read().strip())

                                if coverage_percent > prev_coverage:
                                    with open(prev_coverage_file, "w") as f:
                                        f.write(str(coverage_percent))
                                    return True
                            else:
                                with open(prev_coverage_file, "w") as f:
                                    f.write(str(coverage_percent))
                                return True

        except Exception as e:
            self.logger.debug(f"gcov coverage check failed: {e}")

        return False

    def _check_execution_paths(self, exec_result: dict[str, Any], coverage_dir: str) -> bool:
        """Check for new execution paths based on output patterns."""
        try:
            # Create hash of execution characteristics
            import hashlib

            exec_signature = hashlib.sha256()
            exec_signature.update(str(exec_result["exit_code"]).encode())
            exec_signature.update(exec_result["stdout"].encode())
            exec_signature.update(exec_result["stderr"].encode())

            # Add crash/hang status
            if exec_result["crashed"]:
                exec_signature.update(b"CRASHED")
            if exec_result["hanged"]:
                exec_signature.update(b"HANGED")
            if exec_result.get("signal"):
                exec_signature.update(str(exec_result["signal"]).encode())

            signature_hex = exec_signature.hexdigest()

            # Check if this execution path is new
            paths_file = os.path.join(coverage_dir, "execution_paths.txt")

            if os.path.exists(paths_file):
                with open(paths_file) as f:
                    known_paths = set(line.strip() for line in f)

                if signature_hex not in known_paths:
                    # New execution path found
                    with open(paths_file, "a") as f:
                        f.write(signature_hex + "\n")
                    return True
            else:
                # First execution
                with open(paths_file, "w") as f:
                    f.write(signature_hex + "\n")
                return True

        except Exception as e:
            self.logger.debug(f"Execution path check failed: {e}")

        return False

    def _apply_mutations(self, data: bytes) -> bytes:
        """Apply random mutations to data."""
        if not data:
            return data

        mutated = bytearray(data)

        # Apply random number of mutations
        num_mutations = random.randint(1, max(1, int(len(data) * self.config["mutation_rate"])))

        for _ in range(num_mutations):
            # Select random mutation strategy
            strategy = random.choice(list(self.mutation_strategies.keys()))
            mutation_func = self.mutation_strategies[strategy]

            try:
                mutated = mutation_func(mutated)
            except (OSError, ValueError, RuntimeError) as e:
                self.logger.error("Error in fuzzing_engine: %s", e)
                continue

        return bytes(mutated)

    # Mutation strategy implementations

    def _mutate_bit_flip(self, data: bytearray) -> bytearray:
        """Flip random bits."""
        if not data:
            return data

        pos = random.randint(0, len(data) - 1)
        bit_pos = random.randint(0, 7)
        data[pos] ^= (1 << bit_pos)
        return data

    def _mutate_byte_flip(self, data: bytearray) -> bytearray:
        """Flip random bytes."""
        if not data:
            return data

        pos = random.randint(0, len(data) - 1)
        data[pos] = random.randint(0, 255)
        return data

    def _mutate_arithmetic(self, data: bytearray) -> bytearray:
        """Apply arithmetic operations."""
        if len(data) < 2:
            return data

        pos = random.randint(0, len(data) - 2)

        # Interpret as various integer types
        if random.choice([True, False]):
            # 16-bit integer
            if pos <= len(data) - 2:
                val = struct.unpack("<H", data[pos:pos+2])[0]
                val = (val + random.randint(-100, 100)) % 65536
                struct.pack_into("<H", data, pos, val)
        else:
            # 8-bit integer
            data[pos] = (data[pos] + random.randint(-50, 50)) % 256

        return data

    def _mutate_insert(self, data: bytearray) -> bytearray:
        """Insert random bytes."""
        pos = random.randint(0, len(data))
        insert_data = bytes([random.randint(0, 255) for _ in range(random.randint(1, 10))])
        return bytearray(data[:pos] + insert_data + data[pos:])

    def _mutate_delete(self, data: bytearray) -> bytearray:
        """Delete random bytes."""
        if len(data) <= 1:
            return data

        start = random.randint(0, len(data) - 1)
        end = random.randint(start, min(start + 10, len(data)))
        return bytearray(data[:start] + data[end:])

    def _mutate_duplicate(self, data: bytearray) -> bytearray:
        """Duplicate random chunks."""
        if not data:
            return data

        start = random.randint(0, len(data) - 1)
        end = random.randint(start, min(start + 10, len(data)))
        chunk = data[start:end]

        pos = random.randint(0, len(data))
        return bytearray(data[:pos] + chunk + data[pos:])

    def _mutate_splice(self, data: bytearray) -> bytearray:
        """Splice with magic values."""
        if not data:
            return data

        pos = random.randint(0, len(data))
        magic_value = random.choice(self.magic_values["special_chars"])
        return bytearray(data[:pos] + magic_value + data[pos:])

    def _mutate_magic_values(self, data: bytearray) -> bytearray:
        """Replace with magic values."""
        if len(data) < 4:
            return data

        pos = random.randint(0, len(data) - 4)
        magic_int = random.choice(self.magic_values["integers"])
        struct.pack_into("<I", data, pos, magic_int % (2**32))
        return data

    def _mutate_string_replace(self, data: bytearray) -> bytearray:
        """Replace strings with format string payloads."""
        magic_string = random.choice(self.magic_values["format_strings"])

        if len(data) >= len(magic_string):
            pos = random.randint(0, len(data) - len(magic_string))
            data[pos:pos+len(magic_string)] = magic_string

        return data

    def _mutate_format_aware(self, data: bytearray) -> bytearray:
        """Format-aware mutations."""
        # This would implement format-specific mutations
        # For now, just apply a random mutation
        return self._mutate_byte_flip(data)

    def _generate_input(self) -> bytes:
        """Generate input using common patterns."""
        patterns = [
            self._generate_overflow_pattern,
            self._generate_format_string_pattern,
            self._generate_unicode_pattern,
            self._generate_binary_pattern,
        ]

        generator = random.choice(patterns)
        return generator()

    def _generate_overflow_pattern(self) -> bytes:
        """Generate buffer overflow pattern."""
        size = random.randint(100, 2000)
        char = random.choice([b"A", b"B", b"C", b"\x41"])
        return char[0:1] * size

    def _generate_format_string_pattern(self) -> bytes:
        """Generate format string pattern."""
        formats = [b"%s", b"%x", b"%n", b"%p", b"%d"]
        num_formats = random.randint(1, 20)
        return b"".join(random.choice(formats) for _ in range(num_formats))

    def _generate_unicode_pattern(self) -> bytes:
        """Generate Unicode pattern."""
        unicode_chars = [b"\xff\xfe", b"\xfe\xff", b"\xef\xbb\xbf"]
        base = random.choice(unicode_chars)
        return base + b"A" * random.randint(10, 100)

    def _generate_binary_pattern(self) -> bytes:
        """Generate binary pattern."""
        size = random.randint(10, 1000)
        return bytes([random.randint(0, 255) for _ in range(size)])

    # File format grammar generators

    def _generate_text_grammar(self) -> bytes:
        """Generate text input."""
        words = [b"hello", b"world", b"test", b"data", b"input"]
        num_words = random.randint(1, 20)
        return b" ".join(random.choice(words) for _ in range(num_words))

    def _generate_xml_grammar(self) -> bytes:
        """Generate XML input."""
        tag = random.choice([b"root", b"data", b"element"])
        content = b"A" * random.randint(1, 100)
        return b'<?xml version="1.0"?><' + tag + b">" + content + b"</" + tag + b">"

    def _generate_json_grammar(self) -> bytes:
        """Generate JSON input."""
        key = b"key" + str(random.randint(1, 100)).encode()
        value = b"A" * random.randint(1, 100)
        return b'{"' + key + b'": "' + value + b'"}'

    def _generate_http_grammar(self) -> bytes:
        """Generate HTTP input."""
        method = random.choice([b"GET", b"POST", b"PUT"])
        path = b"/" + b"A" * random.randint(1, 100)
        return method + b" " + path + b" HTTP/1.1\r\nHost: test\r\n\r\n"

    def _generate_binary_grammar(self) -> bytes:
        """Generate binary input."""
        header = b"BIN\x00"
        size = random.randint(10, 1000)
        data = bytes([random.randint(0, 255) for _ in range(size)])
        return header + data

    # Helper methods

    def _generate_default_seeds(self) -> list[str]:
        """Generate default seed inputs."""
        seeds = []

        try:
            # Create temporary seed files
            seed_data = [
                b"AAAA",
                b"test input",
                b"\x00\x01\x02\x03",
                b"%s%s%s%s",
                b"A" * 100,
            ]

            temp_dir = tempfile.mkdtemp()

            for i, data in enumerate(seed_data):
                seed_file = os.path.join(temp_dir, f"seed_{i}.dat")
                with open(seed_file, "wb") as f:
                    f.write(data)
                seeds.append(seed_file)

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Default seed generation error: {e}")

        return seeds

    def _detect_file_format(self, seed_inputs: list[str]) -> str:
        """Detect file format from seed inputs."""
        if not seed_inputs:
            return "binary"

        try:
            # Read first seed
            with open(seed_inputs[0], "rb") as f:
                data = f.read(100)  # Read first 100 bytes

            # Simple format detection
            if data.startswith(b"<?xml"):
                return "xml"
            if data.startswith(b"{") or data.startswith(b"["):
                return "json"
            if b"HTTP" in data:
                return "http"
            if all(32 <= b <= 126 for b in data if b != 0):
                return "text"
            return "binary"

        except (OSError, ValueError, RuntimeError) as e:
            logger.error("Error in fuzzing_engine: %s", e)
            return "binary"

    def _randomize_data(self, data: bytes) -> bytes:
        """Randomize data while maintaining some structure."""
        if not data:
            return data

        mutated = bytearray(data)

        # Randomize some percentage of bytes
        num_changes = int(len(data) * random.uniform(0.01, 0.1))

        for _ in range(num_changes):
            pos = random.randint(0, len(mutated) - 1)
            mutated[pos] = random.randint(0, 255)

        return bytes(mutated)

    def _is_interesting_input(self, execution_result: dict[str, Any]) -> bool:
        """Check if input is interesting for fuzzing."""
        # Consider input interesting if it has new behavior
        return (
            execution_result.get("new_coverage", False) or
            execution_result.get("hanged", False) or
            execution_result["execution_time"] > self.config["timeout"] * 0.8
        )

    def _select_strategy(self, strategies: list[tuple[str, float]]) -> str:
        """Select fuzzing strategy based on probabilities."""
        rand_val = random.random()
        cumulative = 0.0

        for strategy, probability in strategies:
            cumulative += probability
            if rand_val <= cumulative:
                return strategy

        return strategies[0][0]  # Fallback to first strategy

    def _process_crash(self, execution_result: dict[str, Any], input_data: bytes,
                      output_dir: str, iteration: int) -> dict[str, Any]:
        """Process and analyze crash."""
        crash_info = {
            "iteration": iteration,
            "crash_id": self._generate_crash_id(),
            "input_size": len(input_data),
            "exit_code": execution_result["exit_code"],
            "signal": execution_result.get("signal"),
            "stderr": execution_result["stderr"],
            "execution_time": execution_result["execution_time"],
            "crash_file": None,
            "unique": False,
        }

        try:
            # Save crash input
            crash_file = os.path.join(output_dir, "crashes", f"crash_{crash_info['crash_id']}.dat")
            os.makedirs(os.path.dirname(crash_file), exist_ok=True)

            with open(crash_file, "wb") as f:
                f.write(input_data)

            crash_info["crash_file"] = crash_file

            # Check if crash is unique
            crash_hash = self._calculate_crash_hash(execution_result)

            if crash_hash not in self.unique_crashes:
                self.unique_crashes[crash_hash] = crash_info
                crash_info["unique"] = True
                self.stats["unique_crashes"] += 1

            self.stats["crashes_found"] += 1

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Crash processing error: {e}")

        return crash_info

    def _calculate_crash_hash(self, execution_result: dict[str, Any]) -> str:
        """Calculate hash to identify unique crashes."""
        import hashlib

        # Use exit code, signal, and stderr for uniqueness
        crash_data = f"{execution_result['exit_code']}:{execution_result.get('signal', '')}:{execution_result['stderr'][:100]}"
        return hashlib.sha256(crash_data.encode()).hexdigest()[:16]

    def _create_output_directories(self, campaign_id: str) -> str:
        """Create output directories for fuzzing campaign."""
        output_dir = f"/tmp/fuzzing_{campaign_id}"

        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, "crashes"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "hangs"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "queue"), exist_ok=True)

        return output_dir

    def _generate_campaign_id(self) -> str:
        """Generate unique campaign identifier."""
        import hashlib
        import time

        data = f"{time.time()}-{random.randint(1000, 9999)}"
        return hashlib.sha256(data.encode()).hexdigest()[:12]

    def _generate_crash_id(self) -> str:
        """Generate unique crash identifier."""
        import hashlib

        data = f"{time.time()}-{random.randint(1000, 9999)}"
        return hashlib.sha256(data.encode()).hexdigest()[:8]

    # Crash analysis methods (stubs for now)

    def _debug_crash(self, target_command: str, crash_file: str) -> dict[str, Any] | None:
        """Debug crash under debugger."""
        debug_info = {
            "registers": {},
            "stack_trace": [],
            "memory_maps": [],
            "crash_address": None,
            "crash_instruction": None,
            "fault_type": None,
            "signal_info": {},
            "thread_info": {},
            "loaded_modules": [],
            "heap_info": {},
            "stack_info": {},
        }

        try:
            # Try GDB-based debugging (Linux)
            if os.name == "posix":
                debug_info.update(self._debug_with_gdb(target_command, crash_file))

            # Try WinDbg-based debugging (Windows)
            elif os.name == "nt":
                debug_info.update(self._debug_with_windbg(target_command, crash_file))

            # Fallback to crash pattern analysis
            else:
                debug_info.update(self._analyze_crash_pattern(crash_file))

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.warning(f"Debugging failed: {e}")
            debug_info.update(self._analyze_crash_pattern(crash_file))

        return debug_info

    def _debug_with_gdb(self, target_command: str, crash_file: str) -> dict[str, Any]:
        """Debug crash using GDB."""
        gdb_info = {}

        try:
            # Create GDB script
            gdb_script = f"""
set confirm off
set pagination off
set logging file /tmp/gdb_output.txt
set logging on
file {target_command.split()[0]}
run {crash_file}
info registers
bt
info proc mappings
x/32i $pc-16
x/64wx $sp-32
disas $pc-32,$pc+32
info threads
info sharedlibrary
quit
"""

            gdb_script_file = "/tmp/gdb_debug.script"
            with open(gdb_script_file, "w") as f:
                f.write(gdb_script)

            # Run GDB
            gdb_cmd = f"gdb -batch -x {gdb_script_file} 2>/dev/null"
            result = subprocess.run(gdb_cmd, check=False, shell=True, capture_output=True, text=True, timeout=30)

            # Log subprocess result for debugging
            logger.debug(f"GDB command exit code: {result.returncode}")

            # Parse GDB output
            if os.path.exists("/tmp/gdb_output.txt"):
                with open("/tmp/gdb_output.txt") as f:
                    gdb_output = f.read()

                gdb_info = self._parse_gdb_output(gdb_output)

                # Cleanup
                os.remove("/tmp/gdb_output.txt")
                os.remove(gdb_script_file)

        except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
            self.logger.debug(f"GDB debugging failed: {e}")

        return gdb_info

    def _debug_with_windbg(self, target_command: str, crash_file: str) -> dict[str, Any]:
        """Debug crash using WinDbg (Windows)."""
        windbg_info = {}

        try:
            # WinDbg commands for comprehensive crash analysis
            windbg_script = f"""
.logopen c:\\temp\\windbg_output.txt
.load wow64exts
.load ext\\*
{target_command} {crash_file}
g
r
k
lm
!analyze -v
!heap -p -a @$exr
!address @$exr
u @$ip-20 @$ip+20
dd @$esp-40 @$esp+40
q
"""

            # Log the script for debugging purposes
            self.logger.debug(f"Generated WinDbg script with {len(windbg_script)} characters for crash analysis")

            # Check if WinDbg is available on the system
            import shutil
            windbg_path = shutil.which("windbg.exe")
            cdb_path = shutil.which("cdb.exe")

            if windbg_path or cdb_path:
                # WinDbg is available - could implement actual debugging
                debugger_path = windbg_path if windbg_path else cdb_path
                windbg_info = {
                    "platform": "windows",
                    "debugger": "windbg",
                    "debugger_path": debugger_path,
                    "script_length": len(windbg_script),
                    "status": "available_but_not_implemented",
                    "crash_file_analyzed": crash_file,
                    "target_command": target_command.split()[0] if target_command else "unknown",
                }
                self.logger.info(f"WinDbg found at {debugger_path} but full implementation pending")
            else:
                # WinDbg not available
                windbg_info = {
                    "platform": "windows",
                    "debugger": "windbg",
                    "script_length": len(windbg_script),
                    "status": "not_available",
                    "reason": "WinDbg/CDB not found in PATH",
                    "crash_file_analyzed": crash_file,
                    "target_command": target_command.split()[0] if target_command else "unknown",
                }
                self.logger.warning("WinDbg/CDB not available for crash analysis")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"WinDbg debugging failed: {e}")
            windbg_info = {
                "platform": "windows",
                "debugger": "windbg",
                "status": "error",
                "error": str(e),
                "crash_file_analyzed": crash_file,
            }

        return windbg_info

    def _parse_gdb_output(self, gdb_output: str) -> dict[str, Any]:
        """Parse GDB debugging output."""
        parsed_info = {
            "registers": {},
            "stack_trace": [],
            "memory_maps": [],
            "crash_address": None,
            "crash_instruction": None,
            "loaded_modules": [],
            "signal_info": {},
            "parsing_stats": {
                "total_lines": 0,
                "parsed_registers": 0,
                "parsed_frames": 0,
                "parsed_maps": 0,
            },
        }

        lines = gdb_output.split("\n")
        current_section = None
        parsed_info["parsing_stats"]["total_lines"] = len(lines)

        for line in lines:
            line = line.strip()

            # Track current section for context-aware parsing
            if line.startswith("(gdb)") or line.startswith("Breakpoint"):
                current_section = "gdb_prompt"
            elif "registers" in line.lower():
                current_section = "registers"
            elif line.startswith("#") and current_section != "disassembly":
                current_section = "stack_trace"
            elif "mapped" in line or "Start Addr" in line:
                current_section = "memory_maps"

            # Parse registers
            if line.startswith("rax") or line.startswith("eax") or (line.startswith("r") and "x" in line[:5]):
                reg_match = re.search(r"(\w+)\s+0x([0-9a-fA-F]+)", line)
                if reg_match:
                    reg_name, reg_value = reg_match.groups()
                    try:
                        parsed_info["registers"][reg_name] = int(reg_value, 16)
                        parsed_info["parsing_stats"]["parsed_registers"] += 1

                        # Check for interesting register values
                        reg_val = int(reg_value, 16)
                        if reg_val == 0x41414141:  # 'AAAA' - controlled value
                            parsed_info["crash_indicators"] = parsed_info.get("crash_indicators", [])
                            parsed_info["crash_indicators"].append(f"controlled_{reg_name}")

                    except ValueError as e:
                        logger.error("Value error in fuzzing_engine: %s", e)
                        continue

            # Parse stack trace
            elif line.startswith("#"):
                frame_match = re.search(r"#(\d+)\s+0x([0-9a-fA-F]+)\s+in\s+(.+)", line)
                if frame_match:
                    frame_num, addr, func = frame_match.groups()
                    parsed_info["stack_trace"].append({
                        "frame": int(frame_num),
                        "address": int(addr, 16),
                        "function": func,
                        "section": current_section,
                    })
                    parsed_info["parsing_stats"]["parsed_frames"] += 1

            # Parse memory mappings
            elif "mapped" in line and "0x" in line:
                map_match = re.search(r"0x([0-9a-fA-F]+)\s+0x([0-9a-fA-F]+)\s+0x([0-9a-fA-F]+)\s+0x([0-9a-fA-F]+)\s+(.+)", line)
                if map_match:
                    start, end, size, offset, objfile = map_match.groups()
                    memory_map = {
                        "start": int(start, 16),
                        "end": int(end, 16),
                        "size": int(size, 16),
                        "offset": int(offset, 16),
                        "objfile": objfile,
                        "section": current_section,
                    }
                    parsed_info["memory_maps"].append(memory_map)
                    parsed_info["parsing_stats"]["parsed_maps"] += 1

                    # Check for loaded modules
                    if objfile and objfile not in ["/dev/zero", "[stack]", "[heap]"]:
                        if objfile not in parsed_info["loaded_modules"]:
                            parsed_info["loaded_modules"].append(objfile)

            # Parse crash address and signal information
            elif "Program received signal" in line:
                signal_match = re.search(r"signal\s+(\w+),\s+(.+)", line)
                if signal_match:
                    signal_name, signal_desc = signal_match.groups()
                    parsed_info["signal_info"] = {
                        "signal": signal_name,
                        "description": signal_desc,
                        "line_context": current_section,
                    }

            # Parse crash instruction
            elif "=>" in line and "0x" in line:
                # This indicates the current instruction pointer
                instr_match = re.search(r"=>\s*0x([0-9a-fA-F]+)[:\s]+(.+)", line)
                if instr_match:
                    crash_addr, instruction = instr_match.groups()
                    parsed_info["crash_address"] = int(crash_addr, 16)
                    parsed_info["crash_instruction"] = instruction.strip()

        # Log parsing results
        stats = parsed_info["parsing_stats"]
        self.logger.debug(f"GDB output parsed: {stats['parsed_registers']} registers, "
                         f"{stats['parsed_frames']} stack frames, {stats['parsed_maps']} memory maps "
                         f"from {stats['total_lines']} lines")

        return parsed_info

    def _analyze_crash_pattern(self, crash_file: str) -> dict[str, Any]:
        """Analyze crash based on input patterns."""
        pattern_info = {
            "input_analysis": {},
            "crash_indicators": [],
            "pattern_type": "unknown",
        }

        try:
            with open(crash_file, "rb") as f:
                crash_data = f.read()

            # Analyze input patterns
            pattern_info["input_analysis"] = {
                "size": len(crash_data),
                "has_nulls": b"\x00" in crash_data,
                "has_format_strings": any(fs in crash_data for fs in [b"%s", b"%n", b"%x"]),
                "has_long_strings": any(len(chunk) > 100 for chunk in crash_data.split(b"\x00")),
                "entropy": self._calculate_data_entropy(crash_data),
                "repeated_patterns": self._find_repeated_patterns(crash_data),
            }

            # Determine likely crash type
            if b"A" * 50 in crash_data or b"B" * 50 in crash_data:
                pattern_info["pattern_type"] = "buffer_overflow"
                pattern_info["crash_indicators"].append("long_repeated_chars")

            if any(fs in crash_data for fs in [b"%s", b"%n", b"%x"]):
                pattern_info["pattern_type"] = "format_string"
                pattern_info["crash_indicators"].append("format_specifiers")

            if len(crash_data) > 10000:
                pattern_info["crash_indicators"].append("large_input")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Crash pattern analysis failed: {e}")

        return pattern_info

    def _calculate_data_entropy(self, data: bytes) -> float:
        """Calculate entropy of crash data."""
        from ...utils.analysis.entropy_utils import safe_entropy_calculation

        return safe_entropy_calculation(data, max_entropy=8.0)  # Cap at 8 bits for fuzzing analysis

    def _find_repeated_patterns(self, data: bytes) -> list[dict[str, Any]]:
        """Find repeated patterns in crash data."""
        patterns = []

        # Look for repeated byte sequences
        for pattern_len in [1, 2, 4, 8]:
            if len(data) < pattern_len * 10:
                continue

            for i in range(len(data) - pattern_len * 5):
                pattern = data[i:i+pattern_len]
                count = 0
                pos = i

                while pos < len(data) - pattern_len:
                    if data[pos:pos+pattern_len] == pattern:
                        count += 1
                        pos += pattern_len
                    else:
                        pos += 1

                if count >= 10:  # Found significant repetition
                    patterns.append({
                        "pattern": pattern.hex(),
                        "length": pattern_len,
                        "count": count,
                        "start_offset": i,
                    })
                    break

        return patterns[:5]  # Return top 5 patterns

    def _analyze_crash_info(self, debug_info: dict[str, Any]) -> dict[str, Any]:
        """Analyze crash debug information."""
        analysis = {
            "crash_type": "unknown",
            "severity": CrashSeverity.MEDIUM.value,
            "root_cause": "unknown",
            "crash_details": {},
            "vulnerability_indicators": [],
            "exploitation_complexity": "unknown",
            "affected_components": [],
            "security_impact": {},
        }

        try:
            # Analyze signal information
            signal_info = debug_info.get("signal_info", {})
            if signal_info:
                analysis.update(self._analyze_signal_crash(signal_info))

            # Analyze register state
            registers = debug_info.get("registers", {})
            if registers:
                analysis.update(self._analyze_register_state(registers))

            # Analyze stack trace
            stack_trace = debug_info.get("stack_trace", [])
            if stack_trace:
                analysis.update(self._analyze_stack_trace(stack_trace))

            # Analyze memory mappings
            memory_maps = debug_info.get("memory_maps", [])
            if memory_maps:
                analysis.update(self._analyze_memory_layout(memory_maps))

            # Analyze input patterns
            input_analysis = debug_info.get("input_analysis", {})
            if input_analysis:
                analysis.update(self._analyze_input_patterns(input_analysis))

            # Determine overall severity
            analysis["severity"] = self._calculate_crash_severity(analysis)

            # Generate security impact assessment
            analysis["security_impact"] = self._assess_security_impact(analysis)

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.warning(f"Crash analysis failed: {e}")

        return analysis

    def _analyze_signal_crash(self, signal_info: dict[str, Any]) -> dict[str, Any]:
        """Analyze crash based on signal information."""
        signal_analysis = {
            "crash_type": "unknown",
            "vulnerability_indicators": [],
            "crash_details": {},
        }

        signal_name = signal_info.get("signal", "").upper()

        if signal_name == "SIGSEGV":
            signal_analysis.update({
                "crash_type": "segmentation_fault",
                "root_cause": "memory_corruption",
                "vulnerability_indicators": ["memory_access_violation", "potential_buffer_overflow"],
                "crash_details": {
                    "fault_type": "memory_access_violation",
                    "description": "Invalid memory access detected",
                },
            })

        elif signal_name == "SIGABRT":
            signal_analysis.update({
                "crash_type": "abort_signal",
                "root_cause": "assertion_failure",
                "vulnerability_indicators": ["heap_corruption", "double_free"],
                "crash_details": {
                    "fault_type": "program_abort",
                    "description": "Program aborted, possibly due to heap corruption",
                },
            })

        elif signal_name == "SIGILL":
            signal_analysis.update({
                "crash_type": "illegal_instruction",
                "root_cause": "code_corruption",
                "vulnerability_indicators": ["code_injection", "rop_chain"],
                "crash_details": {
                    "fault_type": "illegal_instruction",
                    "description": "Illegal instruction executed",
                },
            })

        elif signal_name == "SIGFPE":
            signal_analysis.update({
                "crash_type": "floating_point_exception",
                "root_cause": "arithmetic_error",
                "vulnerability_indicators": ["division_by_zero", "integer_overflow"],
                "crash_details": {
                    "fault_type": "arithmetic_exception",
                    "description": "Floating point arithmetic error",
                },
            })

        return signal_analysis

    def _analyze_register_state(self, registers: dict[str, Any]) -> dict[str, Any]:
        """Analyze crash based on register state."""
        register_analysis = {
            "vulnerability_indicators": [],
            "crash_details": {},
            "exploitation_complexity": "medium",
        }

        # Check for common exploitation patterns
        for reg_name, reg_value in registers.items():
            if isinstance(reg_value, int):
                # Check for controlled register values (common in exploits)
                if reg_value == 0x41414141:  # 'AAAA'
                    register_analysis["vulnerability_indicators"].append("controlled_eip")
                    register_analysis["exploitation_complexity"] = "low"
                    register_analysis["crash_details"]["controlled_registers"] = [reg_name]

                elif reg_value == 0x42424242:  # 'BBBB'
                    register_analysis["vulnerability_indicators"].append("controlled_register")
                    register_analysis["exploitation_complexity"] = "low"

                elif reg_value & 0xFFFF0000 == 0x41410000:  # Partial control
                    register_analysis["vulnerability_indicators"].append("partial_register_control")
                    register_analysis["exploitation_complexity"] = "medium"

                # Check for NULL dereference
                elif reg_value == 0x0:
                    register_analysis["vulnerability_indicators"].append("null_pointer_dereference")

                # Check for stack addresses (potential stack overflow)
                elif 0x7fff00000000 <= reg_value <= 0x7fffffffffff:  # x64 stack range
                    register_analysis["vulnerability_indicators"].append("stack_address_in_register")

        return register_analysis

    def _analyze_stack_trace(self, stack_trace: list[dict[str, Any]]) -> dict[str, Any]:
        """Analyze crash based on stack trace."""
        stack_analysis = {
            "affected_components": [],
            "vulnerability_indicators": [],
            "crash_details": {},
        }

        if not stack_trace:
            return stack_analysis

        # Analyze function names in stack trace
        functions = [frame.get("function", "") for frame in stack_trace]

        # Look for vulnerable function patterns
        vulnerable_functions = [
            "strcpy", "strcat", "sprintf", "gets", "scanf",
            "memcpy", "memmove", "strncpy", "strncat",
        ]

        for func in functions:
            for vuln_func in vulnerable_functions:
                if vuln_func in func:
                    stack_analysis["vulnerability_indicators"].append(f"vulnerable_function_{vuln_func}")
                    stack_analysis["affected_components"].append(func)

        # Check for recursive calls (potential stack overflow)
        func_counts = {}
        for func in functions:
            func_counts[func] = func_counts.get(func, 0) + 1

        for func, count in func_counts.items():
            if count > 10:  # Likely recursion
                stack_analysis["vulnerability_indicators"].append("stack_overflow_recursion")
                stack_analysis["crash_details"]["recursive_function"] = func

        # Analyze stack depth
        if len(stack_trace) > 50:
            stack_analysis["vulnerability_indicators"].append("deep_stack_trace")

        return stack_analysis

    def _analyze_memory_layout(self, memory_maps: list[dict[str, Any]]) -> dict[str, Any]:
        """Analyze crash based on memory layout."""
        memory_analysis = {
            "vulnerability_indicators": [],
            "crash_details": {},
            "security_mitigations": [],
        }

        for mem_map in memory_maps:
            objfile = mem_map.get("objfile", "")
            start = mem_map.get("start", 0)

            # Check for ASLR
            if "libc" in objfile and start < 0x7f0000000000:
                memory_analysis["security_mitigations"].append("aslr_disabled")
            elif "libc" in objfile:
                memory_analysis["security_mitigations"].append("aslr_enabled")

            # Check for executable stack
            if "[stack]" in objfile and "x" in str(mem_map):
                memory_analysis["vulnerability_indicators"].append("executable_stack")

            # Check for RWX mappings
            if all(perm in str(mem_map) for perm in ["r", "w", "x"]):
                memory_analysis["vulnerability_indicators"].append("rwx_mapping")

        return memory_analysis

    def _analyze_input_patterns(self, input_analysis: dict[str, Any]) -> dict[str, Any]:
        """Analyze crash based on input patterns."""
        pattern_analysis = {
            "root_cause": "unknown",
            "vulnerability_indicators": [],
            "exploitation_complexity": "medium",
        }

        pattern_type = input_analysis.get("pattern_type", "unknown")

        if pattern_type == "buffer_overflow":
            pattern_analysis.update({
                "root_cause": "buffer_overflow",
                "vulnerability_indicators": ["buffer_boundary_violation"],
                "exploitation_complexity": "low",
            })

        elif pattern_type == "format_string":
            pattern_analysis.update({
                "root_cause": "format_string_vulnerability",
                "vulnerability_indicators": ["format_string_injection"],
                "exploitation_complexity": "medium",
            })

        # Analyze entropy
        entropy = input_analysis.get("entropy", 0)
        if entropy > 7.0:
            pattern_analysis["vulnerability_indicators"].append("high_entropy_input")
        elif entropy < 1.0:
            pattern_analysis["vulnerability_indicators"].append("low_entropy_pattern")

        # Analyze size
        size = input_analysis.get("size", 0)
        if size > 100000:
            pattern_analysis["vulnerability_indicators"].append("extremely_large_input")
        elif size > 10000:
            pattern_analysis["vulnerability_indicators"].append("large_input")

        return pattern_analysis

    def _calculate_crash_severity(self, analysis: dict[str, Any]) -> str:
        """Calculate overall crash severity."""
        severity_score = 0

        # Base score from crash type
        crash_type = analysis.get("crash_type", "")
        if crash_type in ["segmentation_fault", "illegal_instruction"]:
            severity_score += 3
        elif crash_type in ["abort_signal", "floating_point_exception"]:
            severity_score += 2
        else:
            severity_score += 1

        # Add score for vulnerability indicators
        indicators = analysis.get("vulnerability_indicators", [])
        if "controlled_eip" in indicators:
            severity_score += 3
        if "controlled_register" in indicators:
            severity_score += 2
        if "executable_stack" in indicators:
            severity_score += 2
        if "rwx_mapping" in indicators:
            severity_score += 2

        # Reduce score for mitigations
        mitigations = analysis.get("security_mitigations", [])
        if "aslr_enabled" in mitigations:
            severity_score -= 1

        # Determine severity level
        if severity_score >= 6:
            return CrashSeverity.CRITICAL.value
        if severity_score >= 4:
            return CrashSeverity.HIGH.value
        if severity_score >= 2:
            return CrashSeverity.MEDIUM.value
        return CrashSeverity.LOW.value

    def _assess_security_impact(self, analysis: dict[str, Any]) -> dict[str, Any]:
        """Assess security impact of crash."""
        impact = {
            "confidentiality": "none",
            "integrity": "none",
            "availability": "low",
            "attack_vector": "local",
            "privileges_required": "none",
            "user_interaction": "required",
        }

        # Determine impact based on vulnerability indicators
        indicators = analysis.get("vulnerability_indicators", [])

        if "controlled_eip" in indicators or "code_injection" in indicators:
            impact.update({
                "confidentiality": "high",
                "integrity": "high",
                "availability": "high",
                "attack_vector": "network",
                "privileges_required": "none",
            })

        elif "controlled_register" in indicators or "memory_corruption" in analysis.get("root_cause", ""):
            impact.update({
                "confidentiality": "partial",
                "integrity": "partial",
                "availability": "high",
            })

        return impact

    def _assess_exploitability(self, debug_info: dict[str, Any],
                             crash_analysis: dict[str, Any]) -> str:
        """Assess crash exploitability."""
        exploitability_score = 0

        try:
            # Check vulnerability indicators
            indicators = crash_analysis.get("vulnerability_indicators", [])

            # High exploitability indicators
            if "controlled_eip" in indicators:
                exploitability_score += 5  # Direct control flow hijacking
            if "controlled_register" in indicators:
                exploitability_score += 3  # Register control
            if "code_injection" in indicators:
                exploitability_score += 4  # Code injection possible
            if "executable_stack" in indicators:
                exploitability_score += 3  # NX bypass not needed
            if "rwx_mapping" in indicators:
                exploitability_score += 3  # Code execution region

            # Medium exploitability indicators
            if "buffer_boundary_violation" in indicators:
                exploitability_score += 2  # Classic buffer overflow
            if "format_string_injection" in indicators:
                exploitability_score += 2  # Format string vuln
            if "heap_corruption" in indicators:
                exploitability_score += 2  # Heap exploitation
            if "partial_register_control" in indicators:
                exploitability_score += 1  # Partial control

            # Low exploitability indicators
            if "null_pointer_dereference" in indicators:
                exploitability_score += 1  # Usually just DoS
            if "division_by_zero" in indicators:
                exploitability_score += 1  # Arithmetic error

            # Check for exploitation complexity factors
            complexity = crash_analysis.get("exploitation_complexity", "medium")
            if complexity == "low":
                exploitability_score += 2
            elif complexity == "high":
                exploitability_score -= 1

            # Check for security mitigations
            mitigations = crash_analysis.get("security_mitigations", [])
            if "aslr_enabled" in mitigations:
                exploitability_score -= 2  # ASLR bypass needed
            if "dep_enabled" in mitigations:
                exploitability_score -= 2  # DEP/NX bypass needed
            if "stack_canary" in mitigations:
                exploitability_score -= 1  # Stack canary bypass needed
            if "cfi_enabled" in mitigations:
                exploitability_score -= 2  # CFI bypass needed

            # Check crash type
            crash_type = crash_analysis.get("crash_type", "")
            if crash_type == "segmentation_fault":
                exploitability_score += 2  # Memory corruption
            elif crash_type == "illegal_instruction":
                exploitability_score += 3  # Possible ROP/code injection
            elif crash_type == "abort_signal":
                exploitability_score += 1  # Heap corruption

            # Analyze register state for additional context
            registers = debug_info.get("registers", {})
            if registers:
                # Check for interesting register values
                for reg_name, reg_value in registers.items():
                    if isinstance(reg_value, int):
                        # Check for user-controlled values
                        if reg_value in [0x41414141, 0x42424242, 0x43434343]:
                            exploitability_score += 2
                            logger.debug(f"Found controlled register {reg_name}: {hex(reg_value)}")
                        # Check for partial control patterns
                        elif (reg_value & 0xFFFF0000) in [0x41410000, 0x42420000]:
                            exploitability_score += 1
                            logger.debug(f"Found partial control in register {reg_name}: {hex(reg_value)}")

            # Check input analysis for exploitation clues
            input_analysis = debug_info.get("input_analysis", {})
            if input_analysis:
                # Repeated patterns suggest controlled input
                patterns = input_analysis.get("repeated_patterns", [])
                if patterns:
                    exploitability_score += 1

                # Large inputs might indicate buffer overflow
                size = input_analysis.get("size", 0)
                if size > 10000:
                    exploitability_score += 1

            # Determine final exploitability assessment
            if exploitability_score >= 8:
                return "highly_exploitable"
            if exploitability_score >= 5:
                return "likely_exploitable"
            if exploitability_score >= 3:
                return "potentially_exploitable"
            if exploitability_score >= 1:
                return "probably_not_exploitable"
            return "not_exploitable"

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.warning(f"Exploitability assessment failed: {e}")
            return "unknown_exploitability"

    def _generate_crash_recommendations(self, crash_analysis: dict[str, Any]) -> list[str]:
        """Generate recommendations for crash."""
        crash_type = crash_analysis.get("crash_type", "unknown")
        vulnerability_indicators = crash_analysis.get("vulnerability_indicators", [])
        severity = crash_analysis.get("severity", "medium")
        exploitation_complexity = crash_analysis.get("exploitation_complexity", "unknown")

        self.logger.debug(f"Generating recommendations for crash type: {crash_type}")

        recommendations = []

        # Basic recommendations
        recommendations.append("Analyze crash with debugger")
        recommendations.append("Check for input validation")
        recommendations.append("Review buffer bounds checking")

        # Specific recommendations based on crash type
        if crash_type == "segmentation_fault":
            recommendations.extend([
                "Verify memory allocation and deallocation",
                "Check for buffer overflow vulnerabilities",
                "Review pointer arithmetic and array indexing",
            ])
        elif crash_type == "abort_signal":
            recommendations.extend([
                "Check for heap corruption issues",
                "Review memory management for double-free vulnerabilities",
                "Analyze malloc/free patterns",
            ])
        elif crash_type == "illegal_instruction":
            recommendations.extend([
                "Check for code injection vulnerabilities",
                "Review ROP/JOP protection mechanisms",
                "Analyze control flow integrity",
            ])
        elif crash_type == "format_string":
            recommendations.extend([
                "Sanitize format string inputs",
                "Use safer printf family functions",
                "Validate user-controlled format strings",
            ])

        # Recommendations based on vulnerability indicators
        if "controlled_eip" in vulnerability_indicators:
            recommendations.extend([
                "CRITICAL: Implement control flow integrity (CFI)",
                "Enable stack canaries and ASLR",
                "Review return address protection",
            ])

        if "executable_stack" in vulnerability_indicators:
            recommendations.extend([
                "Enable DEP/NX bit protection",
                "Remove executable permissions from stack",
                "Use non-executable memory mappings",
            ])

        if "rwx_mapping" in vulnerability_indicators:
            recommendations.extend([
                "Eliminate Read-Write-Execute memory mappings",
                "Use W^X (Write XOR Execute) protection",
                "Review JIT compiler security",
            ])

        # Recommendations based on severity
        if severity in ["critical", "high"]:
            recommendations.extend([
                "URGENT: Prioritize immediate patching",
                "Consider disabling affected functionality",
                "Implement additional security monitoring",
            ])

        # Recommendations based on exploitation complexity
        if exploitation_complexity == "low":
            recommendations.extend([
                "IMMEDIATE ACTION: This vulnerability is easily exploitable",
                "Deploy emergency security measures",
                "Consider temporary service isolation",
            ])
        elif exploitation_complexity == "medium":
            recommendations.append("Monitor for exploitation attempts")

        # Security mitigation recommendations
        general_mitigations = [
            "Enable Address Space Layout Randomization (ASLR)",
            "Use stack protection mechanisms",
            "Implement bounds checking for all buffer operations",
            "Enable compiler security features (-fstack-protector, -D_FORTIFY_SOURCE)",
            "Conduct regular security code reviews",
            "Use static analysis tools to detect similar issues",
        ]

        recommendations.extend(general_mitigations)

        return list(set(recommendations))  # Remove duplicates

    def _minimize_binary_search(self, data: bytes, target_command: str) -> bytes:
        """Minimize using binary search."""
        # Simplified minimization
        current = data

        while len(current) > 1:
            # Try removing half
            half = len(current) // 2
            test_data = current[:half]

            # Test if it still crashes
            execution_result = self._execute_target(target_command, test_data, "/tmp")

            if execution_result["crashed"]:
                current = test_data
            else:
                break

        return current

    def _minimize_delta_debugging(self, data: bytes, target_command: str) -> bytes:
        """Minimize using delta debugging."""
        # Simplified delta debugging
        return self._minimize_binary_search(data, target_command)

    def get_statistics(self) -> dict[str, Any]:
        """Get fuzzing statistics."""
        return self.stats.copy()

    def get_crashes(self) -> dict[str, Any]:
        """Get crash information."""
        return {
            "total_crashes": self.stats["crashes_found"],
            "unique_crashes": self.stats["unique_crashes"],
            "crash_details": list(self.unique_crashes.values()),
        }

    def export_results(self, output_file: str, format: str = "json") -> bool:
        """Export fuzzing results."""
        try:
            results = {
                "statistics": self.get_statistics(),
                "crashes": self.get_crashes(),
                "configuration": self.config,
            }

            if format == "json":
                import json
                with open(output_file, "w") as f:
                    json.dump(results, f, indent=2)
            else:
                with open(output_file, "w") as f:
                    f.write(str(results))

            return True

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.error(f"Results export failed: {e}")
            return False
