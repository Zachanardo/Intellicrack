"""This file is part of Intellicrack.
Copyright (C) 2025 Zachary Flint.

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see https://www.gnu.org/licenses/.
"""

import logging
import multiprocessing
import os
import random
import re
import shlex
import struct
import subprocess
import tempfile
import time
from enum import Enum
from typing import Any

from ...utils.severity_levels import VulnerabilityLevel

"""
Fuzzing Engine

Intelligent fuzzing framework for vulnerability discovery with multiple
fuzzing strategies, coverage-guided fuzzing, and crash analysis.
"""

logger = logging.getLogger(__name__)


class FuzzingStrategy(Enum):
    """Fuzzing strategies."""

    RANDOM = "random"
    MUTATION = "mutation"
    GENERATION = "generation"
    GRAMMAR_BASED = "grammar_based"
    COVERAGE_GUIDED = "coverage_guided"
    HYBRID = "hybrid"


# Use VulnerabilityLevel as CrashSeverity for consistency
CrashSeverity = VulnerabilityLevel


class FuzzingEngine:
    """Advanced fuzzing engine with multiple strategies and crash analysis."""

    def __init__(self):
        """Initialize the fuzzing engine with mutation strategies and crash detection capabilities."""
        self.logger = logging.getLogger("IntellicrackLogger.FuzzingEngine")

        # Fuzzing configuration
        self.config = {
            "max_iterations": 10000,
            "max_file_size": 1024 * 1024,  # 1MB
            "timeout": 30,  # seconds
            "parallel_workers": multiprocessing.cpu_count(),
            "crash_detection": True,
            "coverage_collection": False,
            "mutation_rate": 0.1,
            "crossover_rate": 0.3,
        }

        # Mutation strategies
        self.mutation_strategies = {
            "bit_flip": self._mutate_bit_flip,
            "byte_flip": self._mutate_byte_flip,
            "arithmetic": self._mutate_arithmetic,
            "insert": self._mutate_insert,
            "delete": self._mutate_delete,
            "duplicate": self._mutate_duplicate,
            "splice": self._mutate_splice,
            "magic_values": self._mutate_magic_values,
            "string_replace": self._mutate_string_replace,
            "format_aware": self._mutate_format_aware,
        }

        # Magic values for mutations
        self.magic_values = {
            "integers": [0, 1, -1, 255, 256, 65535, 65536, 2147483647, -2147483648],
            "strings": [b"A" * 1000, b"\x00" * 100, b"\xff" * 100, b"%s%s%s%s"],
            "format_strings": [b"%n", b"%x", b"%s", b"%d", b"%p"],
            "special_chars": [b"\x00", b"\xff", b"\x7f", b"\x80", b"\x0a", b"\x0d"],
        }

        # File format grammars
        self.file_grammars = {
            "text": self._generate_text_grammar,
            "xml": self._generate_xml_grammar,
            "json": self._generate_json_grammar,
            "http": self._generate_http_grammar,
            "binary": self._generate_binary_grammar,
        }

        # Coverage data
        self.coverage_data = {}
        self.interesting_inputs = []

        # Crash analysis
        self.crashes = {}
        self.unique_crashes = {}

        # Statistics
        self.stats = {
            "total_executions": 0,
            "crashes_found": 0,
            "unique_crashes": 0,
            "hangs_found": 0,
            "coverage_paths": 0,
            "execution_speed": 0.0,
            "start_time": 0,
            "elapsed_time": 0,
        }

    def start_fuzzing(
        self,
        target_command: str,
        seed_inputs: list[str] | None = None,
        strategy: FuzzingStrategy = FuzzingStrategy.HYBRID,
        max_iterations: int | None = None,
    ) -> dict[str, Any]:
        """Start fuzzing campaign against target.

        Args:
            target_command: Command to execute target (use @@@ as the substitution marker for input file path)
            seed_inputs: List of seed input files
            strategy: Fuzzing strategy to use
            max_iterations: Maximum number of iterations

        Returns:
            Fuzzing campaign results

        """
        result = {
            "success": False,
            "strategy": strategy.value,
            "target_command": target_command,
            "iterations_completed": 0,
            "crashes_found": [],
            "unique_crashes": [],
            "statistics": {},
            "campaign_id": self._generate_campaign_id(),
            "error": None,
        }

        try:
            self.logger.info(f"Starting fuzzing campaign: {strategy.value}")

            # Initialize campaign
            self.stats["start_time"] = time.time()
            self.stats["total_executions"] = 0

            # Set configuration
            if max_iterations:
                self.config["max_iterations"] = max_iterations

            # Prepare seed inputs
            if not seed_inputs:
                seed_inputs = self._generate_default_seeds()

            # Validate target command
            if "@@@" not in target_command:
                result["error"] = "Target command must contain @@@ substitution marker for input file path"
                return result

            # Create output directories
            output_dir = self._create_output_directories(result["campaign_id"])

            # Execute fuzzing strategy
            if strategy == FuzzingStrategy.RANDOM:
                campaign_result = self._random_fuzzing(target_command, seed_inputs, output_dir)
            elif strategy == FuzzingStrategy.MUTATION:
                campaign_result = self._mutation_fuzzing(target_command, seed_inputs, output_dir)
            elif strategy == FuzzingStrategy.GENERATION:
                campaign_result = self._generation_fuzzing(target_command, output_dir)
            elif strategy == FuzzingStrategy.GRAMMAR_BASED:
                campaign_result = self._grammar_based_fuzzing(target_command, seed_inputs, output_dir)
            elif strategy == FuzzingStrategy.COVERAGE_GUIDED:
                campaign_result = self._coverage_guided_fuzzing(target_command, seed_inputs, output_dir)
            elif strategy == FuzzingStrategy.HYBRID:
                campaign_result = self._hybrid_fuzzing(target_command, seed_inputs, output_dir)
            else:
                result["error"] = f"Unknown fuzzing strategy: {strategy.value}"
                return result

            # Finalize results
            self.stats["elapsed_time"] = time.time() - self.stats["start_time"]

            result.update(campaign_result)
            result["statistics"] = self.stats.copy()
            result["success"] = True

            self.logger.info(f"Fuzzing campaign completed: {result['iterations_completed']} iterations")
            return result

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.error(f"Fuzzing campaign failed: {e}")
            result["error"] = str(e)
            return result

    def analyze_crash(self, crash_file: str, target_command: str) -> dict[str, Any]:
        """Analyze crash for exploitability and root cause.

        Args:
            crash_file: Path to input that caused crash
            target_command: Target command used

        Returns:
            Crash analysis results

        """
        result = {
            "success": False,
            "crash_file": crash_file,
            "crash_type": None,
            "severity": CrashSeverity.INFO.value,
            "exploitability": "unknown",
            "root_cause": None,
            "stack_trace": None,
            "registers": {},
            "memory_info": {},
            "recommendations": [],
            "error": None,
        }

        try:
            self.logger.info(f"Analyzing crash: {crash_file}")

            if not os.path.exists(crash_file):
                result["error"] = f"Crash file not found: {crash_file}"
                return result

            # Execute target with crash input under debugger
            debug_info = self._debug_crash(target_command, crash_file)

            if debug_info:
                # Analyze crash information
                crash_analysis = self._analyze_crash_info(debug_info)
                result.update(crash_analysis)

                # Assess exploitability
                exploitability = self._assess_exploitability(debug_info, crash_analysis)
                result["exploitability"] = exploitability

                # Generate recommendations
                recommendations = self._generate_crash_recommendations(crash_analysis)
                result["recommendations"] = recommendations

                result["success"] = True
            else:
                result["error"] = "Failed to reproduce crash under debugger"

            return result

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.error(f"Crash analysis failed: {e}")
            result["error"] = str(e)
            return result

    def minimize_testcase(self, crash_file: str, target_command: str, strategy: str = "binary_search") -> dict[str, Any]:
        """Minimize crash testcase to smallest reproducing input.

        Args:
            crash_file: Path to crash input
            target_command: Target command
            strategy: Minimization strategy

        Returns:
            Minimization results

        """
        result = {
            "success": False,
            "original_file": crash_file,
            "minimized_file": None,
            "original_size": 0,
            "minimized_size": 0,
            "reduction_ratio": 0.0,
            "iterations": 0,
            "error": None,
        }

        try:
            self.logger.info(f"Minimizing testcase: {crash_file}")

            if not os.path.exists(crash_file):
                result["error"] = f"Crash file not found: {crash_file}"
                return result

            # Read original input
            with open(crash_file, "rb") as f:
                original_data = f.read()

            result["original_size"] = len(original_data)

            # Perform minimization
            if strategy == "binary_search":
                minimized_data = self._minimize_binary_search(original_data, target_command)
            elif strategy == "delta_debugging":
                minimized_data = self._minimize_delta_debugging(original_data, target_command)
            else:
                result["error"] = f"Unknown minimization strategy: {strategy}"
                return result

            # Save minimized input
            minimized_file = crash_file.replace(".crash", ".minimized")
            with open(minimized_file, "wb") as f:
                f.write(minimized_data)

            result["minimized_file"] = minimized_file
            result["minimized_size"] = len(minimized_data)
            result["reduction_ratio"] = 1.0 - (len(minimized_data) / len(original_data))
            result["success"] = True

            self.logger.info(f"Testcase minimized: {result['original_size']} -> {result['minimized_size']} bytes")
            return result

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.error(f"Testcase minimization failed: {e}")
            result["error"] = str(e)
            return result

    def _random_fuzzing(self, target_command: str, seed_inputs: list[str], output_dir: str) -> dict[str, Any]:
        """Perform random fuzzing."""
        result = {
            "iterations_completed": 0,
            "crashes_found": [],
            "unique_crashes": [],
        }

        try:
            self.logger.info("Starting random fuzzing")

            for iteration in range(self.config["max_iterations"]):
                # Generate random input
                if seed_inputs and random.random() < 0.5:  # noqa: S311
                    # Use seed as base
                    seed_file = random.choice(seed_inputs)  # noqa: S311
                    with open(seed_file, "rb") as f:
                        base_data = f.read()
                    fuzz_data = self._randomize_data(base_data)
                else:
                    # Generate completely random data
                    size = random.randint(1, self.config["max_file_size"])  # noqa: S311 - Fuzzing test data generation
                    fuzz_data = bytes([random.randint(0, 255) for _ in range(size)])  # noqa: S311 - Fuzzing test data generation

                # Execute target
                execution_result = self._execute_target(target_command, fuzz_data, output_dir)

                if execution_result["crashed"]:
                    crash_info = self._process_crash(execution_result, fuzz_data, output_dir, iteration)
                    result["crashes_found"].append(crash_info)

                result["iterations_completed"] += 1
                self.stats["total_executions"] += 1

                # Progress logging
                if iteration % 1000 == 0:
                    self.logger.info(f"Random fuzzing progress: {iteration}/{self.config['max_iterations']}")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Random fuzzing error: {e}")

        return result

    def _mutation_fuzzing(self, target_command: str, seed_inputs: list[str], output_dir: str) -> dict[str, Any]:
        """Perform mutation-based fuzzing."""
        result = {
            "iterations_completed": 0,
            "crashes_found": [],
            "unique_crashes": [],
        }

        try:
            self.logger.info("Starting mutation fuzzing")

            # Load seed inputs
            seed_data = []
            for seed_file in seed_inputs:
                try:
                    with open(seed_file, "rb") as f:
                        seed_data.append(f.read())
                except (OSError, ValueError, RuntimeError) as e:
                    self.logger.error("Error in fuzzing_engine: %s", e)
                    continue

            if not seed_data:
                seed_data = [b"AAAA"]  # Minimal seed

            for iteration in range(self.config["max_iterations"]):
                # Select seed
                base_data = random.choice(seed_data)  # noqa: S311 - Fuzzing seed selection

                # Apply mutations
                mutated_data = self._apply_mutations(base_data)

                # Execute target
                execution_result = self._execute_target(target_command, mutated_data, output_dir)

                if execution_result["crashed"]:
                    crash_info = self._process_crash(execution_result, mutated_data, output_dir, iteration)
                    result["crashes_found"].append(crash_info)

                # Add interesting inputs to seed pool
                if self._is_interesting_input(execution_result):
                    seed_data.append(mutated_data)

                result["iterations_completed"] += 1
                self.stats["total_executions"] += 1

                # Progress logging
                if iteration % 1000 == 0:
                    self.logger.info(f"Mutation fuzzing progress: {iteration}/{self.config['max_iterations']}")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Mutation fuzzing error: {e}")

        return result

    def _generation_fuzzing(self, target_command: str, output_dir: str) -> dict[str, Any]:
        """Perform generative fuzzing."""
        result = {
            "iterations_completed": 0,
            "crashes_found": [],
            "unique_crashes": [],
        }

        try:
            self.logger.info("Starting generation fuzzing")

            for iteration in range(self.config["max_iterations"]):
                # Generate input based on common patterns
                generated_data = self._generate_input()

                # Execute target
                execution_result = self._execute_target(target_command, generated_data, output_dir)

                if execution_result["crashed"]:
                    crash_info = self._process_crash(execution_result, generated_data, output_dir, iteration)
                    result["crashes_found"].append(crash_info)

                result["iterations_completed"] += 1
                self.stats["total_executions"] += 1

                # Progress logging
                if iteration % 1000 == 0:
                    self.logger.info(f"Generation fuzzing progress: {iteration}/{self.config['max_iterations']}")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Generation fuzzing error: {e}")

        return result

    def _grammar_based_fuzzing(self, target_command: str, seed_inputs: list[str], output_dir: str) -> dict[str, Any]:
        """Perform grammar-based fuzzing."""
        result = {
            "iterations_completed": 0,
            "crashes_found": [],
            "unique_crashes": [],
        }

        try:
            self.logger.info("Starting grammar-based fuzzing")

            # Detect file format from seeds
            file_format = self._detect_file_format(seed_inputs)

            for iteration in range(self.config["max_iterations"]):
                # Generate input using grammar
                if file_format in self.file_grammars:
                    grammar_func = self.file_grammars[file_format]
                    generated_data = grammar_func()
                else:
                    # Default to text generation
                    generated_data = self._generate_text_grammar()

                # Execute target
                execution_result = self._execute_target(target_command, generated_data, output_dir)

                if execution_result["crashed"]:
                    crash_info = self._process_crash(execution_result, generated_data, output_dir, iteration)
                    result["crashes_found"].append(crash_info)

                result["iterations_completed"] += 1
                self.stats["total_executions"] += 1

                # Progress logging
                if iteration % 1000 == 0:
                    self.logger.info(f"Grammar fuzzing progress: {iteration}/{self.config['max_iterations']}")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Grammar-based fuzzing error: {e}")

        return result

    def _coverage_guided_fuzzing(self, target_command: str, seed_inputs: list[str], output_dir: str) -> dict[str, Any]:
        """Perform coverage-guided fuzzing."""
        result = {
            "iterations_completed": 0,
            "crashes_found": [],
            "unique_crashes": [],
        }

        try:
            self.logger.info("Starting coverage-guided fuzzing")

            # Initialize with seeds
            queue = []
            for seed_file in seed_inputs:
                try:
                    with open(seed_file, "rb") as f:
                        data = f.read()
                        queue.append(data)
                except (OSError, ValueError, RuntimeError) as e:
                    self.logger.error("Error in fuzzing_engine: %s", e)
                    continue

            if not queue:
                queue = [b"AAAA"]

            for iteration in range(self.config["max_iterations"]):
                # Select input from queue
                if queue:
                    base_data = random.choice(queue)  # noqa: S311 - Fuzzing input queue selection
                else:
                    base_data = b"AAAA"

                # Mutate
                mutated_data = self._apply_mutations(base_data)

                # Execute with coverage collection
                execution_result = self._execute_target_with_coverage(target_command, mutated_data, output_dir)

                if execution_result["crashed"]:
                    crash_info = self._process_crash(execution_result, mutated_data, output_dir, iteration)
                    result["crashes_found"].append(crash_info)

                # Check for new coverage
                if execution_result.get("new_coverage"):
                    queue.append(mutated_data)
                    self.stats["coverage_paths"] += 1

                result["iterations_completed"] += 1
                self.stats["total_executions"] += 1

                # Progress logging
                if iteration % 1000 == 0:
                    self.logger.info(f"Coverage fuzzing progress: {iteration}/{self.config['max_iterations']}")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Coverage-guided fuzzing error: {e}")

        return result

    def _hybrid_fuzzing(self, target_command: str, seed_inputs: list[str], output_dir: str) -> dict[str, Any]:
        """Perform hybrid fuzzing combining multiple strategies."""
        result = {
            "iterations_completed": 0,
            "crashes_found": [],
            "unique_crashes": [],
        }

        try:
            self.logger.info("Starting hybrid fuzzing")

            strategies = [
                ("mutation", 0.4),
                ("random", 0.2),
                ("generation", 0.2),
                ("grammar", 0.2),
            ]

            # Load seeds
            seed_data = []
            for seed_file in seed_inputs:
                try:
                    with open(seed_file, "rb") as f:
                        seed_data.append(f.read())
                except (OSError, ValueError, RuntimeError) as e:
                    logger.error("Error in fuzzing_engine: %s", e)
                    continue

            if not seed_data:
                seed_data = [b"AAAA"]

            for iteration in range(self.config["max_iterations"]):
                # Select strategy based on probabilities
                strategy = self._select_strategy(strategies)

                # Generate input based on strategy
                if strategy == "mutation":
                    base_data = random.choice(seed_data)  # noqa: S311 - Fuzzing seed data selection
                    fuzz_data = self._apply_mutations(base_data)
                elif strategy == "random":
                    size = random.randint(1, min(1000, self.config["max_file_size"]))  # noqa: S311 - Fuzzing test data size generation
                    fuzz_data = bytes([random.randint(0, 255) for _ in range(size)])  # noqa: S311 - Fuzzing test data byte generation
                elif strategy == "generation":
                    fuzz_data = self._generate_input()
                elif strategy == "grammar":
                    file_format = self._detect_file_format(seed_inputs)
                    if file_format in self.file_grammars:
                        fuzz_data = self.file_grammars[file_format]()
                    else:
                        fuzz_data = self._generate_text_grammar()
                else:
                    # Fallback to random data if strategy is unknown
                    fuzz_data = bytes([random.randint(0, 255) for _ in range(100)])  # noqa: S311 - Fuzzing fallback data generation

                # Execute target
                execution_result = self._execute_target(target_command, fuzz_data, output_dir)

                if execution_result["crashed"]:
                    crash_info = self._process_crash(execution_result, fuzz_data, output_dir, iteration)
                    result["crashes_found"].append(crash_info)

                # Add interesting inputs to seeds
                if self._is_interesting_input(execution_result):
                    seed_data.append(fuzz_data)
                    if len(seed_data) > 1000:  # Limit seed pool size
                        seed_data = seed_data[-1000:]

                result["iterations_completed"] += 1
                self.stats["total_executions"] += 1

                # Progress logging
                if iteration % 1000 == 0:
                    self.logger.info(f"Hybrid fuzzing progress: {iteration}/{self.config['max_iterations']}")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Hybrid fuzzing error: {e}")

        return result

    def _execute_target(self, target_command: str, input_data: bytes, output_dir: str) -> dict[str, Any]:
        """Execute target with input data."""
        result = {
            "crashed": False,
            "hanged": False,
            "exit_code": 0,
            "stdout": "",
            "stderr": "",
            "execution_time": 0.0,
            "signal": None,
        }

        try:
            # Create temporary input file
            with tempfile.NamedTemporaryFile(mode="wb", delete=False, dir=output_dir) as f:
                f.write(input_data)
                input_file = f.name

            # Substitute input file path marker in command
            cmd = target_command.replace("@@@", input_file)

            start_time = time.time()

            try:
                # Execute with timeout
                process = subprocess.Popen(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                    cmd.split(),
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                )

                try:
                    stdout, stderr = process.communicate(timeout=self.config["timeout"])
                    result["exit_code"] = process.returncode
                    result["stdout"] = stdout
                    result["stderr"] = stderr

                except subprocess.TimeoutExpired as e:
                    logger.error("Subprocess timeout in fuzzing_engine: %s", e)
                    process.kill()
                    result["hanged"] = True

            except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
                logger.error("Error in fuzzing_engine: %s", e)
                result["crashed"] = True
                result["stderr"] = str(e)

            result["execution_time"] = time.time() - start_time

            # Check for crash indicators
            if result["exit_code"] < 0:
                result["crashed"] = True
                result["signal"] = -result["exit_code"]

            # Cleanup
            try:
                os.unlink(input_file)
            except (OSError, ValueError, RuntimeError) as e:
                logger.error("Error in fuzzing_engine: %s", e)

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Target execution error: {e}")
            result["crashed"] = True

        return result

    def _execute_target_with_coverage(self, target_command: str, input_data: bytes, output_dir: str) -> dict[str, Any]:
        """Execute target with real coverage collection using gcov or AFL++."""
        result = self._execute_target(target_command, input_data, output_dir)

        # Real coverage collection implementation
        coverage_dir = os.path.join(output_dir, "coverage")
        os.makedirs(coverage_dir, exist_ok=True)

        # Check if target is instrumented with AFL++
        if "afl-" in target_command or os.environ.get("AFL_USE_ASAN"):
            # Use AFL++ coverage information
            result["new_coverage"] = self._check_afl_coverage(target_command, coverage_dir)
        else:
            # Try gcov for coverage
            result["new_coverage"] = self._check_gcov_coverage(target_command, coverage_dir)

        # If no coverage tools available, check for new execution paths
        if result["new_coverage"] is None:
            result["new_coverage"] = self._check_execution_paths(result, coverage_dir)

        return result

    def _check_afl_coverage(self, target_command: str, coverage_dir: str) -> bool:
        """Check AFL++ coverage information."""
        try:
            # Check AFL coverage map
            afl_out_dir = os.environ.get("AFL_OUT_DIR", os.path.join(tempfile.gettempdir(), "afl_out"))
            coverage_map = os.path.join(afl_out_dir, "fuzz_bitmap")

            if os.path.exists(coverage_map):
                # Read current coverage bitmap
                with open(coverage_map, "rb") as f:
                    current_coverage = f.read()

                # Compare with previous coverage
                prev_coverage_file = os.path.join(coverage_dir, "prev_coverage.bin")
                new_coverage = False

                if os.path.exists(prev_coverage_file):
                    with open(prev_coverage_file, "rb") as f:
                        prev_coverage = f.read()

                    # Check if any new bits are set
                    for i in range(min(len(current_coverage), len(prev_coverage))):
                        if current_coverage[i] != prev_coverage[i]:
                            new_coverage = True
                            break
                else:
                    new_coverage = True

                # Save current coverage for next comparison
                with open(prev_coverage_file, "wb") as f:
                    f.write(current_coverage)

                return new_coverage

        except Exception as e:
            self.logger.debug(f"AFL coverage check failed: {e}")

        return False

    def _check_gcov_coverage(self, target_command: str, coverage_dir: str) -> bool:
        """Check gcov coverage information."""
        try:
            # Run gcov to generate coverage data
            target_binary = target_command.split()[0]
            gcov_cmd = ["gcov", "-b", "-c", target_binary]

            result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                gcov_cmd,
                check=False,
                cwd=coverage_dir,
                capture_output=True,
                text=True,
                timeout=10,
            )

            if result.returncode == 0:
                # Parse gcov output for new lines/branches
                output_lines = result.stdout.split("\n")
                for line in output_lines:
                    if "Lines executed:" in line or "Branches executed:" in line:
                        # Extract percentage
                        percent_match = re.search(r"(\d+\.\d+)%", line)
                        if percent_match:
                            coverage_percent = float(percent_match.group(1))

                            # Check if coverage increased
                            prev_coverage_file = os.path.join(coverage_dir, "prev_coverage.txt")

                            if os.path.exists(prev_coverage_file):
                                with open(prev_coverage_file) as f:
                                    prev_coverage = float(f.read().strip())

                                if coverage_percent > prev_coverage:
                                    with open(prev_coverage_file, "w") as f:
                                        f.write(str(coverage_percent))
                                    return True
                            else:
                                with open(prev_coverage_file, "w") as f:
                                    f.write(str(coverage_percent))
                                return True

        except Exception as e:
            self.logger.debug(f"gcov coverage check failed: {e}")

        return False

    def _check_execution_paths(self, exec_result: dict[str, Any], coverage_dir: str) -> bool:
        """Check for new execution paths based on output patterns."""
        try:
            # Create hash of execution characteristics
            import hashlib

            exec_signature = hashlib.sha256()
            exec_signature.update(str(exec_result["exit_code"]).encode())
            exec_signature.update(exec_result["stdout"].encode())
            exec_signature.update(exec_result["stderr"].encode())

            # Add crash/hang status
            if exec_result["crashed"]:
                exec_signature.update(b"CRASHED")
            if exec_result["hanged"]:
                exec_signature.update(b"HANGED")
            if exec_result.get("signal"):
                exec_signature.update(str(exec_result["signal"]).encode())

            signature_hex = exec_signature.hexdigest()

            # Check if this execution path is new
            paths_file = os.path.join(coverage_dir, "execution_paths.txt")

            if os.path.exists(paths_file):
                with open(paths_file) as f:
                    known_paths = set(line.strip() for line in f)

                if signature_hex not in known_paths:
                    # New execution path found
                    with open(paths_file, "a") as f:
                        f.write(signature_hex + "\n")
                    return True
            else:
                # First execution
                with open(paths_file, "w") as f:
                    f.write(signature_hex + "\n")
                return True

        except Exception as e:
            self.logger.debug(f"Execution path check failed: {e}")

        return False

    def _apply_mutations(self, data: bytes) -> bytes:
        """Apply random mutations to data."""
        if not data:
            return data

        mutated = bytearray(data)

        # Apply random number of mutations
        num_mutations = random.randint(1, max(1, int(len(data) * self.config["mutation_rate"])))  # noqa: S311 - Fuzzing mutation count

        for _ in range(num_mutations):
            # Select random mutation strategy
            strategy = random.choice(list(self.mutation_strategies.keys()))  # noqa: S311 - Fuzzing mutation strategy selection
            mutation_func = self.mutation_strategies[strategy]

            try:
                mutated = mutation_func(mutated)
            except (OSError, ValueError, RuntimeError) as e:
                self.logger.error("Error in fuzzing_engine: %s", e)
                continue

        return bytes(mutated)

    # Mutation strategy implementations

    def _mutate_bit_flip(self, data: bytearray) -> bytearray:
        """Flip random bits."""
        if not data:
            return data

        pos = random.randint(0, len(data) - 1)  # noqa: S311 - Fuzzing bit flip position
        bit_pos = random.randint(0, 7)  # noqa: S311 - Fuzzing bit position selection
        data[pos] ^= 1 << bit_pos
        return data

    def _mutate_byte_flip(self, data: bytearray) -> bytearray:
        """Flip random bytes."""
        if not data:
            return data

        pos = random.randint(0, len(data) - 1)  # noqa: S311 - Fuzzing byte position selection
        data[pos] = random.randint(0, 255)  # noqa: S311 - Fuzzing byte value mutation
        return data

    def _mutate_arithmetic(self, data: bytearray) -> bytearray:
        """Apply arithmetic operations."""
        if len(data) < 2:
            return data

        pos = random.randint(0, len(data) - 2)  # noqa: S311 - Fuzzing position selection for arithmetic mutation

        # Interpret as various integer types
        if random.choice([True, False]):  # noqa: S311 - Fuzzing choice between 8-bit and 16-bit arithmetic mutation
            # 16-bit integer
            if pos <= len(data) - 2:
                val = struct.unpack("<H", data[pos : pos + 2])[0]
                val = (val + random.randint(-100, 100)) % 65536  # noqa: S311 - Fuzzing arithmetic offset for 16-bit value mutation
                struct.pack_into("<H", data, pos, val)
        else:
            # 8-bit integer
            data[pos] = (data[pos] + random.randint(-50, 50)) % 256  # noqa: S311 - Fuzzing arithmetic offset for 8-bit value mutation

        return data

    def _mutate_insert(self, data: bytearray) -> bytearray:
        """Insert random bytes."""
        pos = random.randint(0, len(data))  # noqa: S311 - Fuzzing insertion position selection
        insert_data = bytes([random.randint(0, 255) for _ in range(random.randint(1, 10))])  # noqa: S311 - Fuzzing random bytes and length for insertion
        return bytearray(data[:pos] + insert_data + data[pos:])

    def _mutate_delete(self, data: bytearray) -> bytearray:
        """Delete random bytes."""
        if len(data) <= 1:
            return data

        start = random.randint(0, len(data) - 1)  # noqa: S311 - Fuzzing deletion start position selection
        end = random.randint(start, min(start + 10, len(data)))  # noqa: S311 - Fuzzing deletion end position selection
        return bytearray(data[:start] + data[end:])

    def _mutate_duplicate(self, data: bytearray) -> bytearray:
        """Duplicate random chunks."""
        if not data:
            return data

        start = random.randint(0, len(data) - 1)  # noqa: S311 - Fuzzing chunk duplication start position
        end = random.randint(start, min(start + 10, len(data)))  # noqa: S311 - Fuzzing chunk duplication end position
        chunk = data[start:end]

        pos = random.randint(0, len(data))  # noqa: S311 - Fuzzing duplicated chunk insertion position
        return bytearray(data[:pos] + chunk + data[pos:])

    def _mutate_splice(self, data: bytearray) -> bytearray:
        """Splice with magic values."""
        if not data:
            return data

        pos = random.randint(0, len(data))  # noqa: S311 - Fuzzing magic value splice position
        magic_value = random.choice(self.magic_values["special_chars"])  # noqa: S311 - Fuzzing magic value selection for splicing
        return bytearray(data[:pos] + magic_value + data[pos:])

    def _mutate_magic_values(self, data: bytearray) -> bytearray:
        """Replace with magic values."""
        if len(data) < 4:
            return data

        pos = random.randint(0, len(data) - 4)  # noqa: S311 - Fuzzing magic integer replacement position
        magic_int = random.choice(self.magic_values["integers"])  # noqa: S311 - Fuzzing magic integer value selection
        struct.pack_into("<I", data, pos, magic_int % (2**32))
        return data

    def _mutate_string_replace(self, data: bytearray) -> bytearray:
        """Replace strings with format string payloads."""
        magic_string = random.choice(self.magic_values["format_strings"])  # noqa: S311 - Fuzzing format string payload selection

        if len(data) >= len(magic_string):
            pos = random.randint(0, len(data) - len(magic_string))  # noqa: S311 - Fuzzing string replacement position
            data[pos : pos + len(magic_string)] = magic_string

        return data

    def _mutate_format_aware(self, data: bytearray) -> bytearray:
        """Format-aware mutations."""
        # This would implement format-specific mutations
        # For now, just apply a random mutation
        return self._mutate_byte_flip(data)

    def _generate_input(self) -> bytes:
        """Generate input using common patterns."""
        patterns = [
            self._generate_overflow_pattern,
            self._generate_format_string_pattern,
            self._generate_unicode_pattern,
            self._generate_binary_pattern,
        ]

        generator = random.choice(patterns)  # noqa: S311 - Fuzzing input pattern generator selection
        return generator()

    def _generate_overflow_pattern(self) -> bytes:
        """Generate buffer overflow pattern."""
        size = random.randint(100, 2000)  # noqa: S311 - Fuzzing buffer overflow pattern size
        char = random.choice([b"A", b"B", b"C", b"\x41"])  # noqa: S311 - Fuzzing overflow character selection
        return char[0:1] * size

    def _generate_format_string_pattern(self) -> bytes:
        """Generate format string pattern."""
        formats = [b"%s", b"%x", b"%n", b"%p", b"%d"]
        num_formats = random.randint(1, 20)  # noqa: S311 - Fuzzing format string count
        return b"".join(random.choice(formats) for _ in range(num_formats))  # noqa: S311 - Fuzzing format string selection

    def _generate_unicode_pattern(self) -> bytes:
        """Generate Unicode pattern."""
        unicode_chars = [b"\xff\xfe", b"\xfe\xff", b"\xef\xbb\xbf"]
        base = random.choice(unicode_chars)  # noqa: S311 - Fuzzing Unicode BOM selection
        return base + b"A" * random.randint(10, 100)  # noqa: S311 - Fuzzing Unicode pattern size

    def _generate_binary_pattern(self) -> bytes:
        """Generate binary pattern."""
        size = random.randint(10, 1000)  # noqa: S311 - Fuzzing binary pattern size
        return bytes([random.randint(0, 255) for _ in range(size)])  # noqa: S311 - Fuzzing random binary data generation

    # File format grammar generators

    def _generate_text_grammar(self) -> bytes:
        """Generate text input."""
        words = [b"hello", b"world", b"test", b"data", b"input"]
        num_words = random.randint(1, 20)  # noqa: S311 - Fuzzing text word count
        return b" ".join(random.choice(words) for _ in range(num_words))  # noqa: S311 - Fuzzing text word selection

    def _generate_xml_grammar(self) -> bytes:
        """Generate XML input."""
        tag = random.choice([b"root", b"data", b"element"])  # noqa: S311 - Fuzzing XML tag selection
        content = b"A" * random.randint(1, 100)  # noqa: S311 - Fuzzing XML content size
        return b'<?xml version="1.0"?><' + tag + b">" + content + b"</" + tag + b">"

    def _generate_json_grammar(self) -> bytes:
        """Generate JSON input."""
        key = b"key" + str(random.randint(1, 100)).encode()  # noqa: S311 - Fuzzing JSON key generation
        value = b"A" * random.randint(1, 100)  # noqa: S311 - Fuzzing JSON value size
        return b'{"' + key + b'": "' + value + b'"}'

    def _generate_http_grammar(self) -> bytes:
        """Generate HTTP input."""
        method = random.choice([b"GET", b"POST", b"PUT"])  # noqa: S311 - Fuzzing HTTP method selection
        path = b"/" + b"A" * random.randint(1, 100)  # noqa: S311 - Fuzzing HTTP path size
        return method + b" " + path + b" HTTP/1.1\r\nHost: test\r\n\r\n"

    def _generate_binary_grammar(self) -> bytes:
        """Generate binary input."""
        header = b"BIN\x00"
        size = random.randint(10, 1000)  # noqa: S311 - Fuzzing binary grammar data size
        data = bytes([random.randint(0, 255) for _ in range(size)])  # noqa: S311 - Fuzzing binary grammar random data
        return header + data

    # Helper methods

    def _generate_default_seeds(self) -> list[str]:
        """Generate default seed inputs."""
        seeds = []

        try:
            # Create temporary seed files
            seed_data = [
                b"AAAA",
                b"test input",
                b"\x00\x01\x02\x03",
                b"%s%s%s%s",
                b"A" * 100,
            ]

            temp_dir = tempfile.mkdtemp()

            for i, data in enumerate(seed_data):
                seed_file = os.path.join(temp_dir, f"seed_{i}.dat")
                with open(seed_file, "wb") as f:
                    f.write(data)
                seeds.append(seed_file)

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Default seed generation error: {e}")

        return seeds

    def _detect_file_format(self, seed_inputs: list[str]) -> str:
        """Detect file format from seed inputs."""
        if not seed_inputs:
            return "binary"

        try:
            # Read first seed
            with open(seed_inputs[0], "rb") as f:
                data = f.read(100)  # Read first 100 bytes

            # Simple format detection
            if data.startswith(b"<?xml"):
                return "xml"
            if data.startswith(b"{") or data.startswith(b"["):
                return "json"
            if b"HTTP" in data:
                return "http"
            if all(32 <= b <= 126 for b in data if b != 0):
                return "text"
            return "binary"

        except (OSError, ValueError, RuntimeError) as e:
            logger.error("Error in fuzzing_engine: %s", e)
            return "binary"

    def _randomize_data(self, data: bytes) -> bytes:
        """Randomize data while maintaining some structure."""
        if not data:
            return data

        mutated = bytearray(data)

        # Randomize some percentage of bytes
        num_changes = int(len(data) * random.uniform(0.01, 0.1))  # noqa: S311 - Fuzzing mutation percentage selection

        for _ in range(num_changes):
            pos = random.randint(0, len(mutated) - 1)  # noqa: S311 - Fuzzing mutation position selection
            mutated[pos] = random.randint(0, 255)  # noqa: S311 - Fuzzing random byte mutation

        return bytes(mutated)

    def _is_interesting_input(self, execution_result: dict[str, Any]) -> bool:
        """Check if input is interesting for fuzzing."""
        # Consider input interesting if it has new behavior
        return (
            execution_result.get("new_coverage", False)
            or execution_result.get("hanged", False)
            or execution_result["execution_time"] > self.config["timeout"] * 0.8
        )

    def _select_strategy(self, strategies: list[tuple[str, float]]) -> str:
        """Select fuzzing strategy based on probabilities."""
        rand_val = random.random()  # noqa: S311 - Fuzzing strategy selection based on probability weights
        cumulative = 0.0

        for strategy, probability in strategies:
            cumulative += probability
            if rand_val <= cumulative:
                return strategy

        return strategies[0][0]  # Fallback to first strategy

    def _process_crash(self, execution_result: dict[str, Any], input_data: bytes, output_dir: str, iteration: int) -> dict[str, Any]:
        """Process and analyze crash."""
        crash_info = {
            "iteration": iteration,
            "crash_id": self._generate_crash_id(),
            "input_size": len(input_data),
            "exit_code": execution_result["exit_code"],
            "signal": execution_result.get("signal"),
            "stderr": execution_result["stderr"],
            "execution_time": execution_result["execution_time"],
            "crash_file": None,
            "unique": False,
        }

        try:
            # Save crash input
            crash_file = os.path.join(output_dir, "crashes", f"crash_{crash_info['crash_id']}.dat")
            os.makedirs(os.path.dirname(crash_file), exist_ok=True)

            with open(crash_file, "wb") as f:
                f.write(input_data)

            crash_info["crash_file"] = crash_file

            # Check if crash is unique
            crash_hash = self._calculate_crash_hash(execution_result)

            if crash_hash not in self.unique_crashes:
                self.unique_crashes[crash_hash] = crash_info
                crash_info["unique"] = True
                self.stats["unique_crashes"] += 1

            self.stats["crashes_found"] += 1

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Crash processing error: {e}")

        return crash_info

    def _calculate_crash_hash(self, execution_result: dict[str, Any]) -> str:
        """Calculate hash to identify unique crashes."""
        import hashlib

        # Use exit code, signal, and stderr for uniqueness
        crash_data = f"{execution_result['exit_code']}:{execution_result.get('signal', '')}:{execution_result['stderr'][:100]}"
        return hashlib.sha256(crash_data.encode()).hexdigest()[:16]

    def _create_output_directories(self, campaign_id: str) -> str:
        """Create output directories for fuzzing campaign."""
        output_dir = os.path.join(tempfile.gettempdir(), f"fuzzing_{campaign_id}")

        os.makedirs(output_dir, exist_ok=True)
        os.makedirs(os.path.join(output_dir, "crashes"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "hangs"), exist_ok=True)
        os.makedirs(os.path.join(output_dir, "queue"), exist_ok=True)

        return output_dir

    def _generate_campaign_id(self) -> str:
        """Generate unique campaign identifier."""
        import hashlib
        import time

        data = f"{time.time()}-{random.randint(1000, 9999)}"  # noqa: S311 - Campaign ID generation
        return hashlib.sha256(data.encode()).hexdigest()[:12]

    def _generate_crash_id(self) -> str:
        """Generate unique crash identifier."""
        import hashlib

        data = f"{time.time()}-{random.randint(1000, 9999)}"  # noqa: S311 - Crash ID generation
        return hashlib.sha256(data.encode()).hexdigest()[:8]

    # Crash analysis methods for vulnerability detection and exploitation

    def _debug_crash(self, target_command: str, crash_file: str) -> dict[str, Any] | None:
        """Debug crash under debugger."""
        debug_info = {
            "registers": {},
            "stack_trace": [],
            "memory_maps": [],
            "crash_address": None,
            "crash_instruction": None,
            "fault_type": None,
            "signal_info": {},
            "thread_info": {},
            "loaded_modules": [],
            "heap_info": {},
            "stack_info": {},
        }

        try:
            # Try GDB-based debugging (Linux)
            if os.name == "posix":
                debug_info.update(self._debug_with_gdb(target_command, crash_file))

            # Try WinDbg-based debugging (Windows)
            elif os.name == "nt":
                debug_info.update(self._debug_with_windbg(target_command, crash_file))

            # Fallback to crash pattern analysis
            else:
                debug_info.update(self._analyze_crash_pattern(crash_file))

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.warning(f"Debugging failed: {e}")
            debug_info.update(self._analyze_crash_pattern(crash_file))

        return debug_info

    def _debug_with_gdb(self, target_command: str, crash_file: str) -> dict[str, Any]:
        """Debug crash using GDB."""
        gdb_info = {}

        try:
            # Create GDB script
            gdb_script = f"""
set confirm off
set pagination off
set logging file /tmp/gdb_output.txt
set logging on
file {target_command.split()[0]}
run {crash_file}
info registers
bt
info proc mappings
x/32i $pc-16
x/64wx $sp-32
disas $pc-32,$pc+32
info threads
info sharedlibrary
quit
"""

            gdb_script_file = os.path.join(tempfile.gettempdir(), "gdb_debug.script")
            with open(gdb_script_file, "w") as f:
                f.write(gdb_script)

            # Run GDB
            gdb_cmd = f"gdb -batch -x {gdb_script_file}"
            result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                shlex.split(gdb_cmd),
                check=False,
                capture_output=True,
                text=True,
                timeout=30,
                stderr=subprocess.DEVNULL,
                shell=False,  # Explicitly secure - using list format prevents shell injection
            )

            # Log subprocess result for debugging
            logger.debug(f"GDB command exit code: {result.returncode}")

            # Parse GDB output
            gdb_output_file = os.path.join(tempfile.gettempdir(), "gdb_output.txt")
            if os.path.exists(gdb_output_file):
                with open(gdb_output_file) as f:
                    gdb_output = f.read()

                gdb_info = self._parse_gdb_output(gdb_output)

                # Cleanup
                os.remove(gdb_output_file)
                os.remove(gdb_script_file)

        except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
            self.logger.debug(f"GDB debugging failed: {e}")

        return gdb_info

    def _debug_with_windbg(self, target_command: str, crash_file: str) -> dict[str, Any]:
        """Debug crash using WinDbg (Windows)."""
        windbg_info = {}

        try:
            import shutil
            import subprocess
            import tempfile

            # Check if WinDbg/CDB is available on the system
            windbg_path = shutil.which("windbg.exe")
            cdb_path = shutil.which("cdb.exe")

            if not (windbg_path or cdb_path):
                # WinDbg not available
                windbg_info = {
                    "platform": "windows",
                    "debugger": "windbg",
                    "status": "not_available",
                    "reason": "WinDbg/CDB not found in PATH",
                    "crash_file_analyzed": crash_file,
                    "target_command": target_command.split()[0] if target_command else "unknown",
                }
                self.logger.warning("WinDbg/CDB not available for crash analysis")
                return windbg_info

            # Use CDB (console debugger) as it's easier to automate
            debugger_path = cdb_path if cdb_path else windbg_path
            debugger_name = "cdb" if cdb_path else "windbg"

            # Create temporary script file for debugger commands
            with tempfile.NamedTemporaryFile(mode="w", suffix=".txt", delete=False) as script_file:
                # Write comprehensive debugging commands
                script_file.write("* Crash Analysis Script\n")
                script_file.write(".reload\n")  # Reload symbols
                script_file.write(".excr\n")  # Display exception record
                script_file.write("r\n")  # Display registers
                script_file.write("k 50\n")  # Display call stack (50 frames)
                script_file.write("lm\n")  # List loaded modules
                script_file.write("!analyze -v\n")  # Automated crash analysis
                script_file.write("!address @$csp\n")  # Address info at stack pointer
                script_file.write("!address @$ip\n")  # Address info at instruction pointer
                script_file.write("u @$ip-10 @$ip+10\n")  # Disassembly around crash
                script_file.write("dps @$csp-40 @$csp+40\n")  # Stack dump with symbols
                script_file.write("!heap -s\n")  # Heap summary
                script_file.write("!peb\n")  # Process Environment Block
                script_file.write("!teb\n")  # Thread Environment Block
                script_file.write(".lastevent\n")  # Last event info
                script_file.write("q\n")  # Quit
                script_file.flush()
                script_path = script_file.name

            # Prepare debugger command
            cmd_parts = target_command.split()
            if crash_file and os.path.exists(crash_file):
                # Add crash file as argument to target
                cmd_parts.append(crash_file)

            # Build CDB command line
            debugger_cmd = [
                debugger_path,
                "-cf",
                script_path,  # Command file
                "-logo",
                tempfile.NamedTemporaryFile(suffix="_crash.log", delete=False).name,  # Log output
                "-y",
                "srv*c:\\symbols*http://msdl.microsoft.com/download/symbols",  # Symbol path
                "-c",
                "g;q",  # Go and quit on completion
            ]
            debugger_cmd.extend(cmd_parts)

            # Run debugger with timeout
            try:
                result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                    debugger_cmd,
                    capture_output=True,
                    text=True,
                    timeout=30,  # 30 second timeout
                    check=False,
                    shell=False,  # Explicitly secure - using list format prevents shell injection
                )

                # Parse the output
                output = result.stdout + result.stderr
                windbg_info = self._parse_windbg_output(output)
                windbg_info.update(
                    {
                        "platform": "windows",
                        "debugger": debugger_name,
                        "debugger_path": debugger_path,
                        "status": "success",
                        "crash_file_analyzed": crash_file,
                        "target_command": cmd_parts[0] if cmd_parts else "unknown",
                        "return_code": result.returncode,
                    }
                )

                self.logger.info(f"Successfully analyzed crash with {debugger_name}")

            except subprocess.TimeoutExpired:
                windbg_info = {
                    "platform": "windows",
                    "debugger": debugger_name,
                    "status": "timeout",
                    "crash_file_analyzed": crash_file,
                    "target_command": cmd_parts[0] if cmd_parts else "unknown",
                }
                self.logger.warning("Debugger analysis timed out")

            finally:
                # Clean up temporary script file
                try:
                    os.unlink(script_path)
                except OSError as e:
                    logger.debug("Could not remove script file %s: %s", script_path, e)

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"WinDbg debugging failed: {e}")
            windbg_info = {
                "platform": "windows",
                "debugger": "windbg",
                "status": "error",
                "error": str(e),
                "crash_file_analyzed": crash_file,
            }

        return windbg_info

    def _parse_windbg_output(self, output: str) -> dict[str, Any]:
        """Parse WinDbg/CDB debugging output."""
        import re

        parsed_info = {
            "registers": {},
            "stack_trace": [],
            "memory_maps": [],
            "crash_address": None,
            "crash_instruction": None,
            "exception_code": None,
            "exception_type": None,
            "fault_type": None,
            "loaded_modules": [],
            "heap_info": {},
            "exploitability": "unknown",
        }

        lines = output.split("\n")

        # Parse exception information
        for i, line in enumerate(lines):
            # Exception code and type
            if "ExceptionCode:" in line:
                match = re.search(r"ExceptionCode:\s*([0-9a-fA-F]+)\s*\((.*?)\)", line)
                if match:
                    parsed_info["exception_code"] = match.group(1)
                    parsed_info["exception_type"] = match.group(2)

            # Exception address
            if "ExceptionAddress:" in line:
                match = re.search(r"ExceptionAddress:\s*([0-9a-fA-F]+)", line)
                if match:
                    parsed_info["crash_address"] = match.group(1)

            # Registers (looking for register dump)
            if line.startswith("eax=") or line.startswith("rax="):
                # x86 or x64 register dump
                reg_matches = re.findall(r"([er][a-z]{2})=([0-9a-fA-F]+)", line)
                for reg_name, reg_value in reg_matches:
                    parsed_info["registers"][reg_name] = reg_value

            # Stack trace (looking for frame information)
            if re.match(r"^\d+\s+[0-9a-fA-F]+\s+[0-9a-fA-F]+", line):
                # Frame format: ## address return_address module!function
                parts = line.split(None, 3)
                if len(parts) >= 3:
                    frame = {
                        "frame_num": parts[0],
                        "stack_addr": parts[1],
                        "return_addr": parts[2],
                        "symbol": parts[3] if len(parts) > 3 else "unknown",
                    }
                    parsed_info["stack_trace"].append(frame)

            # Loaded modules
            if re.match(r"^[0-9a-fA-F]+\s+[0-9a-fA-F]+\s+\w+", line):
                # Module format: start end module_name
                parts = line.split(None, 2)
                if len(parts) >= 3 and not parts[2].startswith("0x"):
                    module = {"start": parts[0], "end": parts[1], "name": parts[2].split()[0] if parts[2] else "unknown"}
                    parsed_info["loaded_modules"].append(module)

            # Exploitability assessment
            if "Exploitability Classification:" in line:
                match = re.search(r"Exploitability Classification:\s*(\w+)", line)
                if match:
                    parsed_info["exploitability"] = match.group(1)

            # Fault type
            if "DEFAULT_BUCKET_ID:" in line:
                match = re.search(r"DEFAULT_BUCKET_ID:\s*(\w+)", line)
                if match:
                    parsed_info["fault_type"] = match.group(1)

            # Crash instruction
            if "FAULTING_IP:" in line:
                if i + 1 < len(lines):
                    parsed_info["crash_instruction"] = lines[i + 1].strip()

            # Heap corruption detection
            if "heap corruption" in line.lower() or "heap error" in line.lower():
                parsed_info["heap_info"]["corruption_detected"] = True
                parsed_info["heap_info"]["corruption_details"] = line.strip()

        # Determine crash severity
        parsed_info["severity"] = self._assess_crash_severity(parsed_info)

        return parsed_info

    def _assess_crash_severity(self, crash_info: dict[str, Any]) -> str:
        """Assess the severity of a crash based on parsed information."""
        exception_code = crash_info.get("exception_code", "")
        exploitability = crash_info.get("exploitability", "unknown").lower()
        fault_type = crash_info.get("fault_type", "").lower()

        # Critical severity indicators
        if exploitability in ["exploitable", "probably_exploitable"]:
            return "critical"

        if exception_code in ["c0000005", "C0000005"]:  # Access violation
            if "write" in fault_type or "heap" in fault_type:
                return "high"
            return "medium"

        if exception_code in ["c0000094", "C0000094"]:  # Integer divide by zero
            return "low"

        if exception_code in ["c00000fd", "C00000FD"]:  # Stack overflow
            return "medium"

        if "heap_corruption" in crash_info.get("heap_info", {}):
            return "high"

        # Default severity
        return "medium" if crash_info.get("crash_address") else "low"

    def _parse_gdb_output(self, gdb_output: str) -> dict[str, Any]:
        """Parse GDB debugging output."""
        parsed_info = {
            "registers": {},
            "stack_trace": [],
            "memory_maps": [],
            "crash_address": None,
            "crash_instruction": None,
            "loaded_modules": [],
            "signal_info": {},
            "parsing_stats": {
                "total_lines": 0,
                "parsed_registers": 0,
                "parsed_frames": 0,
                "parsed_maps": 0,
            },
        }

        lines = gdb_output.split("\n")
        current_section = None
        parsed_info["parsing_stats"]["total_lines"] = len(lines)

        for line in lines:
            line = line.strip()

            # Track current section for context-aware parsing
            if line.startswith("(gdb)") or line.startswith("Breakpoint"):
                current_section = "gdb_prompt"
            elif "registers" in line.lower():
                current_section = "registers"
            elif line.startswith("#") and current_section != "disassembly":
                current_section = "stack_trace"
            elif "mapped" in line or "Start Addr" in line:
                current_section = "memory_maps"

            # Parse registers
            if line.startswith("rax") or line.startswith("eax") or (line.startswith("r") and "x" in line[:5]):
                reg_match = re.search(r"(\w+)\s+0x([0-9a-fA-F]+)", line)
                if reg_match:
                    reg_name, reg_value = reg_match.groups()
                    try:
                        parsed_info["registers"][reg_name] = int(reg_value, 16)
                        parsed_info["parsing_stats"]["parsed_registers"] += 1

                        # Check for interesting register values
                        reg_val = int(reg_value, 16)
                        if reg_val == 0x41414141:  # 'AAAA' - controlled value
                            parsed_info["crash_indicators"] = parsed_info.get("crash_indicators", [])
                            parsed_info["crash_indicators"].append(f"controlled_{reg_name}")

                    except ValueError as e:
                        logger.error("Value error in fuzzing_engine: %s", e)
                        continue

            # Parse stack trace
            elif line.startswith("#"):
                frame_match = re.search(r"#(\d+)\s+0x([0-9a-fA-F]+)\s+in\s+(.+)", line)
                if frame_match:
                    frame_num, addr, func = frame_match.groups()
                    parsed_info["stack_trace"].append(
                        {
                            "frame": int(frame_num),
                            "address": int(addr, 16),
                            "function": func,
                            "section": current_section,
                        }
                    )
                    parsed_info["parsing_stats"]["parsed_frames"] += 1

            # Parse memory mappings
            elif "mapped" in line and "0x" in line:
                map_match = re.search(
                    r"0x([0-9a-fA-F]+)\s+0x([0-9a-fA-F]+)\s+0x([0-9a-fA-F]+)\s+0x([0-9a-fA-F]+)\s+(.+)",
                    line,
                )
                if map_match:
                    start, end, size, offset, objfile = map_match.groups()
                    memory_map = {
                        "start": int(start, 16),
                        "end": int(end, 16),
                        "size": int(size, 16),
                        "offset": int(offset, 16),
                        "objfile": objfile,
                        "section": current_section,
                    }
                    parsed_info["memory_maps"].append(memory_map)
                    parsed_info["parsing_stats"]["parsed_maps"] += 1

                    # Check for loaded modules
                    if objfile and objfile not in ["/dev/zero", "[stack]", "[heap]"]:
                        if objfile not in parsed_info["loaded_modules"]:
                            parsed_info["loaded_modules"].append(objfile)

            # Parse crash address and signal information
            elif "Program received signal" in line:
                signal_match = re.search(r"signal\s+(\w+),\s+(.+)", line)
                if signal_match:
                    signal_name, signal_desc = signal_match.groups()
                    parsed_info["signal_info"] = {
                        "signal": signal_name,
                        "description": signal_desc,
                        "line_context": current_section,
                    }

            # Parse crash instruction
            elif "=>" in line and "0x" in line:
                # This indicates the current instruction pointer
                instr_match = re.search(r"=>\s*0x([0-9a-fA-F]+)[:\s]+(.+)", line)
                if instr_match:
                    crash_addr, instruction = instr_match.groups()
                    parsed_info["crash_address"] = int(crash_addr, 16)
                    parsed_info["crash_instruction"] = instruction.strip()

        # Log parsing results
        stats = parsed_info["parsing_stats"]
        self.logger.debug(
            f"GDB output parsed: {stats['parsed_registers']} registers, "
            f"{stats['parsed_frames']} stack frames, {stats['parsed_maps']} memory maps "
            f"from {stats['total_lines']} lines"
        )

        return parsed_info

    def _analyze_crash_pattern(self, crash_file: str) -> dict[str, Any]:
        """Analyze crash based on input patterns."""
        pattern_info = {
            "input_analysis": {},
            "crash_indicators": [],
            "pattern_type": "unknown",
        }

        try:
            with open(crash_file, "rb") as f:
                crash_data = f.read()

            # Analyze input patterns
            pattern_info["input_analysis"] = {
                "size": len(crash_data),
                "has_nulls": b"\x00" in crash_data,
                "has_format_strings": any(fs in crash_data for fs in [b"%s", b"%n", b"%x"]),
                "has_long_strings": any(len(chunk) > 100 for chunk in crash_data.split(b"\x00")),
                "entropy": self._calculate_data_entropy(crash_data),
                "repeated_patterns": self._find_repeated_patterns(crash_data),
            }

            # Determine likely crash type
            if b"A" * 50 in crash_data or b"B" * 50 in crash_data:
                pattern_info["pattern_type"] = "buffer_overflow"
                pattern_info["crash_indicators"].append("long_repeated_chars")

            if any(fs in crash_data for fs in [b"%s", b"%n", b"%x"]):
                pattern_info["pattern_type"] = "format_string"
                pattern_info["crash_indicators"].append("format_specifiers")

            if len(crash_data) > 10000:
                pattern_info["crash_indicators"].append("large_input")

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.debug(f"Crash pattern analysis failed: {e}")

        return pattern_info

    def _calculate_data_entropy(self, data: bytes) -> float:
        """Calculate entropy of crash data."""
        from ...utils.analysis.entropy_utils import safe_entropy_calculation

        return safe_entropy_calculation(data, max_entropy=8.0)  # Cap at 8 bits for fuzzing analysis

    def _find_repeated_patterns(self, data: bytes) -> list[dict[str, Any]]:
        """Find repeated patterns in crash data."""
        patterns = []

        # Look for repeated byte sequences
        for pattern_len in [1, 2, 4, 8]:
            if len(data) < pattern_len * 10:
                continue

            for i in range(len(data) - pattern_len * 5):
                pattern = data[i : i + pattern_len]
                count = 0
                pos = i

                while pos < len(data) - pattern_len:
                    if data[pos : pos + pattern_len] == pattern:
                        count += 1
                        pos += pattern_len
                    else:
                        pos += 1

                if count >= 10:  # Found significant repetition
                    patterns.append(
                        {
                            "pattern": pattern.hex(),
                            "length": pattern_len,
                            "count": count,
                            "start_offset": i,
                        }
                    )
                    break

        return patterns[:5]  # Return top 5 patterns

    def _analyze_crash_info(self, debug_info: dict[str, Any]) -> dict[str, Any]:
        """Analyze crash debug information."""
        analysis = {
            "crash_type": "unknown",
            "severity": CrashSeverity.MEDIUM.value,
            "root_cause": "unknown",
            "crash_details": {},
            "vulnerability_indicators": [],
            "exploitation_complexity": "unknown",
            "affected_components": [],
            "security_impact": {},
        }

        try:
            # Analyze signal information
            signal_info = debug_info.get("signal_info", {})
            if signal_info:
                analysis.update(self._analyze_signal_crash(signal_info))

            # Analyze register state
            registers = debug_info.get("registers", {})
            if registers:
                analysis.update(self._analyze_register_state(registers))

            # Analyze stack trace
            stack_trace = debug_info.get("stack_trace", [])
            if stack_trace:
                analysis.update(self._analyze_stack_trace(stack_trace))

            # Analyze memory mappings
            memory_maps = debug_info.get("memory_maps", [])
            if memory_maps:
                analysis.update(self._analyze_memory_layout(memory_maps))

            # Analyze input patterns
            input_analysis = debug_info.get("input_analysis", {})
            if input_analysis:
                analysis.update(self._analyze_input_patterns(input_analysis))

            # Determine overall severity
            analysis["severity"] = self._calculate_crash_severity(analysis)

            # Generate security impact assessment
            analysis["security_impact"] = self._assess_security_impact(analysis)

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.warning(f"Crash analysis failed: {e}")

        return analysis

    def _analyze_signal_crash(self, signal_info: dict[str, Any]) -> dict[str, Any]:
        """Analyze crash based on signal information."""
        signal_analysis = {
            "crash_type": "unknown",
            "vulnerability_indicators": [],
            "crash_details": {},
        }

        signal_name = signal_info.get("signal", "").upper()

        if signal_name == "SIGSEGV":
            signal_analysis.update(
                {
                    "crash_type": "segmentation_fault",
                    "root_cause": "memory_corruption",
                    "vulnerability_indicators": [
                        "memory_access_violation",
                        "potential_buffer_overflow",
                    ],
                    "crash_details": {
                        "fault_type": "memory_access_violation",
                        "description": "Invalid memory access detected",
                    },
                }
            )

        elif signal_name == "SIGABRT":
            signal_analysis.update(
                {
                    "crash_type": "abort_signal",
                    "root_cause": "assertion_failure",
                    "vulnerability_indicators": ["heap_corruption", "double_free"],
                    "crash_details": {
                        "fault_type": "program_abort",
                        "description": "Program aborted, possibly due to heap corruption",
                    },
                }
            )

        elif signal_name == "SIGILL":
            signal_analysis.update(
                {
                    "crash_type": "illegal_instruction",
                    "root_cause": "code_corruption",
                    "vulnerability_indicators": ["code_injection", "rop_chain"],
                    "crash_details": {
                        "fault_type": "illegal_instruction",
                        "description": "Illegal instruction executed",
                    },
                }
            )

        elif signal_name == "SIGFPE":
            signal_analysis.update(
                {
                    "crash_type": "floating_point_exception",
                    "root_cause": "arithmetic_error",
                    "vulnerability_indicators": ["division_by_zero", "integer_overflow"],
                    "crash_details": {
                        "fault_type": "arithmetic_exception",
                        "description": "Floating point arithmetic error",
                    },
                }
            )

        return signal_analysis

    def _analyze_register_state(self, registers: dict[str, Any]) -> dict[str, Any]:
        """Analyze crash based on register state."""
        register_analysis = {
            "vulnerability_indicators": [],
            "crash_details": {},
            "exploitation_complexity": "medium",
        }

        # Check for common exploitation patterns
        for reg_name, reg_value in registers.items():
            if isinstance(reg_value, int):
                # Check for controlled register values (common in exploits)
                if reg_value == 0x41414141:  # 'AAAA'
                    register_analysis["vulnerability_indicators"].append("controlled_eip")
                    register_analysis["exploitation_complexity"] = "low"
                    register_analysis["crash_details"]["controlled_registers"] = [reg_name]

                elif reg_value == 0x42424242:  # 'BBBB'
                    register_analysis["vulnerability_indicators"].append("controlled_register")
                    register_analysis["exploitation_complexity"] = "low"

                elif reg_value & 0xFFFF0000 == 0x41410000:  # Partial control
                    register_analysis["vulnerability_indicators"].append("partial_register_control")
                    register_analysis["exploitation_complexity"] = "medium"

                # Check for NULL dereference
                elif reg_value == 0x0:
                    register_analysis["vulnerability_indicators"].append("null_pointer_dereference")

                # Check for stack addresses (potential stack overflow)
                elif 0x7FFF00000000 <= reg_value <= 0x7FFFFFFFFFFF:  # x64 stack range
                    register_analysis["vulnerability_indicators"].append("stack_address_in_register")

        return register_analysis

    def _analyze_stack_trace(self, stack_trace: list[dict[str, Any]]) -> dict[str, Any]:
        """Analyze crash based on stack trace."""
        stack_analysis = {
            "affected_components": [],
            "vulnerability_indicators": [],
            "crash_details": {},
        }

        if not stack_trace:
            return stack_analysis

        # Analyze function names in stack trace
        functions = [frame.get("function", "") for frame in stack_trace]

        # Look for vulnerable function patterns
        vulnerable_functions = [
            "strcpy",
            "strcat",
            "sprintf",
            "gets",
            "scanf",
            "memcpy",
            "memmove",
            "strncpy",
            "strncat",
        ]

        for func in functions:
            for vuln_func in vulnerable_functions:
                if vuln_func in func:
                    stack_analysis["vulnerability_indicators"].append(f"vulnerable_function_{vuln_func}")
                    stack_analysis["affected_components"].append(func)

        # Check for recursive calls (potential stack overflow)
        func_counts = {}
        for func in functions:
            func_counts[func] = func_counts.get(func, 0) + 1

        for func, count in func_counts.items():
            if count > 10:  # Likely recursion
                stack_analysis["vulnerability_indicators"].append("stack_overflow_recursion")
                stack_analysis["crash_details"]["recursive_function"] = func

        # Analyze stack depth
        if len(stack_trace) > 50:
            stack_analysis["vulnerability_indicators"].append("deep_stack_trace")

        return stack_analysis

    def _analyze_memory_layout(self, memory_maps: list[dict[str, Any]]) -> dict[str, Any]:
        """Analyze crash based on memory layout."""
        memory_analysis = {
            "vulnerability_indicators": [],
            "crash_details": {},
            "security_mitigations": [],
        }

        for mem_map in memory_maps:
            objfile = mem_map.get("objfile", "")
            start = mem_map.get("start", 0)

            # Check for ASLR
            if "libc" in objfile and start < 0x7F0000000000:
                memory_analysis["security_mitigations"].append("aslr_disabled")
            elif "libc" in objfile:
                memory_analysis["security_mitigations"].append("aslr_enabled")

            # Check for executable stack
            if "[stack]" in objfile and "x" in str(mem_map):
                memory_analysis["vulnerability_indicators"].append("executable_stack")

            # Check for RWX mappings
            if all(perm in str(mem_map) for perm in ["r", "w", "x"]):
                memory_analysis["vulnerability_indicators"].append("rwx_mapping")

        return memory_analysis

    def _analyze_input_patterns(self, input_analysis: dict[str, Any]) -> dict[str, Any]:
        """Analyze crash based on input patterns."""
        pattern_analysis = {
            "root_cause": "unknown",
            "vulnerability_indicators": [],
            "exploitation_complexity": "medium",
        }

        pattern_type = input_analysis.get("pattern_type", "unknown")

        if pattern_type == "buffer_overflow":
            pattern_analysis.update(
                {
                    "root_cause": "buffer_overflow",
                    "vulnerability_indicators": ["buffer_boundary_violation"],
                    "exploitation_complexity": "low",
                }
            )

        elif pattern_type == "format_string":
            pattern_analysis.update(
                {
                    "root_cause": "format_string_vulnerability",
                    "vulnerability_indicators": ["format_string_injection"],
                    "exploitation_complexity": "medium",
                }
            )

        # Analyze entropy
        entropy = input_analysis.get("entropy", 0)
        if entropy > 7.0:
            pattern_analysis["vulnerability_indicators"].append("high_entropy_input")
        elif entropy < 1.0:
            pattern_analysis["vulnerability_indicators"].append("low_entropy_pattern")

        # Analyze size
        size = input_analysis.get("size", 0)
        if size > 100000:
            pattern_analysis["vulnerability_indicators"].append("extremely_large_input")
        elif size > 10000:
            pattern_analysis["vulnerability_indicators"].append("large_input")

        return pattern_analysis

    def _calculate_crash_severity(self, analysis: dict[str, Any]) -> str:
        """Calculate overall crash severity."""
        severity_score = 0

        # Base score from crash type
        crash_type = analysis.get("crash_type", "")
        if crash_type in ["segmentation_fault", "illegal_instruction"]:
            severity_score += 3
        elif crash_type in ["abort_signal", "floating_point_exception"]:
            severity_score += 2
        else:
            severity_score += 1

        # Add score for vulnerability indicators
        indicators = analysis.get("vulnerability_indicators", [])
        if "controlled_eip" in indicators:
            severity_score += 3
        if "controlled_register" in indicators:
            severity_score += 2
        if "executable_stack" in indicators:
            severity_score += 2
        if "rwx_mapping" in indicators:
            severity_score += 2

        # Reduce score for mitigations
        mitigations = analysis.get("security_mitigations", [])
        if "aslr_enabled" in mitigations:
            severity_score -= 1

        # Determine severity level
        if severity_score >= 6:
            return CrashSeverity.CRITICAL.value
        if severity_score >= 4:
            return CrashSeverity.HIGH.value
        if severity_score >= 2:
            return CrashSeverity.MEDIUM.value
        return CrashSeverity.LOW.value

    def _assess_security_impact(self, analysis: dict[str, Any]) -> dict[str, Any]:
        """Assess security impact of crash."""
        impact = {
            "confidentiality": "none",
            "integrity": "none",
            "availability": "low",
            "attack_vector": "local",
            "privileges_required": "none",
            "user_interaction": "required",
        }

        # Determine impact based on vulnerability indicators
        indicators = analysis.get("vulnerability_indicators", [])

        if "controlled_eip" in indicators or "code_injection" in indicators:
            impact.update(
                {
                    "confidentiality": "high",
                    "integrity": "high",
                    "availability": "high",
                    "attack_vector": "network",
                    "privileges_required": "none",
                }
            )

        elif "controlled_register" in indicators or "memory_corruption" in analysis.get("root_cause", ""):
            impact.update(
                {
                    "confidentiality": "partial",
                    "integrity": "partial",
                    "availability": "high",
                }
            )

        return impact

    def _score_vulnerability_indicators(self, indicators: list[str]) -> int:
        """Score vulnerability indicators for exploitability."""
        score = 0

        # High exploitability indicators
        high_indicators = {"controlled_eip": 5, "controlled_register": 3, "code_injection": 4, "executable_stack": 3, "rwx_mapping": 3}

        # Medium exploitability indicators
        medium_indicators = {
            "buffer_boundary_violation": 2,
            "format_string_injection": 2,
            "heap_corruption": 2,
            "partial_register_control": 1,
        }

        # Low exploitability indicators
        low_indicators = {"null_pointer_dereference": 1, "division_by_zero": 1}

        for indicator in indicators:
            if indicator in high_indicators:
                score += high_indicators[indicator]
            elif indicator in medium_indicators:
                score += medium_indicators[indicator]
            elif indicator in low_indicators:
                score += low_indicators[indicator]

        return score

    def _score_complexity_and_mitigations(self, crash_analysis: dict[str, Any]) -> int:
        """Score complexity factors and security mitigations."""
        score = 0

        # Check for exploitation complexity factors
        complexity = crash_analysis.get("exploitation_complexity", "medium")
        if complexity == "low":
            score += 2
        elif complexity == "high":
            score -= 1

        # Check for security mitigations
        mitigations = crash_analysis.get("security_mitigations", [])
        mitigation_penalties = {"aslr_enabled": -2, "dep_enabled": -2, "stack_canary": -1, "cfi_enabled": -2}

        for mitigation in mitigations:
            if mitigation in mitigation_penalties:
                score += mitigation_penalties[mitigation]

        return score

    def _score_crash_type(self, crash_type: str) -> int:
        """Score crash type for exploitability."""
        crash_scores = {"segmentation_fault": 2, "illegal_instruction": 3, "abort_signal": 1}
        return crash_scores.get(crash_type, 0)

    def _analyze_register_control(self, registers: dict[str, Any]) -> int:
        """Analyze register state for control patterns."""
        score = 0

        for reg_name, reg_value in registers.items():
            if isinstance(reg_value, int):
                # Check for user-controlled values
                if reg_value in [0x41414141, 0x42424242, 0x43434343]:
                    score += 2
                    logger.debug(f"Found controlled register {reg_name}: {hex(reg_value)}")
                # Check for partial control patterns
                elif (reg_value & 0xFFFF0000) in [0x41410000, 0x42420000]:
                    score += 1
                    logger.debug(f"Found partial control in register {reg_name}: {hex(reg_value)}")

        return score

    def _analyze_input_patterns(self, input_analysis: dict[str, Any]) -> int:
        """Analyze input patterns for exploitation clues."""
        score = 0

        # Repeated patterns suggest controlled input
        patterns = input_analysis.get("repeated_patterns", [])
        if patterns:
            score += 1

        # Large inputs might indicate buffer overflow
        size = input_analysis.get("size", 0)
        if size > 10000:
            score += 1

        return score

    def _determine_exploitability_level(self, score: int) -> str:
        """Determine final exploitability assessment from score."""
        if score >= 8:
            return "highly_exploitable"
        if score >= 5:
            return "likely_exploitable"
        if score >= 3:
            return "potentially_exploitable"
        if score >= 1:
            return "probably_not_exploitable"
        return "not_exploitable"

    def _assess_exploitability(self, debug_info: dict[str, Any], crash_analysis: dict[str, Any]) -> str:
        """Assess crash exploitability."""
        try:
            exploitability_score = 0

            # Score vulnerability indicators
            indicators = crash_analysis.get("vulnerability_indicators", [])
            exploitability_score += self._score_vulnerability_indicators(indicators)

            # Score complexity and mitigations
            exploitability_score += self._score_complexity_and_mitigations(crash_analysis)

            # Score crash type
            crash_type = crash_analysis.get("crash_type", "")
            exploitability_score += self._score_crash_type(crash_type)

            # Analyze register control
            registers = debug_info.get("registers", {})
            if registers:
                exploitability_score += self._analyze_register_control(registers)

            # Analyze input patterns
            input_analysis = debug_info.get("input_analysis", {})
            if input_analysis:
                exploitability_score += self._analyze_input_patterns(input_analysis)

            # Determine final exploitability assessment
            return self._determine_exploitability_level(exploitability_score)

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.warning(f"Exploitability assessment failed: {e}")
            return "unknown_exploitability"

    def _generate_crash_recommendations(self, crash_analysis: dict[str, Any]) -> list[str]:
        """Generate actionable recommendations based on comprehensive crash analysis."""
        crash_type = crash_analysis.get("crash_type", "unknown")
        vulnerability_indicators = crash_analysis.get("vulnerability_indicators", [])
        severity = crash_analysis.get("severity", "medium")
        exploitation_complexity = crash_analysis.get("exploitation_complexity", "unknown")
        registers = crash_analysis.get("registers", {})
        stack_trace = crash_analysis.get("stack_trace", [])
        security_mitigations = crash_analysis.get("security_mitigations", [])
        root_cause = crash_analysis.get("root_cause", "unknown")
        crash_address = crash_analysis.get("crash_address", None)

        self.logger.debug(f"Generating recommendations for crash type: {crash_type} with {len(vulnerability_indicators)} indicators")

        recommendations = []

        # Priority 1: Immediate exploitation recommendations
        if "controlled_eip" in vulnerability_indicators or "controlled_rip" in vulnerability_indicators:
            recommendations.append(" CRITICAL: Direct EIP/RIP control achieved - immediate code execution possible")
            recommendations.append(" Develop ROP chain using ROPgadget or ropper on the binary")
            recommendations.append(" If DEP enabled, chain gadgets to mprotect() or VirtualProtect() for shellcode execution")
            recommendations.append(" Calculate exact offset to EIP/RIP overwrite using pattern_create/pattern_offset")

            # Check for useful register control
            controlled_regs = [reg for reg in registers if registers[reg] == 0x41414141]
            if controlled_regs:
                recommendations.append(f" Controlled registers: {', '.join(controlled_regs)} - use for parameter passing in exploitation")

        # Priority 2: Memory corruption exploitation paths
        if "buffer_overflow" in root_cause or "stack_overflow" in vulnerability_indicators:
            recommendations.append(" Stack-based buffer overflow detected")

            if "aslr_disabled" in security_mitigations or not security_mitigations:
                recommendations.append(" ASLR disabled/unknown - use hardcoded addresses for exploitation")
                recommendations.append(" Find JMP ESP gadget: msfpescan -j esp [binary]")
            else:
                recommendations.append(" ASLR enabled - leak stack/libc address first or use partial overwrite")
                recommendations.append(" Bruteforce ASLR on 32-bit: ~2^8 to 2^16 attempts needed")

            if "executable_stack" in vulnerability_indicators:
                recommendations.append(" Stack is executable! Direct shellcode execution possible")
                recommendations.append(" Generate position-independent shellcode: msfvenom -p linux/x86/shell_reverse_tcp")
            else:
                recommendations.append(" NX/DEP enabled - use ret2libc or ROP chain")
                recommendations.append(" Find system() address: objdump -T libc.so.6 | grep system")

            recommendations.append(f" Crash at 0x{crash_address:08x} - analyze surrounding code for gadgets")

        elif "heap_overflow" in root_cause or "heap_corruption" in vulnerability_indicators:
            recommendations.append(" Heap-based vulnerability detected")
            recommendations.append(" Identify heap allocator: glibc ptmalloc2, jemalloc, or tcmalloc")
            recommendations.append(" For ptmalloc2: target unsafe unlink, house of spirit, or tcache poisoning")
            recommendations.append(" Heap spray for reliability: allocate predictable memory layout")
            recommendations.append(" Use heap feng shui to position controlled data adjacent to target")

            if "double_free" in vulnerability_indicators:
                recommendations.append(" Double-free detected: exploit via tcache dup or fastbin dup")
                recommendations.append(" Allocate controlled chunk at arbitrary address")

        elif "use_after_free" in root_cause or "uaf" in vulnerability_indicators:
            recommendations.append(" Use-After-Free vulnerability detected")
            recommendations.append(" Identify freed object size and type")
            recommendations.append(" Spray heap with controlled objects of same size")
            recommendations.append(" Overwrite vtable pointer for control flow hijack")
            recommendations.append(" Use GDB: watch *(void**)0xaddress to track object lifecycle")

        # Priority 3: Format string specific
        if "format_string" in root_cause or "%n" in str(crash_analysis.get("input_analysis", {})):
            recommendations.append(" Format string vulnerability confirmed")
            recommendations.append(" Use %x to leak stack/libc addresses")
            recommendations.append(" Calculate offset to format string on stack")
            recommendations.append(" Overwrite GOT entry: printf@got  system")
            recommendations.append(" Use %hn for 2-byte writes to bypass small buffer constraints")
            recommendations.append(" Chain multiple %n writes for arbitrary memory modification")

        # Priority 4: Integer overflow exploitation
        if "integer_overflow" in root_cause or "integer_overflow" in vulnerability_indicators:
            recommendations.append(" Integer overflow/underflow detected")
            recommendations.append(" Trigger allocation of undersized buffer")
            recommendations.append(" Overflow into adjacent heap metadata or objects")
            recommendations.append(" Look for multiplication in size calculations")
            recommendations.append(" Test boundary values: 0xFFFFFFFF, 0x7FFFFFFF, 0x80000000")

        # Priority 5: Debugger-specific recommendations
        if stack_trace:
            vulnerable_functions = []
            for frame in stack_trace[:5]:  # Check top 5 frames
                func_name = frame.get("function", "")
                if any(vuln in func_name.lower() for vuln in ["strcpy", "strcat", "gets", "sprintf", "scanf"]):
                    vulnerable_functions.append(func_name)

            if vulnerable_functions:
                recommendations.append(f" Vulnerable functions in stack: {', '.join(vulnerable_functions)}")
                recommendations.append(" These functions lack bounds checking - prime overflow targets")

        # Priority 6: Exploitation tooling recommendations
        if severity in ["critical", "high"]:
            recommendations.append("\n Recommended exploitation tools:")
            recommendations.append(" pwntools: Rapid exploit development framework")
            recommendations.append(" ROPgadget/ropper: ROP chain generation")
            recommendations.append(" one_gadget: Find execve('/bin/sh') gadgets in libc")
            recommendations.append(" voltron: Enhanced GDB interface for exploit dev")
            recommendations.append(" PEDA/pwndbg/GEF: GDB enhancement for exploitation")

        # Priority 7: Mitigation bypass strategies
        if security_mitigations:
            recommendations.append("\n Mitigation bypass strategies:")

            if "pie_enabled" in security_mitigations:
                recommendations.append(" PIE enabled: Leak .text address first via format string or info leak")
                recommendations.append(" Use partial overwrites to maintain upper bytes")

            if "fortify_source" in security_mitigations:
                recommendations.append(" FORTIFY_SOURCE: Target non-fortified functions or use size confusion")

            if "stack_canary" in security_mitigations:
                recommendations.append(" Stack canary present: Leak canary value first or target non-stack vectors")
                recommendations.append(" Bruteforce canary byte-by-byte in forking servers")

        # Priority 8: Next steps for complex exploits
        if exploitation_complexity == "high":
            recommendations.append("\n Complex exploit - recommended approach:")
            recommendations.append(" Step 1: Achieve reliable crash reproduction")
            recommendations.append(" Step 2: Leak memory addresses (stack/heap/libc)")
            recommendations.append(" Step 3: Bypass mitigations systematically")
            recommendations.append(" Step 4: Gain code execution via ROP/ret2libc")
            recommendations.append(" Step 5: Stabilize exploit for different environments")

        # Priority 9: Additional forensics
        if crash_address:
            recommendations.append(f"\n Crash forensics for address 0x{crash_address:08x}:")
            recommendations.append(f" Disassemble: x/10i 0x{crash_address:08x}")
            recommendations.append(" Check mappings: info proc mappings")
            recommendations.append(" Examine memory: x/100wx $rsp-0x50")
            recommendations.append(f" Find references: grep -r '{crash_address:08x}' /proc/[pid]/maps")

        return recommendations

        # Recommendations based on vulnerability indicators
        if "controlled_eip" in vulnerability_indicators:
            recommendations.extend(
                [
                    "CRITICAL: Implement control flow integrity (CFI)",
                    "Enable stack canaries and ASLR",
                    "Review return address protection",
                ]
            )

        if "executable_stack" in vulnerability_indicators:
            recommendations.extend(
                [
                    "Enable DEP/NX bit protection",
                    "Remove executable permissions from stack",
                    "Use non-executable memory mappings",
                ]
            )

        if "rwx_mapping" in vulnerability_indicators:
            recommendations.extend(
                [
                    "Eliminate Read-Write-Execute memory mappings",
                    "Use W^X (Write XOR Execute) protection",
                    "Review JIT compiler security",
                ]
            )

        # Recommendations based on severity
        if severity in ["critical", "high"]:
            recommendations.extend(
                [
                    "URGENT: Prioritize immediate patching",
                    "Consider disabling affected functionality",
                    "Implement additional security monitoring",
                ]
            )

        # Recommendations based on exploitation complexity
        if exploitation_complexity == "low":
            recommendations.extend(
                [
                    "IMMEDIATE ACTION: This vulnerability is easily exploitable",
                    "Deploy emergency security measures",
                    "Consider temporary service isolation",
                ]
            )
        elif exploitation_complexity == "medium":
            recommendations.append("Monitor for exploitation attempts")

        # Security mitigation recommendations
        general_mitigations = [
            "Enable Address Space Layout Randomization (ASLR)",
            "Use stack protection mechanisms",
            "Implement bounds checking for all buffer operations",
            "Enable compiler security features (-fstack-protector, -D_FORTIFY_SOURCE)",
            "Conduct regular security code reviews",
            "Use static analysis tools to detect similar issues",
        ]

        recommendations.extend(general_mitigations)

        return list(set(recommendations))  # Remove duplicates

    def _minimize_binary_search(self, data: bytes, target_command: str) -> bytes:
        """Minimize using binary search with intelligent chunk removal."""
        import hashlib
        import tempfile

        self.logger.info(f"Starting binary search minimization, original size: {len(data)}")

        # Track minimization progress
        current_data = data
        best_data = data
        iteration = 0
        max_iterations = 100

        # Cache crash results for performance
        crash_cache = {}

        def test_crash(test_data):
            """Test if input still causes crash with caching."""
            data_hash = hashlib.sha256(test_data).hexdigest()
            if data_hash in crash_cache:
                return crash_cache[data_hash]

            with tempfile.NamedTemporaryFile(delete=False, suffix=".crash") as tmp:
                tmp.write(test_data)
                tmp_path = tmp.name

            try:
                result = self._execute_target(target_command, test_data, os.path.dirname(tmp_path))
                crashed = result.get("crashed", False)
                crash_cache[data_hash] = crashed
                return crashed
            finally:
                try:
                    os.unlink(tmp_path)
                except OSError as e:
                    logger.debug("Could not remove temp file %s: %s", tmp_path, e)

        # Multi-granularity binary search
        granularities = [0.5, 0.25, 0.125, 0.0625]  # 50%, 25%, 12.5%, 6.25%

        for granularity in granularities:
            chunk_size = max(1, int(len(current_data) * granularity))
            improved = True

            while improved and iteration < max_iterations:
                improved = False
                iteration += 1

                # Try removing chunks from different positions
                for start_pos in range(0, len(current_data), chunk_size):
                    if start_pos + chunk_size > len(current_data):
                        continue

                    # Create test data with chunk removed
                    test_data = current_data[:start_pos] + current_data[start_pos + chunk_size :]

                    if len(test_data) == 0:
                        continue

                    # Test if it still crashes
                    if test_crash(test_data):
                        current_data = test_data
                        best_data = test_data
                        improved = True
                        self.logger.debug(f"Iteration {iteration}: Reduced to {len(current_data)} bytes")
                        break

        # Try byte-level minimization for final reduction
        if len(best_data) < 1000:  # Only for small inputs
            for i in range(len(best_data)):
                test_data = best_data[:i] + best_data[i + 1 :]
                if test_data and test_crash(test_data):
                    best_data = test_data
                    self.logger.debug(f"Byte-level: Reduced to {len(best_data)} bytes")

        reduction_ratio = (1 - len(best_data) / len(data)) * 100
        self.logger.info(f"Minimization complete: {len(data)} -> {len(best_data)} bytes ({reduction_ratio:.1f}% reduction)")

        return best_data

    def _minimize_delta_debugging(self, data: bytes, target_command: str) -> bytes:
        """Minimize using Zeller's delta debugging algorithm."""
        import hashlib
        import tempfile

        self.logger.info(f"Starting delta debugging minimization, original size: {len(data)}")

        # Cache for crash test results
        crash_cache = {}

        def test_crash(test_data):
            """Test if input causes crash with caching."""
            if not test_data:
                return False

            data_hash = hashlib.sha256(test_data).hexdigest()
            if data_hash in crash_cache:
                return crash_cache[data_hash]

            with tempfile.NamedTemporaryFile(delete=False, suffix=".crash") as tmp:
                tmp.write(test_data)
                tmp_path = tmp.name

            try:
                result = self._execute_target(target_command, test_data, os.path.dirname(tmp_path))
                crashed = result.get("crashed", False)
                crash_cache[data_hash] = crashed
                return crashed
            finally:
                try:
                    os.unlink(tmp_path)
                except OSError as e:
                    logger.debug("Could not remove temp file %s: %s", tmp_path, e)

        def delta_debug(chunks, n):
            """Delta debugging core algorithm."""
            if n > len(chunks):
                return chunks

            # Try removing each n-sized subset
            for i in range(0, len(chunks), n):
                # Create complement (everything except this subset)
                complement = chunks[:i] + chunks[i + n :]
                if not complement:
                    continue

                test_data = b"".join(complement)

                if test_crash(test_data):
                    # Recurse with smaller input
                    self.logger.debug(f"Delta debug: Reduced to {len(complement)} chunks")
                    return delta_debug(complement, max(n - 1, 1))

            # Try increasing granularity
            if n < len(chunks):
                return delta_debug(chunks, min(n * 2, len(chunks)))

            return chunks

        # Convert data to chunks for processing
        initial_chunk_size = max(1, len(data) // 32)  # Start with 32 chunks
        chunks = []

        for i in range(0, len(data), initial_chunk_size):
            chunk = data[i : i + initial_chunk_size]
            if chunk:
                chunks.append(chunk)

        # Run delta debugging
        minimized_chunks = delta_debug(chunks, 1)
        minimized_data = b"".join(minimized_chunks)

        # Try token-level minimization if data appears to be text
        if len(minimized_data) < 10000 and all(32 <= b < 127 or b in [9, 10, 13] for b in minimized_data[:100]):
            tokens = minimized_data.split()
            if 1 < len(tokens) < 1000:
                token_chunks = [token + b" " for token in tokens]
                minimized_tokens = delta_debug(token_chunks, 1)
                token_result = b"".join(minimized_tokens).strip()

                if test_crash(token_result):
                    minimized_data = token_result
                    self.logger.debug(f"Token-level: Reduced to {len(minimized_data)} bytes")

        reduction_ratio = (1 - len(minimized_data) / len(data)) * 100
        self.logger.info(f"Delta debugging complete: {len(data)} -> {len(minimized_data)} bytes ({reduction_ratio:.1f}% reduction)")

        return minimized_data

    def get_statistics(self) -> dict[str, Any]:
        """Get fuzzing statistics."""
        return self.stats.copy()

    def get_crashes(self) -> dict[str, Any]:
        """Get crash information."""
        return {
            "total_crashes": self.stats["crashes_found"],
            "unique_crashes": self.stats["unique_crashes"],
            "crash_details": list(self.unique_crashes.values()),
        }

    def export_results(self, output_file: str, format: str = "json") -> bool:
        """Export fuzzing results."""
        try:
            results = {
                "statistics": self.get_statistics(),
                "crashes": self.get_crashes(),
                "configuration": self.config,
            }

            if format == "json":
                import json

                with open(output_file, "w") as f:
                    json.dump(results, f, indent=2)
            else:
                with open(output_file, "w") as f:
                    f.write(str(results))

            return True

        except (OSError, ValueError, RuntimeError, KeyError, AttributeError) as e:
            self.logger.error(f"Results export failed: {e}")
            return False
