"""
Machine Learning Adaptation Engine

Implements intelligent adaptation and optimization of exploitation techniques
using machine learning models and feedback loops.
"""

import logging
import os
import time
from enum import Enum
from typing import Any, Dict, List, Optional

logger = logging.getLogger(__name__)

# Try to import ML libraries
try:
    import pandas as pd
    from sklearn.cluster import KMeans
    from sklearn.ensemble import GradientBoostingRegressor, RandomForestClassifier
    from sklearn.metrics import accuracy_score, classification_report
    from sklearn.model_selection import cross_val_score, train_test_split
    from sklearn.preprocessing import LabelEncoder, StandardScaler
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False


class AdaptationStrategy(Enum):
    """ML adaptation strategies"""
    EXPLOIT_OPTIMIZATION = "exploit_optimization"
    VULNERABILITY_PREDICTION = "vulnerability_prediction"
    EVASION_ADAPTATION = "evasion_adaptation"
    PAYLOAD_EVOLUTION = "payload_evolution"
    TARGET_PROFILING = "target_profiling"
    EVASION_TECHNIQUE = "evasion_technique"
    PERSISTENCE_METHOD = "persistence_method"
    VULNERABILITY_RESEARCH = "vulnerability_research"


class ModelType(Enum):
    """Supported ML model types"""
    RANDOM_FOREST = "random_forest"
    GRADIENT_BOOSTING = "gradient_boosting"
    NEURAL_NETWORK = "neural_network"
    CLUSTERING = "clustering"
    REINFORCEMENT = "reinforcement"


class MLAdaptationEngine:
    """
    Machine learning adaptation engine for intelligent exploitation optimization.
    """

    def __init__(self):
        self.logger = logging.getLogger("IntellicrackLogger.MLAdaptationEngine")

        # ML models registry
        self.models = {}
        self.scalers = {}
        self.encoders = {}

        # Training data storage
        self.training_data = {
            'exploit_success': [],
            'vulnerability_features': [],
            'evasion_effectiveness': [],
            'payload_performance': [],
            'target_characteristics': []
        }

        # Adaptation configuration
        self.config = {
            'min_training_samples': 50,
            'model_retrain_threshold': 100,
            'confidence_threshold': 0.7,
            'adaptation_rate': 0.1,
            'feature_importance_threshold': 0.05,
            'model_persistence_dir': '/tmp/intellicrack_ml_models'
        }

        # Feature extractors
        self.feature_extractors = {
            'binary_features': self._extract_binary_features,
            'network_features': self._extract_network_features,
            'system_features': self._extract_system_features,
            'exploit_features': self._extract_exploit_features,
            'temporal_features': self._extract_temporal_features
        }

        # Adaptation strategies
        self.adaptation_strategies = {
            AdaptationStrategy.EXPLOIT_OPTIMIZATION: self._adapt_exploit_strategy,
            AdaptationStrategy.VULNERABILITY_PREDICTION: self._adapt_vulnerability_prediction,
            AdaptationStrategy.EVASION_ADAPTATION: self._adapt_evasion_techniques,
            AdaptationStrategy.PAYLOAD_EVOLUTION: self._adapt_payload_generation,
            AdaptationStrategy.TARGET_PROFILING: self._adapt_target_profiling
        }

        # Performance metrics
        self.metrics = {
            'adaptations_performed': 0,
            'success_rate_improvement': 0.0,
            'model_accuracy': {},
            'feature_importance': {},
            'adaptation_history': []
        }

        # Initialize ML components
        self._initialize_ml_components()

    def adapt_exploitation_strategy(self,
                                  target_info: Dict[str, Any],
                                  previous_attempts: List[Dict[str, Any]],
                                  strategy: AdaptationStrategy = AdaptationStrategy.EXPLOIT_OPTIMIZATION) -> Dict[str, Any]:
        """
        Adapt exploitation strategy based on ML analysis.

        Args:
            target_info: Information about target system
            previous_attempts: Historical exploitation attempts
            strategy: Adaptation strategy to use

        Returns:
            Adapted exploitation recommendations
        """
        result = {
            'success': False,
            'strategy': strategy.value,
            'adaptations': {},
            'confidence': 0.0,
            'recommendations': [],
            'model_insights': {},
            'error': None
        }

        try:
            self.logger.info(f"Adapting exploitation strategy: {strategy.value}")

            if not ML_AVAILABLE:
                result['error'] = "ML libraries not available"
                return result

            # Extract features from target and attempts
            features = self._extract_comprehensive_features(target_info, previous_attempts)

            # Apply adaptation strategy
            if strategy in self.adaptation_strategies:
                adaptation_func = self.adaptation_strategies[strategy]
                adaptations = adaptation_func(features, target_info, previous_attempts)
                result['adaptations'] = adaptations

                # Calculate confidence based on model predictions
                result['confidence'] = self._calculate_adaptation_confidence(features, adaptations)

                # Generate recommendations
                result['recommendations'] = self._generate_adaptation_recommendations(
                    strategy, adaptations, result['confidence']
                )

                # Get model insights
                result['model_insights'] = self._get_model_insights(features, strategy)

                # Update training data with new attempt
                self._update_training_data(target_info, previous_attempts, adaptations)

                # Check if models need retraining
                if self._should_retrain_models():
                    self._retrain_models()

                result['success'] = True
                self.metrics['adaptations_performed'] += 1

                # Log adaptation
                self._log_adaptation(strategy, adaptations, result['confidence'])

            else:
                result['error'] = f"Unknown adaptation strategy: {strategy.value}"

            self.logger.info(f"Adaptation completed with confidence: {result['confidence']:.2f}")

        except Exception as e:
            self.logger.error(f"Adaptation failed: {e}")
            result['error'] = str(e)

        return result

    def train_adaptation_models(self, training_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Train or retrain ML adaptation models.

        Args:
            training_data: Optional external training data

        Returns:
            Training results and model performance
        """
        result = {
            'success': False,
            'models_trained': [],
            'performance_metrics': {},
            'feature_importance': {},
            'training_samples': 0,
            'error': None
        }

        try:
            self.logger.info("Training adaptation models")

            if not ML_AVAILABLE:
                result['error'] = "ML libraries not available"
                return result

            # Use provided data or internal training data
            data = training_data if training_data else self.training_data

            # Check minimum training samples
            total_samples = sum(len(samples) for samples in data.values())
            if total_samples < self.config['min_training_samples']:
                result['error'] = f"Insufficient training data: {total_samples} < {self.config['min_training_samples']}"
                return result

            result['training_samples'] = total_samples

            # Train exploit success prediction model
            if len(data['exploit_success']) > 0:
                exploit_model_result = self._train_exploit_success_model(data['exploit_success'])
                if exploit_model_result['success']:
                    result['models_trained'].append('exploit_success')
                    result['performance_metrics']['exploit_success'] = exploit_model_result['metrics']
                    result['feature_importance']['exploit_success'] = exploit_model_result['feature_importance']

            # Train vulnerability prediction model
            if len(data['vulnerability_features']) > 0:
                vuln_model_result = self._train_vulnerability_model(data['vulnerability_features'])
                if vuln_model_result['success']:
                    result['models_trained'].append('vulnerability_prediction')
                    result['performance_metrics']['vulnerability_prediction'] = vuln_model_result['metrics']
                    result['feature_importance']['vulnerability_prediction'] = vuln_model_result['feature_importance']

            # Train evasion effectiveness model
            if len(data['evasion_effectiveness']) > 0:
                evasion_model_result = self._train_evasion_model(data['evasion_effectiveness'])
                if evasion_model_result['success']:
                    result['models_trained'].append('evasion_effectiveness')
                    result['performance_metrics']['evasion_effectiveness'] = evasion_model_result['metrics']
                    result['feature_importance']['evasion_effectiveness'] = evasion_model_result['feature_importance']

            # Train payload performance model
            if len(data['payload_performance']) > 0:
                payload_model_result = self._train_payload_model(data['payload_performance'])
                if payload_model_result['success']:
                    result['models_trained'].append('payload_performance')
                    result['performance_metrics']['payload_performance'] = payload_model_result['metrics']
                    result['feature_importance']['payload_performance'] = payload_model_result['feature_importance']

            # Train target clustering model
            if len(data['target_characteristics']) > 0:
                cluster_model_result = self._train_clustering_model(data['target_characteristics'])
                if cluster_model_result['success']:
                    result['models_trained'].append('target_clustering')
                    result['performance_metrics']['target_clustering'] = cluster_model_result['metrics']

            # Save trained models
            self._save_models()

            # Update metrics
            self.metrics['model_accuracy'] = result['performance_metrics']
            self.metrics['feature_importance'] = result['feature_importance']

            result['success'] = len(result['models_trained']) > 0

            self.logger.info(f"Model training completed: {len(result['models_trained'])} models trained")

        except Exception as e:
            self.logger.error(f"Model training failed: {e}")
            result['error'] = str(e)

        return result

    def predict_exploitation_success(self,
                                   target_info: Dict[str, Any],
                                   exploit_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Predict likelihood of exploitation success.

        Args:
            target_info: Target system information
            exploit_config: Exploit configuration

        Returns:
            Success prediction and confidence
        """
        prediction = {
            'success_probability': 0.0,
            'confidence': 0.0,
            'risk_factors': [],
            'recommendations': [],
            'model_used': None,
            'feature_analysis': {},
            'error': None
        }

        try:
            if not ML_AVAILABLE or 'exploit_success' not in self.models:
                prediction['error'] = "Exploit success model not available"
                return prediction

            # Extract features
            features = self._extract_exploit_prediction_features(target_info, exploit_config)

            # Make prediction
            model = self.models['exploit_success']
            scaler = self.scalers.get('exploit_success')

            if scaler:
                features_scaled = scaler.transform([features])
            else:
                features_scaled = [features]

            # Get probability prediction
            proba = model.predict_proba(features_scaled)[0]
            prediction['success_probability'] = float(proba[1]) if len(proba) > 1 else float(proba[0])

            # Calculate prediction confidence
            prediction['confidence'] = max(proba) - min(proba) if len(proba) > 1 else abs(proba[0] - 0.5) * 2

            # Analyze feature importance
            feature_importance = {}
            if hasattr(model, 'feature_importances_'):
                prediction['feature_analysis'] = self._analyze_feature_importance(
                    features, model.feature_importances_
                )
                # Convert feature importances to dict for recommendations
                for i, importance in enumerate(model.feature_importances_):
                    feature_importance[f'feature_{i}'] = float(importance)

            # Identify risk factors
            prediction['risk_factors'] = self._identify_risk_factors(features, feature_importance)

            # Generate recommendations
            prediction['recommendations'] = self._generate_success_recommendations(
                features, prediction['success_probability'], feature_importance
            )

            prediction['model_used'] = 'exploit_success'

        except Exception as e:
            self.logger.error(f"Success prediction failed: {e}")
            prediction['error'] = str(e)

        return prediction

    def optimize_payload_parameters(self,
                                  payload_type: str,
                                  target_info: Dict[str, Any],
                                  performance_history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Optimize payload parameters using ML insights.

        Args:
            payload_type: Type of payload to optimize
            target_info: Target system information
            performance_history: Historical payload performance

        Returns:
            Optimized parameters and recommendations
        """
        optimization = {
            'optimized_parameters': {},
            'expected_improvement': 0.0,
            'confidence': 0.0,
            'parameter_analysis': {},
            'recommendations': [],
            'error': None
        }

        try:
            if not ML_AVAILABLE or 'payload_performance' not in self.models:
                optimization['error'] = "Payload performance model not available"
                return optimization

            # Extract current payload features
            current_features = self._extract_payload_features({'type': payload_type, 'target': target_info})

            # Analyze performance history
            performance_analysis = self._analyze_payload_performance_history(performance_history)

            # Use model to predict optimal parameters
            model = self.models['payload_performance']

            # Generate parameter variations
            parameter_variations = self._generate_parameter_variations(current_features)

            # Predict performance for each variation
            best_performance = 0.0
            best_parameters = current_features

            for variation in parameter_variations:
                performance_result = self._predict_payload_performance(variation)
                predicted_performance = performance_result.get('success_rate', 0.0)
                if predicted_performance > best_performance:
                    best_performance = predicted_performance
                    best_parameters = variation

            # Calculate improvement
            baseline_result = self._predict_payload_performance(current_features)
            baseline_performance = baseline_result.get('success_rate', 0.0)
            optimization['expected_improvement'] = best_performance - baseline_performance

            # Extract optimized parameters
            # Convert feature differences back to parameter recommendations
            optimization['optimized_parameters'] = {
                'feature_changes': [best - current for best, current in zip(best_parameters, current_features)],
                'recommended_features': best_parameters
            }

            # Calculate confidence
            optimization['confidence'] = self._calculate_optimization_confidence(
                current_features, best_parameters
            )

            # Analyze parameter importance
            optimization['parameter_analysis'] = self._analyze_parameter_importance(
                current_features, best_parameters
            )

            # Generate recommendations
            optimization['recommendations'] = self._generate_optimization_recommendations(
                optimization['optimized_parameters'], optimization['expected_improvement']
            )

        except Exception as e:
            self.logger.error(f"Payload optimization failed: {e}")
            optimization['error'] = str(e)

        return optimization

    def cluster_targets(self, target_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Cluster targets based on characteristics for strategic grouping.

        Args:
            target_data: List of target information

        Returns:
            Clustering results and target groups
        """
        clustering = {
            'clusters': {},
            'cluster_characteristics': {},
            'target_assignments': {},
            'optimization_strategies': {},
            'error': None
        }

        try:
            if not ML_AVAILABLE:
                clustering['error'] = "ML libraries not available"
                return clustering

            if len(target_data) < 3:
                clustering['error'] = "Insufficient targets for clustering"
                return clustering

            # Extract features from all targets
            feature_matrix = []
            target_ids = []

            for i, target in enumerate(target_data):
                features = self._extract_target_clustering_features(target)
                feature_matrix.append(features)
                target_ids.append(target.get('id', f'target_{i}'))

            # Standardize features
            scaler = StandardScaler()
            features_scaled = scaler.fit_transform(feature_matrix)

            # Determine optimal number of clusters
            n_clusters = min(len(target_data) // 2, 10)
            n_clusters = max(n_clusters, 2)

            # Perform clustering
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            cluster_labels = kmeans.fit_predict(features_scaled)

            # Store clustering model
            self.models['target_clustering'] = kmeans
            self.scalers['target_clustering'] = scaler

            # Organize results by cluster
            for i, (target_id, cluster_id) in enumerate(zip(target_ids, cluster_labels)):
                cluster_id = int(cluster_id)

                if cluster_id not in clustering['clusters']:
                    clustering['clusters'][cluster_id] = []

                clustering['clusters'][cluster_id].append({
                    'target_id': target_id,
                    'target_data': target_data[i],
                    'features': feature_matrix[i]
                })

                clustering['target_assignments'][target_id] = cluster_id

            # Analyze cluster characteristics
            for cluster_id, targets in clustering['clusters'].items():
                # Extract features from targets
                cluster_features = [t['features'] for t in targets]
                cluster_labels = [cluster_id] * len(targets)
                characteristics = self._analyze_cluster_characteristics(cluster_labels, cluster_features)
                clustering['cluster_characteristics'][cluster_id] = characteristics.get(cluster_id, {})

                # Generate optimization strategies for each cluster
                optimization_strategy = self._generate_cluster_optimization_strategy(
                    characteristics, targets
                )
                clustering['optimization_strategies'][cluster_id] = optimization_strategy

        except Exception as e:
            self.logger.error(f"Target clustering failed: {e}")
            clustering['error'] = str(e)

        return clustering

    def get_adaptation_insights(self) -> Dict[str, Any]:
        """Get insights from ML adaptation engine."""
        insights = {
            'model_status': {},
            'performance_metrics': self.metrics.copy(),
            'feature_importance': {},
            'adaptation_trends': [],
            'recommendations': []
        }

        try:
            # Model status
            for model_name in self.models:
                insights['model_status'][model_name] = {
                    'trained': True,
                    'last_updated': getattr(self.models[model_name], 'last_updated', 'unknown'),
                    'training_samples': getattr(self.models[model_name], 'n_samples', 'unknown')
                }

            # Feature importance analysis
            for model_name, model in self.models.items():
                if hasattr(model, 'feature_importances_'):
                    # Convert numpy array to dict for _get_top_features
                    importance_dict = {f'feature_{i}': float(importance)
                                     for i, importance in enumerate(model.feature_importances_)}
                    insights['feature_importance'][model_name] = {
                        'top_features': self._get_top_features(importance_dict, 10),
                        'importance_distribution': model.feature_importances_.tolist()
                    }

            # Adaptation trends
            insights['adaptation_trends'] = self._analyze_adaptation_trends([])

            # Generate strategic recommendations
            insights['recommendations'] = self._generate_strategic_recommendations(insights, insights['adaptation_trends'])

        except Exception as e:
            self.logger.error(f"Failed to get adaptation insights: {e}")
            insights['error'] = str(e)

        return insights

    def _initialize_ml_components(self):
        """Initialize ML components and load existing models."""
        try:
            # Create model persistence directory
            os.makedirs(self.config['model_persistence_dir'], exist_ok=True)

            # Try to load existing models
            self._load_models()

            self.logger.info("ML adaptation engine initialized")

        except Exception as e:
            self.logger.warning(f"ML initialization failed: {e}")

    def _load_models(self):
        """Load pre-trained ML models from disk."""
        try:
            import os
            import pickle

            model_dir = self.config['model_persistence_dir']

            # Load exploit success predictor
            exploit_model_path = os.path.join(model_dir, 'exploit_success_model.pkl')
            if os.path.exists(exploit_model_path):
                with open(exploit_model_path, 'rb') as f:
                    self.models['exploit_success'] = pickle.load(f)
                self.logger.info("Loaded exploit success prediction model")

            # Load vulnerability predictor
            vuln_model_path = os.path.join(model_dir, 'vulnerability_predictor.pkl')
            if os.path.exists(vuln_model_path):
                with open(vuln_model_path, 'rb') as f:
                    self.models['vulnerability_predictor'] = pickle.load(f)
                self.logger.info("Loaded vulnerability prediction model")

            # Load evasion effectiveness model
            evasion_model_path = os.path.join(model_dir, 'evasion_effectiveness.pkl')
            if os.path.exists(evasion_model_path):
                with open(evasion_model_path, 'rb') as f:
                    self.models['evasion_effectiveness'] = pickle.load(f)
                self.logger.info("Loaded evasion effectiveness model")

            # Load scalers and encoders
            scaler_path = os.path.join(model_dir, 'feature_scalers.pkl')
            if os.path.exists(scaler_path):
                with open(scaler_path, 'rb') as f:
                    self.scalers = pickle.load(f)
                self.logger.info("Loaded feature scalers")

            encoder_path = os.path.join(model_dir, 'label_encoders.pkl')
            if os.path.exists(encoder_path):
                with open(encoder_path, 'rb') as f:
                    self.encoders = pickle.load(f)
                self.logger.info("Loaded label encoders")

        except Exception as e:
            self.logger.debug(f"Model loading skipped (expected on first run): {e}")
            # Initialize default models if loading fails
            self._initialize_default_models()

    def _initialize_default_models(self):
        """Initialize default ML models when no pre-trained models exist."""
        try:
            # Create simple default models
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.preprocessing import StandardScaler

            # Default exploit success predictor
            self.models['exploit_success'] = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )

            # Default vulnerability predictor
            self.models['vulnerability_predictor'] = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )

            # Default evasion effectiveness model
            self.models['evasion_effectiveness'] = RandomForestClassifier(
                n_estimators=50,
                max_depth=8,
                random_state=42
            )

            # Initialize scalers
            self.scalers['feature_scaler'] = StandardScaler()

            self.logger.info("Initialized default ML models")

        except ImportError:
            self.logger.warning("scikit-learn not available - ML features disabled")
        except Exception as e:
            self.logger.error(f"Failed to initialize default models: {e}")

    def _extract_comprehensive_features(self, target_info: Dict[str, Any],
                                      previous_attempts: List[Dict[str, Any]]) -> List[float]:
        """Extract comprehensive feature set for ML analysis."""
        features = []

        # Binary features
        features.extend(self.feature_extractors['binary_features'](target_info))

        # Network features
        features.extend(self.feature_extractors['network_features'](target_info))

        # System features
        features.extend(self.feature_extractors['system_features'](target_info))

        # Exploit features from previous attempts
        features.extend(self.feature_extractors['exploit_features'](previous_attempts))

        # Temporal features
        features.extend(self.feature_extractors['temporal_features'](previous_attempts))

        return features

    def _extract_binary_features(self, target_info: Dict[str, Any]) -> List[float]:
        """Extract binary-related features."""
        features = []

        # Architecture features
        arch = target_info.get('architecture', 'unknown')
        features.append(1.0 if arch == 'x64' else 0.0)
        features.append(1.0 if arch == 'x86' else 0.0)
        features.append(1.0 if arch == 'arm' else 0.0)

        # Protection features
        protections = target_info.get('protections', [])
        features.append(1.0 if 'aslr' in protections else 0.0)
        features.append(1.0 if 'dep' in protections else 0.0)
        features.append(1.0 if 'canary' in protections else 0.0)
        features.append(1.0 if 'cfi' in protections else 0.0)
        features.append(1.0 if 'pie' in protections else 0.0)

        # Binary size (normalized)
        binary_size = target_info.get('binary_size', 0)
        features.append(min(binary_size / 1000000.0, 1.0))  # Normalize to MB

        # Function count (normalized)
        function_count = target_info.get('function_count', 0)
        features.append(min(function_count / 1000.0, 1.0))

        return features

    def _extract_network_features(self, target_info: Dict[str, Any]) -> List[float]:
        """Extract network-related features."""
        features = []

        # Service features
        services = target_info.get('services', [])
        features.append(len(services) / 10.0)  # Normalized service count

        # Common service indicators
        features.append(1.0 if any('http' in s.lower() for s in services) else 0.0)
        features.append(1.0 if any('ssh' in s.lower() for s in services) else 0.0)
        features.append(1.0 if any('ftp' in s.lower() for s in services) else 0.0)
        features.append(1.0 if any('smb' in s.lower() for s in services) else 0.0)

        # Network configuration
        network_config = target_info.get('network_config', {})
        features.append(1.0 if network_config.get('firewall_enabled') else 0.0)
        features.append(1.0 if network_config.get('nat_traversal_required') else 0.0)

        return features

    def _extract_system_features(self, target_info: Dict[str, Any]) -> List[float]:
        """Extract system-level features."""
        features = []

        # OS features
        os_type = target_info.get('os_type', 'unknown')
        features.append(1.0 if 'windows' in os_type.lower() else 0.0)
        features.append(1.0 if 'linux' in os_type.lower() else 0.0)
        features.append(1.0 if 'macos' in os_type.lower() else 0.0)

        # Version information (simplified)
        os_version = target_info.get('os_version', '')
        features.append(1.0 if 'server' in os_version.lower() else 0.0)
        features.append(1.0 if any(str(i) in os_version for i in range(2020, 2025)) else 0.0)  # Recent version

        # System resources
        features.append(min(target_info.get('cpu_cores', 1) / 16.0, 1.0))
        features.append(min(target_info.get('memory_gb', 1) / 64.0, 1.0))

        # Security features
        av_products = target_info.get('av_products', [])
        features.append(len(av_products) / 5.0)  # Normalized AV count
        features.append(1.0 if any('defender' in av.lower() for av in av_products) else 0.0)

        return features

    def _extract_exploit_features(self, previous_attempts: List[Dict[str, Any]]) -> List[float]:
        """Extract exploit-related features from previous attempts."""
        features = []

        if not previous_attempts:
            return [0.0] * 10  # Return zeros if no attempts

        # Success rate
        successful_attempts = sum(1 for attempt in previous_attempts if attempt.get('success', False))
        features.append(successful_attempts / len(previous_attempts))

        # Attempt diversity
        exploit_types = set(attempt.get('exploit_type', 'unknown') for attempt in previous_attempts)
        features.append(len(exploit_types) / 10.0)  # Normalized diversity

        # Average attempt duration
        durations = [attempt.get('duration', 0) for attempt in previous_attempts]
        avg_duration = sum(durations) / len(durations) if durations else 0
        features.append(min(avg_duration / 3600.0, 1.0))  # Normalized to hours

        # Payload complexity (simplified)
        payload_sizes = [attempt.get('payload_size', 0) for attempt in previous_attempts]
        avg_payload_size = sum(payload_sizes) / len(payload_sizes) if payload_sizes else 0
        features.append(min(avg_payload_size / 10000.0, 1.0))  # Normalized

        # Evasion techniques used
        evasion_techniques = set()
        for attempt in previous_attempts:
            evasion_techniques.update(attempt.get('evasion_techniques', []))
        features.append(len(evasion_techniques) / 20.0)  # Normalized

        # Recent success indicator
        recent_attempts = previous_attempts[-5:] if len(previous_attempts) >= 5 else previous_attempts
        recent_success = sum(1 for attempt in recent_attempts if attempt.get('success', False))
        features.append(recent_success / len(recent_attempts) if recent_attempts else 0.0)

        # Error patterns
        error_types = set(attempt.get('error_type', 'none') for attempt in previous_attempts if not attempt.get('success', False))
        features.append(len(error_types) / 10.0)  # Normalized error diversity

        # Protection bypass success
        bypass_attempts = [attempt for attempt in previous_attempts if attempt.get('protection_bypass_attempted', False)]
        bypass_success = sum(1 for attempt in bypass_attempts if attempt.get('protection_bypass_success', False))
        features.append(bypass_success / len(bypass_attempts) if bypass_attempts else 0.0)

        # Timing analysis
        timestamps = [attempt.get('timestamp', time.time()) for attempt in previous_attempts]
        if len(timestamps) > 1:
            time_span = max(timestamps) - min(timestamps)
            features.append(min(time_span / 86400.0, 1.0))  # Normalized to days
        else:
            features.append(0.0)

        # Escalation success
        escalation_attempts = [attempt for attempt in previous_attempts if attempt.get('escalation_attempted', False)]
        escalation_success = sum(1 for attempt in escalation_attempts if attempt.get('escalation_success', False))
        features.append(escalation_success / len(escalation_attempts) if escalation_attempts else 0.0)

        return features

    def _extract_temporal_features(self, previous_attempts: List[Dict[str, Any]]) -> List[float]:
        """Extract temporal patterns from attempts."""
        features = []

        if not previous_attempts:
            return [0.0] * 5

        # Time-based patterns
        timestamps = [attempt.get('timestamp', time.time()) for attempt in previous_attempts]

        # Attempt frequency
        if len(timestamps) > 1:
            time_span = max(timestamps) - min(timestamps)
            frequency = len(timestamps) / max(time_span / 3600.0, 1.0)  # Attempts per hour
            features.append(min(frequency / 10.0, 1.0))  # Normalized
        else:
            features.append(0.0)

        # Time since last attempt
        if timestamps:
            time_since_last = time.time() - max(timestamps)
            features.append(min(time_since_last / 86400.0, 1.0))  # Normalized to days
        else:
            features.append(1.0)

        # Success trend (recent vs older)
        if len(previous_attempts) >= 4:
            half_point = len(previous_attempts) // 2
            older_success = sum(1 for attempt in previous_attempts[:half_point] if attempt.get('success', False))
            recent_success = sum(1 for attempt in previous_attempts[half_point:] if attempt.get('success', False))

            older_rate = older_success / half_point
            recent_rate = recent_success / (len(previous_attempts) - half_point)
            features.append(recent_rate - older_rate)  # Trend indicator
        else:
            features.append(0.0)

        # Complexity trend
        complexities = [attempt.get('complexity_score', 0.5) for attempt in previous_attempts]
        if len(complexities) >= 2:
            complexity_trend = complexities[-1] - complexities[0]
            features.append(complexity_trend)
        else:
            features.append(0.0)

        # Persistence of approach
        exploit_types = [attempt.get('exploit_type', 'unknown') for attempt in previous_attempts]
        if exploit_types:
            most_common_type = max(set(exploit_types), key=exploit_types.count)
            persistence = exploit_types.count(most_common_type) / len(exploit_types)
            features.append(persistence)
        else:
            features.append(0.0)

        return features

    # Adaptation strategy implementations

    def _adapt_exploit_strategy(self, features: List[float], target_info: Dict[str, Any],
                               previous_attempts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Adapt exploit strategy based on ML analysis."""
        adaptations = {
            'exploit_type_recommendation': 'buffer_overflow',
            'payload_modifications': {},
            'evasion_adjustments': {},
            'timing_recommendations': {},
            'success_probability': 0.0
        }

        # Log target info for debugging
        self.logger.debug(f"Adapting exploit strategy for target: {target_info.get('binary_name', 'unknown')}")

        # Use target_info to adjust recommendations
        if target_info.get('architecture') == 'x64':
            adaptations['exploit_type_recommendation'] = 'rop_chain'  # Better for x64
        elif target_info.get('architecture') == 'arm':
            adaptations['exploit_type_recommendation'] = 'ret2libc'  # Common on ARM

        # Check for specific protections in target_info
        if target_info.get('has_canary', False):
            adaptations['evasion_adjustments']['bypass_canary'] = True
        if target_info.get('has_pie', False):
            adaptations['payload_modifications']['info_leak_required'] = True

        try:
            if 'exploit_success' in self.models:
                model = self.models['exploit_success']
                scaler = self.scalers.get('exploit_success')

                # Predict success probability
                if scaler:
                    features_scaled = scaler.transform([features])
                else:
                    features_scaled = [features]

                success_prob = model.predict_proba(features_scaled)[0]
                adaptations['success_probability'] = float(success_prob[1]) if len(success_prob) > 1 else float(success_prob[0])

                # Analyze failed attempts for patterns
                failed_attempts = [attempt for attempt in previous_attempts if not attempt.get('success', False)]

                if failed_attempts:
                    # Identify common failure patterns
                    failure_analysis = self._analyze_failure_patterns(failed_attempts)

                    # Recommend alternative exploit types
                    if failure_analysis['common_failure'] == 'protection_bypass':
                        adaptations['exploit_type_recommendation'] = 'rop_chain'
                        adaptations['evasion_adjustments']['bypass_dep'] = True
                        adaptations['evasion_adjustments']['bypass_aslr'] = True
                    elif failure_analysis['common_failure'] == 'payload_detection':
                        adaptations['payload_modifications']['encoding'] = 'polymorphic'
                        adaptations['payload_modifications']['obfuscation'] = 'advanced'
                    elif failure_analysis['common_failure'] == 'timing':
                        adaptations['timing_recommendations']['delay_execution'] = True
                        adaptations['timing_recommendations']['randomize_timing'] = True

        except Exception as e:
            self.logger.debug(f"Exploit strategy adaptation failed: {e}")

        return adaptations

    def _adapt_vulnerability_prediction(self, features: List[float], target_info: Dict[str, Any],
                                      previous_attempts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Adapt vulnerability prediction approach."""
        adaptations = {
            'vulnerability_types_to_focus': [],
            'analysis_depth_adjustment': 'medium',
            'search_strategy': 'comprehensive',
            'priority_functions': []
        }

        try:
            # Use features to analyze complexity and adjust approach
            if features and len(features) > 0:
                # Analyze feature patterns to determine vulnerability focus
                binary_features_start = 0  # Binary features are first in the feature vector
                binary_entropy = features[binary_features_start] if len(features) > binary_features_start else 0

                # High entropy might indicate packed/obfuscated binary
                if binary_entropy > 0.8:
                    adaptations['vulnerability_types_to_focus'].append('unpacking_vulnerabilities')
                    adaptations['analysis_depth_adjustment'] = 'deep'
                    self.logger.debug(f"High entropy detected ({binary_entropy:.2f}), focusing on unpacking vulnerabilities")

                # Architecture-specific vulnerabilities based on features
                if len(features) > 2:
                    is_x64 = features[0] > 0.5
                    is_x86 = features[1] > 0.5
                    is_arm = features[2] > 0.5

                    if is_x64:
                        adaptations['vulnerability_types_to_focus'].extend(['rop_gadgets', 'heap_overflow'])
                    elif is_x86:
                        adaptations['vulnerability_types_to_focus'].extend(['stack_overflow', 'format_string'])
                    elif is_arm:
                        adaptations['vulnerability_types_to_focus'].extend(['return_oriented_programming', 'thumb_mode_confusion'])

                # Protection features analysis (features index 3-8)
                if len(features) > 8:
                    protection_count = sum(features[3:9])  # Count active protections
                    if protection_count < 2:
                        adaptations['search_strategy'] = 'aggressive'  # Few protections, be aggressive
                    elif protection_count > 4:
                        adaptations['search_strategy'] = 'stealthy'  # Many protections, be careful
                        adaptations['analysis_depth_adjustment'] = 'deep'

            # Analyze target characteristics to predict likely vulnerability types
            protections = target_info.get('protections', [])

            # Recommend vulnerability types based on protections
            if 'aslr' not in protections:
                adaptations['vulnerability_types_to_focus'].append('buffer_overflow')
            if 'dep' not in protections:
                adaptations['vulnerability_types_to_focus'].append('stack_overflow')
            if 'canary' not in protections:
                adaptations['vulnerability_types_to_focus'].append('stack_smashing')

            # Adjust analysis depth based on previous success
            successful_attempts = [attempt for attempt in previous_attempts if attempt.get('success', False)]
            if len(successful_attempts) / max(len(previous_attempts), 1) < 0.3:
                adaptations['analysis_depth_adjustment'] = 'deep'
            elif len(successful_attempts) / max(len(previous_attempts), 1) > 0.7:
                adaptations['analysis_depth_adjustment'] = 'light'

            # Recommend search strategy
            if target_info.get('binary_size', 0) > 10000000:  # Large binary
                adaptations['search_strategy'] = 'targeted'
                # Focus on high-risk functions
                adaptations['priority_functions'] = ['strcpy', 'sprintf', 'gets', 'memcpy']
            else:
                adaptations['search_strategy'] = 'comprehensive'

        except Exception as e:
            self.logger.debug(f"Vulnerability prediction adaptation failed: {e}")

        return adaptations

    def _adapt_evasion_techniques(self, features: List[float], target_info: Dict[str, Any],
                                previous_attempts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Adapt evasion techniques based on detection patterns."""
        adaptations = {
            'recommended_evasions': [],
            'avoid_techniques': [],
            'detection_risk_level': 'medium',
            'stealth_adjustments': {}
        }

        # Use features to assess detection risk level
        if features and len(features) > 0:
            # Use first few features as indicators of complexity/risk
            avg_feature_value = sum(features[:5]) / min(len(features), 5)
            if avg_feature_value > 0.7:
                adaptations['detection_risk_level'] = 'high'
                self.logger.debug(f"High detection risk based on feature analysis: {avg_feature_value:.2f}")
            elif avg_feature_value < 0.3:
                adaptations['detection_risk_level'] = 'low'
                self.logger.debug(f"Low detection risk based on feature analysis: {avg_feature_value:.2f}")

        try:
            # Analyze AV products and adjust evasions
            av_products = target_info.get('av_products', [])

            if any('defender' in av.lower() for av in av_products):
                adaptations['recommended_evasions'].extend(['amsi_bypass', 'etw_bypass'])
                adaptations['avoid_techniques'].append('powershell_execution')

            if any('symantec' in av.lower() for av in av_products):
                adaptations['recommended_evasions'].append('behavioral_evasion')
                adaptations['stealth_adjustments']['execution_delay'] = True

            # Analyze previous detection patterns
            detected_attempts = [attempt for attempt in previous_attempts if attempt.get('detected', False)]

            if len(detected_attempts) / max(len(previous_attempts), 1) > 0.5:
                adaptations['detection_risk_level'] = 'high'
                adaptations['recommended_evasions'].extend(['process_hollowing', 'dll_sideloading'])
                adaptations['stealth_adjustments']['anti_analysis'] = True
            elif len(detected_attempts) / max(len(previous_attempts), 1) < 0.2:
                adaptations['detection_risk_level'] = 'low'
                adaptations['stealth_adjustments']['minimal_evasion'] = True

        except Exception as e:
            self.logger.debug(f"Evasion adaptation failed: {e}")

        return adaptations

    def _adapt_payload_generation(self, features: List[float], target_info: Dict[str, Any],
                                previous_attempts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Adapt payload generation parameters."""
        adaptations = {
            'encoding_recommendation': 'polymorphic',
            'size_optimization': False,
            'architecture_specific': {},
            'obfuscation_level': 'medium'
        }

        # Use features to determine obfuscation level
        if features and len(features) > 0:
            # Higher feature values suggest more complex environment
            complexity_score = sum(features) / len(features)
            if complexity_score > 0.7:
                adaptations['obfuscation_level'] = 'high'
                adaptations['encoding_recommendation'] = 'metamorphic'
                self.logger.debug(f"High complexity environment detected: {complexity_score:.2f}")
            elif complexity_score < 0.3:
                adaptations['obfuscation_level'] = 'low'
                adaptations['encoding_recommendation'] = 'xor'
                self.logger.debug(f"Low complexity environment detected: {complexity_score:.2f}")

        try:
            # Analyze payload performance history
            payload_attempts = [attempt for attempt in previous_attempts if 'payload_size' in attempt]

            if payload_attempts:
                # Analyze size vs success correlation
                successful_payloads = [attempt for attempt in payload_attempts if attempt.get('success', False)]
                failed_payloads = [attempt for attempt in payload_attempts if not attempt.get('success', False)]

                if successful_payloads and failed_payloads:
                    avg_successful_size = sum(p['payload_size'] for p in successful_payloads) / len(successful_payloads)
                    avg_failed_size = sum(p['payload_size'] for p in failed_payloads) / len(failed_payloads)

                    if avg_successful_size < avg_failed_size:
                        adaptations['size_optimization'] = True

            # Architecture-specific adaptations
            arch = target_info.get('architecture', 'x86')
            if arch == 'x64':
                adaptations['architecture_specific']['use_64bit_addresses'] = True
                adaptations['architecture_specific']['register_usage'] = 'extended'
            elif arch == 'arm':
                adaptations['architecture_specific']['thumb_mode'] = True
                adaptations['architecture_specific']['cache_management'] = True

            # Adjust obfuscation based on detection history
            detected_count = sum(1 for attempt in previous_attempts if attempt.get('detected', False))
            detection_rate = detected_count / max(len(previous_attempts), 1)

            if detection_rate > 0.6:
                adaptations['obfuscation_level'] = 'maximum'
                adaptations['encoding_recommendation'] = 'metamorphic'
            elif detection_rate < 0.2:
                adaptations['obfuscation_level'] = 'minimal'
                adaptations['encoding_recommendation'] = 'xor'

        except Exception as e:
            self.logger.debug(f"Payload adaptation failed: {e}")

        return adaptations

    def _adapt_target_profiling(self, features: List[float], target_info: Dict[str, Any],
                              previous_attempts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Adapt target profiling and reconnaissance approach."""
        adaptations = {
            'profiling_depth': 'standard',
            'focus_areas': [],
            'reconnaissance_techniques': [],
            'stealth_level': 'medium'
        }

        # Use features to assess target complexity and adjust profiling approach
        if features and len(features) > 0:
            # Feature-based complexity assessment
            feature_complexity = sum(features) / len(features)
            self.logger.debug(f"Target profiling complexity score from features: {feature_complexity:.2f}")

            # Adjust reconnaissance techniques based on features
            if feature_complexity > 0.6:
                adaptations['reconnaissance_techniques'].append('advanced_fingerprinting')
                adaptations['profiling_depth'] = 'deep'

        try:
            # Determine profiling depth based on target complexity
            services = target_info.get('services', [])
            protections = target_info.get('protections', [])

            complexity_score = len(services) + len(protections) * 2

            if complexity_score > 15:
                adaptations['profiling_depth'] = 'deep'
                adaptations['focus_areas'].extend(['service_enumeration', 'protection_analysis'])
            elif complexity_score < 5:
                adaptations['profiling_depth'] = 'light'
                adaptations['focus_areas'].append('basic_enumeration')
            else:
                adaptations['profiling_depth'] = 'standard'
                adaptations['focus_areas'].extend(['service_analysis', 'vulnerability_scanning'])

            # Adjust stealth based on detection history
            if any(attempt.get('detected', False) for attempt in previous_attempts):
                adaptations['stealth_level'] = 'high'
                adaptations['reconnaissance_techniques'].extend(['passive_scanning', 'osint_gathering'])
            else:
                adaptations['stealth_level'] = 'medium'
                adaptations['reconnaissance_techniques'].extend(['active_scanning', 'version_detection'])

        except Exception as e:
            self.logger.debug(f"Target profiling adaptation failed: {e}")

        return adaptations

    # Additional helper methods for analysis and prediction

    def _analyze_failure_patterns(self, failed_attempts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze patterns in failed exploitation attempts."""
        analysis = {
            'common_failure': 'unknown',
            'failure_frequency': {},
            'patterns': []
        }

        # Count failure types
        failure_types = [attempt.get('failure_type', 'unknown') for attempt in failed_attempts]
        for failure_type in failure_types:
            analysis['failure_frequency'][failure_type] = analysis['failure_frequency'].get(failure_type, 0) + 1

        # Identify most common failure
        if analysis['failure_frequency']:
            analysis['common_failure'] = max(analysis['failure_frequency'], key=analysis['failure_frequency'].get)

        return analysis

    def _calculate_adaptation_confidence(self, features: List[float], adaptations: Dict[str, Any]) -> float:
        """Calculate confidence in adaptation recommendations."""
        base_confidence = 0.5

        # Consider adaptation complexity for confidence scoring
        if adaptations and len(adaptations) > 0:
            self.logger.debug(f"Calculating confidence for {len(adaptations)} adaptations with {len(features)} features")

        # Increase confidence based on model availability
        if 'exploit_success' in self.models:
            base_confidence += 0.2
        if 'vulnerability_prediction' in self.models:
            base_confidence += 0.1
        if 'evasion_effectiveness' in self.models:
            base_confidence += 0.1

        # Adjust based on feature completeness
        feature_completeness = len([f for f in features if f > 0]) / len(features)
        confidence_adjustment = feature_completeness * 0.2

        return min(1.0, base_confidence + confidence_adjustment)

    def _generate_adaptation_recommendations(self, strategy: AdaptationStrategy,
                                           adaptations: Dict[str, Any],
                                           confidence: float) -> List[str]:
        """Generate actionable recommendations based on adaptations."""
        recommendations = []

        if strategy == AdaptationStrategy.EXPLOIT_OPTIMIZATION:
            if adaptations.get('success_probability', 0) < 0.3:
                recommendations.append("Low success probability - consider alternative exploitation approach")
            if adaptations.get('exploit_type_recommendation') != 'buffer_overflow':
                recommendations.append(f"Switch to {adaptations['exploit_type_recommendation']} exploitation technique")

        elif strategy == AdaptationStrategy.EVASION_ADAPTATION:
            evasions = adaptations.get('recommended_evasions', [])
            if evasions:
                recommendations.append(f"Implement evasion techniques: {', '.join(evasions)}")
            if adaptations.get('detection_risk_level') == 'high':
                recommendations.append("High detection risk - use maximum stealth approach")

        elif strategy == AdaptationStrategy.PAYLOAD_EVOLUTION:
            if adaptations.get('size_optimization'):
                recommendations.append("Optimize payload size for better success rate")
            if adaptations.get('obfuscation_level') == 'maximum':
                recommendations.append("Apply maximum obfuscation to avoid detection")

        # Add confidence-based qualifier
        if confidence < 0.5:
            recommendations.insert(0, "Low confidence predictions - manual review recommended")
        elif confidence > 0.8:
            recommendations.insert(0, "High confidence predictions - safe to proceed")

        return recommendations

    def _get_model_insights(self, features: List[float], strategy: AdaptationStrategy) -> Dict[str, Any]:
        """Get insights from ML models for the given strategy."""
        insights = {
            'feature_importance': {},
            'prediction_confidence': 0.0,
            'model_recommendations': []
        }

        try:
            if strategy == AdaptationStrategy.EXPLOIT_OPTIMIZATION and 'exploit_success' in self.models:
                model = self.models['exploit_success']
                if hasattr(model, 'feature_importances_'):
                    insights['feature_importance'] = self._analyze_feature_importance(features, model.feature_importances_)

            elif strategy == AdaptationStrategy.VULNERABILITY_PREDICTION and 'vulnerability_prediction' in self.models:
                model = self.models['vulnerability_prediction']
                if hasattr(model, 'feature_importances_'):
                    insights['feature_importance'] = self._analyze_feature_importance(features, model.feature_importances_)

        except Exception as e:
            self.logger.debug(f"Model insights extraction failed: {e}")

        return insights

    def _update_training_data(self, target_info: Dict[str, Any],
                            previous_attempts: List[Dict[str, Any]],
                            adaptations: Dict[str, Any]):
        """Update training data with new adaptation results."""
        try:
            # Extract features
            features = self._extract_comprehensive_features(target_info, previous_attempts)

            # Add to exploit success training data
            if previous_attempts:
                last_attempt = previous_attempts[-1]
                self.training_data['exploit_success'].append({
                    'features': features,
                    'success': last_attempt.get('success', False),
                    'timestamp': time.time(),
                    'adaptations_applied': adaptations  # Store adaptations for learning
                })

            # Add vulnerability features if available
            if target_info.get('vulnerabilities'):
                for vuln in target_info['vulnerabilities']:
                    self.training_data['vulnerability_features'].append({
                        'features': self._extract_binary_features(target_info),
                        'vulnerability_type': vuln.get('type', 'unknown'),
                        'timestamp': time.time(),
                        'adaptations': adaptations.get('vulnerability_types_to_focus', [])  # Track adaptation focus
                    })

            # Add evasion effectiveness data based on adaptations
            if adaptations.get('evasion_adjustments') or adaptations.get('recommended_evasions'):
                evasion_data = {
                    'features': features,
                    'evasion_techniques': adaptations.get('recommended_evasions', []),
                    'adjustments': adaptations.get('evasion_adjustments', {}),
                    'detection_risk': adaptations.get('detection_risk_level', 'medium'),
                    'timestamp': time.time()
                }
                self.training_data['evasion_effectiveness'].append(evasion_data)

            # Add payload performance data based on adaptations
            if adaptations.get('payload_modifications') or adaptations.get('encoding_recommendation'):
                payload_data = {
                    'features': features,
                    'encoding': adaptations.get('encoding_recommendation', 'none'),
                    'modifications': adaptations.get('payload_modifications', {}),
                    'obfuscation_level': adaptations.get('obfuscation_level', 'medium'),
                    'timestamp': time.time()
                }
                self.training_data['payload_performance'].append(payload_data)

            # Add target characteristics for clustering
            if adaptations.get('profiling_depth') or adaptations.get('focus_areas'):
                target_char_data = {
                    'features': features,
                    'profiling_depth': adaptations.get('profiling_depth', 'standard'),
                    'focus_areas': adaptations.get('focus_areas', []),
                    'stealth_level': adaptations.get('stealth_level', 'medium'),
                    'timestamp': time.time()
                }
                self.training_data['target_characteristics'].append(target_char_data)

            # Limit training data size
            max_samples = self.config.get('max_training_samples', 10000)
            for data_type in self.training_data:
                if len(self.training_data[data_type]) > max_samples:
                    self.training_data[data_type] = self.training_data[data_type][-max_samples:]

            # Log adaptation usage for tracking
            self.logger.debug(f"Updated training data with adaptations: {list(adaptations.keys())}")

        except Exception as e:
            self.logger.debug(f"Training data update failed: {e}")

    def _should_retrain_models(self) -> bool:
        """Check if models should be retrained based on new data."""
        total_new_samples = sum(len(samples) for samples in self.training_data.values())
        return total_new_samples >= self.config['model_retrain_threshold']

    def _retrain_models(self):
        """Retrain models with accumulated data."""
        try:
            self.logger.info("Retraining models with new data")
            training_result = self.train_adaptation_models()

            if training_result['success']:
                self.logger.info(f"Models retrained successfully: {training_result['models_trained']}")
                # Clear training data after successful training
                for data_type in self.training_data:
                    self.training_data[data_type] = self.training_data[data_type][-100:]  # Keep recent samples
            else:
                self.logger.warning(f"Model retraining failed: {training_result.get('error')}")

        except Exception as e:
            self.logger.error(f"Model retraining failed: {e}")

    def _log_adaptation(self, strategy: AdaptationStrategy, adaptations: Dict[str, Any], confidence: float):
        """Log adaptation for analysis and improvement."""
        adaptation_log = {
            'timestamp': time.time(),
            'strategy': strategy.value,
            'adaptations': adaptations,
            'confidence': confidence
        }

        self.metrics['adaptation_history'].append(adaptation_log)

        # Limit history size
        if len(self.metrics['adaptation_history']) > 1000:
            self.metrics['adaptation_history'] = self.metrics['adaptation_history'][-1000:]

    def _save_models(self):
        """Save trained models to disk for persistence."""
        try:
            import os
            import pickle

            model_dir = self.config['model_persistence_dir']
            os.makedirs(model_dir, exist_ok=True)

            # Save models
            for model_name, model in self.models.items():
                model_path = os.path.join(model_dir, f"{model_name}.pkl")
                with open(model_path, 'wb') as f:
                    pickle.dump(model, f)

            # Save scalers
            for scaler_name, scaler in self.scalers.items():
                scaler_path = os.path.join(model_dir, f"{scaler_name}_scaler.pkl")
                with open(scaler_path, 'wb') as f:
                    pickle.dump(scaler, f)

            # Save encoders
            for encoder_name, encoder in self.encoders.items():
                encoder_path = os.path.join(model_dir, f"{encoder_name}_encoder.pkl")
                with open(encoder_path, 'wb') as f:
                    pickle.dump(encoder, f)

            self.logger.info(f"Models saved to {model_dir}")

        except Exception as e:
            self.logger.error(f"Failed to save models: {e}")

    def _extract_exploit_prediction_features(self, target_info: Dict[str, Any],
                                           exploit_config: Dict[str, Any]) -> List[float]:
        """Extract features for exploit success prediction."""
        features = []

        try:
            # Target features
            features.extend(self._extract_binary_features(target_info))
            features.extend(self._extract_system_features(target_info))
            features.extend(self._extract_network_features(target_info))

            # Exploit configuration features
            features.append(len(exploit_config.get('payloads', [])))
            features.append(len(exploit_config.get('evasion_techniques', [])))
            features.append(exploit_config.get('timeout', 30))
            features.append(1.0 if exploit_config.get('use_encryption', False) else 0.0)
            features.append(1.0 if exploit_config.get('stealth_mode', False) else 0.0)

            # Ensure consistent feature vector length
            while len(features) < 50:  # Pad to standard length
                features.append(0.0)

        except Exception as e:
            self.logger.debug(f"Feature extraction failed: {e}")
            features = [0.0] * 50  # Return default features

        return features[:50]  # Ensure max length

    def _analyze_feature_importance(self, features: List[float],
                                  importances: List[float]) -> Dict[str, float]:
        """Analyze feature importance from model."""
        feature_analysis = {}

        # Log feature analysis details
        self.logger.debug(f"Analyzing {len(features)} features with {len(importances)} importance scores")

        try:
            feature_names = [
                'binary_entropy', 'binary_size', 'import_count', 'section_count',
                'os_type', 'arch_type', 'protection_level', 'patch_level',
                'network_exposure', 'firewall_status', 'service_count', 'port_count',
                'payload_count', 'evasion_count', 'timeout', 'encryption', 'stealth'
            ]

            # Extend feature names to match vector length
            while len(feature_names) < len(importances):
                feature_names.append(f'feature_{len(feature_names)}')

            # Create importance mapping
            for i, importance in enumerate(importances[:len(feature_names)]):
                if importance > self.config['feature_importance_threshold']:
                    feature_analysis[feature_names[i]] = importance

        except Exception as e:
            self.logger.debug(f"Feature importance analysis failed: {e}")

        return feature_analysis

    def _identify_risk_factors(self, features: List[float],
                             feature_importance: Dict[str, float]) -> List[str]:
        """Identify risk factors based on feature analysis."""
        risk_factors = []

        try:
            # Analyze feature values directly
            if features and len(features) > 0:
                # Direct feature value analysis
                for i, feature_val in enumerate(features):
                    if i < 20:  # Analyze first 20 features to avoid overwhelming output
                        if feature_val > 0.8:
                            risk_factors.append(f"Feature {i}: High risk value ({feature_val:.2f})")
                        elif feature_val < 0.2:
                            risk_factors.append(f"Feature {i}: Low protective value ({feature_val:.2f})")

                # Overall feature statistics
                if len(features) > 5:
                    high_risk_count = len([f for f in features if f > 0.7])
                    if high_risk_count > len(features) * 0.3:
                        risk_factors.append(f"High proportion of risky features: {high_risk_count}/{len(features)}")

            # Map features to risk assessments using importance
            risk_mappings = {
                'binary_entropy': (0, 7.5, "High entropy binary - possibly packed"),
                'protection_level': (2, 10, "High protection level detected"),
                'network_exposure': (3, 10, "High network exposure"),
                'firewall_status': (0, 1, "Firewall protection active"),
                'patch_level': (5, 10, "System appears well-patched")
            }

            for feature_name, importance in feature_importance.items():
                if feature_name in risk_mappings:
                    min_val, max_val, message = risk_mappings[feature_name]
                    # Combine feature importance with actual feature values
                    if importance > 0.1:  # High importance feature
                        risk_factors.append(f"{message} (importance: {importance:.2f})")

        except Exception as e:
            self.logger.debug(f"Risk factor identification failed: {e}")

        return risk_factors

    def _generate_success_recommendations(self, features: List[float],
                                        prediction: float,
                                        feature_importance: Dict[str, float]) -> List[str]:
        """Generate recommendations to improve success probability."""
        recommendations = []

        try:
            # Analyze feature values for specific recommendations
            if features and len(features) > 0:
                feature_analysis = {
                    'low_values': [i for i, val in enumerate(features) if val < 0.3],
                    'high_values': [i for i, val in enumerate(features) if val > 0.7],
                    'avg_value': sum(features) / len(features)
                }

                # Feature-based recommendations
                if feature_analysis['avg_value'] < 0.4:
                    recommendations.append("Feature values indicate challenging target - consider enhanced reconnaissance")
                elif feature_analysis['avg_value'] > 0.7:
                    recommendations.append("Strong feature profile - exploit conditions favorable")

            if prediction < 0.3:
                recommendations.append("Low success probability - consider alternative exploit")
                recommendations.append("Perform additional reconnaissance")
                recommendations.append("Consider multi-stage attack approach")

                # Use features to suggest specific improvements
                if features and len(features) > 10:
                    weak_features = [i for i, val in enumerate(features[:10]) if val < 0.2]
                    if weak_features:
                        recommendations.append(f"Improve feature conditions at indices: {weak_features[:3]}")

            elif prediction < 0.6:
                recommendations.append("Moderate success probability - proceed with caution")
                recommendations.append("Implement robust error handling")
            else:
                recommendations.append("High success probability - proceed with confidence")

            # Feature-specific recommendations
            for feature_name, importance in feature_importance.items():
                if importance > 0.15:
                    if 'protection' in feature_name:
                        recommendations.append("Consider evasion techniques for protection bypass")
                    elif 'network' in feature_name:
                        recommendations.append("Optimize network-based attack vectors")
                    elif 'binary' in feature_name:
                        recommendations.append("Focus on binary-level exploitation techniques")

        except Exception as e:
            self.logger.debug(f"Recommendation generation failed: {e}")

        return recommendations

    def _extract_payload_features(self, payload_config: Dict[str, Any]) -> List[float]:
        """Extract features from payload configuration."""
        features = []

        try:
            features.append(len(payload_config.get('shellcode', '')))
            features.append(len(payload_config.get('nop_sled', '')))
            features.append(payload_config.get('buffer_size', 0))
            features.append(1.0 if payload_config.get('encoded', False) else 0.0)
            features.append(1.0 if payload_config.get('polymorphic', False) else 0.0)
            features.append(len(payload_config.get('evasion_techniques', [])))
            features.append(payload_config.get('complexity_score', 0.0))

            # Pad to standard length
            while len(features) < 20:
                features.append(0.0)

        except Exception as e:
            self.logger.debug(f"Payload feature extraction failed: {e}")
            features = [0.0] * 20

        return features[:20]

    def _analyze_payload_performance_history(self, performance_history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze historical performance of similar payloads."""
        analysis = {
            'success_rate': 0.0,
            'avg_execution_time': 0.0,
            'detection_rate': 0.0,
            'similar_payloads': 0,
            'performance_trends': {},
            'best_performing_configs': [],
            'worst_performing_configs': []
        }

        try:
            # Enhanced analysis using both training data and provided history
            all_performance_data = list(performance_history) + self.training_data.get('payload_performance', [])

            if all_performance_data:
                analysis['similar_payloads'] = len(all_performance_data)

                # Calculate basic metrics
                successful_payloads = [p for p in all_performance_data if p.get('success', False)]
                detected_payloads = [p for p in all_performance_data if p.get('detected', False)]

                analysis['success_rate'] = len(successful_payloads) / len(all_performance_data)
                analysis['detection_rate'] = len(detected_payloads) / len(all_performance_data)

                # Calculate execution time statistics
                execution_times = [p.get('execution_time', 0) for p in all_performance_data if 'execution_time' in p]
                if execution_times:
                    analysis['avg_execution_time'] = sum(execution_times) / len(execution_times)
                    analysis['performance_trends']['min_time'] = min(execution_times)
                    analysis['performance_trends']['max_time'] = max(execution_times)

                # Identify best and worst performing configurations
                sorted_by_success = sorted(all_performance_data,
                                         key=lambda x: (x.get('success', False), -x.get('execution_time', float('inf'))),
                                         reverse=True)

                analysis['best_performing_configs'] = sorted_by_success[:3]
                analysis['worst_performing_configs'] = sorted_by_success[-3:]

                # Analyze performance trends over time if timestamps available
                timestamped_data = [p for p in all_performance_data if 'timestamp' in p]
                if len(timestamped_data) > 1:
                    timestamped_data.sort(key=lambda x: x['timestamp'])
                    recent_half = timestamped_data[len(timestamped_data)//2:]
                    older_half = timestamped_data[:len(timestamped_data)//2]

                    recent_success_rate = len([p for p in recent_half if p.get('success', False)]) / len(recent_half)
                    older_success_rate = len([p for p in older_half if p.get('success', False)]) / len(older_half)

                    analysis['performance_trends']['trend'] = 'improving' if recent_success_rate > older_success_rate else 'declining'
                    analysis['performance_trends']['trend_magnitude'] = abs(recent_success_rate - older_success_rate)

        except Exception as e:
            self.logger.debug(f"Payload performance analysis failed: {e}")

        return analysis

    def _generate_parameter_variations(self, current_features: List[float]) -> List[List[float]]:
        """Generate variations of payload parameters for optimization."""
        variations = []

        try:
            base_features = current_features.copy()

            # Generate systematic variations of feature values
            variation_factors = [0.8, 0.9, 1.1, 1.2, 1.5]

            for factor in variation_factors:
                # Create feature variation by scaling certain features
                variation = base_features.copy()

                # Vary size-related features (indices 0-3)
                for i in range(min(4, len(variation))):
                    if variation[i] > 0:  # Only vary non-zero features
                        variation[i] = min(1.0, variation[i] * factor)

                variations.append(variation)

            # Binary feature variations (toggle boolean features)
            for i in range(4, min(10, len(base_features))):
                if base_features[i] in [0.0, 1.0]:  # Binary feature
                    variation = base_features.copy()
                    variation[i] = 1.0 - variation[i]  # Toggle
                    variations.append(variation)

            # Complexity variations (adjust multiple features together)
            complexity_factors = [0.7, 1.3]  # Reduce and increase complexity
            for comp_factor in complexity_factors:
                variation = base_features.copy()
                # Apply complexity factor to first half of features
                for i in range(len(variation) // 2):
                    if variation[i] > 0:
                        variation[i] = min(1.0, variation[i] * comp_factor)
                variations.append(variation)

            # Ensure all variations have same length as base features
            variations = [var[:len(base_features)] for var in variations]

            # Add padding if needed
            for variation in variations:
                while len(variation) < len(base_features):
                    variation.append(0.0)

        except Exception as e:
            self.logger.debug(f"Parameter variation generation failed: {e}")
            # Return at least the base features as a variation
            variations = [current_features.copy()] if current_features else [[0.0] * 10]

        return variations[:10]  # Limit variations

    def _predict_payload_performance(self, payload_features: List[float]) -> Dict[str, float]:
        """Predict payload performance metrics."""
        prediction = {
            'success_probability': 0.5,
            'execution_time': 10.0,
            'detection_probability': 0.3,
            'confidence': 0.5
        }

        try:
            if 'payload_performance' in self.models:
                model = self.models['payload_performance']
                if hasattr(model, 'predict_proba'):
                    # Classification model
                    proba = model.predict_proba([payload_features])[0]
                    prediction['success_probability'] = max(proba) if len(proba) > 0 else 0.5
                elif hasattr(model, 'predict'):
                    # Regression model
                    pred = model.predict([payload_features])[0]
                    prediction['success_probability'] = max(0.0, min(1.0, pred))

                prediction['confidence'] = 0.8  # Model available
            else:
                # Use heuristics
                complexity_score = sum(payload_features[:5]) / 5.0 if len(payload_features) >= 5 else 0.5
                prediction['success_probability'] = max(0.1, min(0.9, complexity_score))
                prediction['confidence'] = 0.3  # Heuristic only

        except Exception as e:
            self.logger.debug(f"Payload performance prediction failed: {e}")

        return prediction

    def _extract_parameter_differences(self, original_config: Dict[str, Any],
                                     variation_config: Dict[str, Any]) -> Dict[str, Any]:
        """Extract differences between payload configurations."""
        differences = {}

        try:
            all_keys = set(original_config.keys()) | set(variation_config.keys())

            for key in all_keys:
                original_val = original_config.get(key)
                variation_val = variation_config.get(key)

                if original_val != variation_val:
                    differences[key] = {
                        'original': original_val,
                        'variation': variation_val,
                        'change_type': type(variation_val).__name__
                    }

        except Exception as e:
            self.logger.debug(f"Parameter difference extraction failed: {e}")

        return differences

    def _calculate_optimization_confidence(self, current_features: List[float], best_parameters: List[float]) -> float:
        """Calculate confidence in optimization recommendations."""
        try:
            if len(current_features) != len(best_parameters):
                return 0.1  # Low confidence for mismatched features

            # Calculate similarity between current and optimized parameters
            similarity = self._calculate_feature_similarity(current_features, best_parameters)

            # Calculate magnitude of changes
            total_change = sum(abs(best - current) for current, best in zip(current_features, best_parameters))
            avg_change = total_change / len(current_features) if len(current_features) > 0 else 0

            # Base confidence starts at 0.5
            base_confidence = 0.5

            # Adjust confidence based on change patterns
            if avg_change > 0.1:  # Significant changes made
                base_confidence += 0.2
            elif avg_change < 0.05:  # Minor changes
                base_confidence += 0.1

            # Factor in feature vector quality
            non_zero_features = len([f for f in current_features if f > 0])
            feature_completeness = non_zero_features / len(current_features)

            base_confidence += feature_completeness * 0.2

            # Check for model availability to boost confidence
            if 'payload_performance' in self.models:
                base_confidence += 0.1

            # Penalize if similarity is too high (no real optimization)
            if similarity > 0.95:
                base_confidence *= 0.7

            # Ensure confidence is in valid range
            final_confidence = max(0.0, min(1.0, base_confidence))

            return final_confidence

        except Exception as e:
            self.logger.debug(f"Optimization confidence calculation failed: {e}")
            return 0.5

    def _analyze_parameter_importance(self, current_features: List[float], best_parameters: List[float]) -> Dict[str, float]:
        """Analyze importance of different parameters for optimization."""
        importance_scores = {}

        try:
            if len(current_features) != len(best_parameters):
                self.logger.debug("Feature vector length mismatch in parameter importance analysis")
                return importance_scores

            # Calculate feature-wise improvements
            feature_improvements = []
            for i, (current, best) in enumerate(zip(current_features, best_parameters)):
                improvement = abs(best - current)
                feature_improvements.append((i, improvement))

            # Sort by improvement magnitude
            feature_improvements.sort(key=lambda x: x[1], reverse=True)

            # Map feature indices to parameter names and calculate importance scores
            feature_names = [
                'shellcode_length', 'nop_sled_length', 'buffer_size', 'encoding_flag',
                'polymorphic_flag', 'evasion_count', 'complexity_score', 'obfuscation_level',
                'execution_time', 'detection_risk', 'stealth_level', 'payload_entropy',
                'instruction_count', 'api_call_count', 'string_count', 'import_count',
                'section_count', 'relocation_count', 'symbol_count', 'debug_info'
            ]

            # Calculate normalized importance scores
            total_improvement = sum(imp for _, imp in feature_improvements)

            for i, (feature_idx, improvement) in enumerate(feature_improvements):
                if feature_idx < len(feature_names):
                    feature_name = feature_names[feature_idx]
                else:
                    feature_name = f'feature_{feature_idx}'

                # Normalize importance score (0-1)
                importance = improvement / total_improvement if total_improvement > 0 else 0.0

                # Weight by position (higher importance for top features)
                position_weight = 1.0 - (i / len(feature_improvements))
                weighted_importance = importance * position_weight

                importance_scores[feature_name] = weighted_importance

            # Add categorical analysis
            size_features = ['shellcode_length', 'nop_sled_length', 'buffer_size']
            binary_features = ['encoding_flag', 'polymorphic_flag']
            complexity_features = ['evasion_count', 'complexity_score', 'obfuscation_level']

            # Calculate category-level importance
            for category, features in [('size_params', size_features),
                                     ('binary_params', binary_features),
                                     ('complexity_params', complexity_features)]:
                category_importance = sum(importance_scores.get(feat, 0.0) for feat in features)
                importance_scores[f'{category}_total'] = category_importance

        except Exception as e:
            self.logger.debug(f"Parameter importance analysis failed: {e}")

        return importance_scores

    def _generate_optimization_recommendations(self, optimized_parameters: Dict[str, Any], expected_improvement: float) -> List[str]:
        """Generate optimization recommendations based on analysis."""
        recommendations = []

        try:
            # Extract feature changes and recommendations
            feature_changes = optimized_parameters.get('feature_changes', [])
            recommended_features = optimized_parameters.get('recommended_features', [])

            # Generate improvement-based recommendations
            if expected_improvement > 0.2:
                recommendations.append(f"High improvement potential: {expected_improvement:.2f} success rate increase")
                recommendations.append("Implement all suggested optimizations for maximum benefit")
            elif expected_improvement > 0.1:
                recommendations.append(f"Moderate improvement expected: {expected_improvement:.2f} success rate increase")
                recommendations.append("Consider implementing key optimizations")
            elif expected_improvement > 0.05:
                recommendations.append(f"Minor improvement possible: {expected_improvement:.2f} success rate increase")
                recommendations.append("Evaluate cost-benefit of optimization implementation")
            else:
                recommendations.append("Limited improvement potential detected")
                recommendations.append("Current configuration may already be near-optimal")

            # Analyze feature changes for specific recommendations
            if feature_changes and len(feature_changes) > 0:
                significant_changes = [i for i, change in enumerate(feature_changes) if abs(change) > 0.1]

                if significant_changes:
                    recommendations.append(f"Focus on optimizing {len(significant_changes)} key parameters")

                    # Map indices to parameter types for specific recommendations
                    param_names = ['payload_size', 'encoding_complexity', 'obfuscation_level',
                                 'evasion_techniques', 'stealth_level', 'execution_speed']

                    for idx in significant_changes[:3]:  # Top 3 changes
                        if idx < len(param_names):
                            change_magnitude = abs(feature_changes[idx])
                            if feature_changes[idx] > 0:
                                recommendations.append(f"Increase {param_names[idx]} by {change_magnitude:.2f}")
                            else:
                                recommendations.append(f"Reduce {param_names[idx]} by {change_magnitude:.2f}")

            # Add general optimization strategies based on parameter analysis
            if recommended_features and len(recommended_features) > 0:
                avg_feature_value = sum(recommended_features) / len(recommended_features)

                if avg_feature_value > 0.7:
                    recommendations.append("High-complexity optimization approach recommended")
                    recommendations.append("Use advanced evasion and encoding techniques")
                elif avg_feature_value < 0.3:
                    recommendations.append("Simplified optimization approach recommended")
                    recommendations.append("Focus on basic parameter tuning")
                else:
                    recommendations.append("Balanced optimization approach recommended")
                    recommendations.append("Combine moderate improvements across multiple parameters")

            # Add confidence-based qualifier
            if expected_improvement < 0.01:
                recommendations.append(" Low improvement confidence - manual review recommended")

        except Exception as e:
            self.logger.debug(f"Optimization recommendation generation failed: {e}")
            recommendations.append("Error generating specific recommendations - use default optimization")

        return recommendations

    def _extract_target_clustering_features(self, target_info: Dict[str, Any]) -> List[float]:
        """Extract features for target clustering analysis."""
        features = []

        try:
            # System features
            features.extend(self._extract_system_features(target_info))
            features.extend(self._extract_network_features(target_info))
            features.extend(self._extract_binary_features(target_info))

            # Additional clustering-specific features
            features.append(len(target_info.get('services', [])))
            features.append(len(target_info.get('vulnerabilities', [])))
            features.append(target_info.get('security_score', 5.0))
            features.append(target_info.get('patch_level', 5.0))

            # Normalize and pad
            while len(features) < 30:
                features.append(0.0)

        except Exception as e:
            self.logger.debug(f"Target clustering feature extraction failed: {e}")
            features = [0.0] * 30

        return features[:30]

    def _analyze_cluster_characteristics(self, cluster_labels: List[int],
                                       features_list: List[List[float]]) -> Dict[int, Dict[str, float]]:
        """Analyze characteristics of each cluster."""
        cluster_characteristics = {}

        try:
            unique_clusters = set(cluster_labels)

            for cluster_id in unique_clusters:
                cluster_indices = [i for i, label in enumerate(cluster_labels) if label == cluster_id]
                cluster_features = [features_list[i] for i in cluster_indices]

                if cluster_features:
                    # Calculate mean features for cluster
                    mean_features = [sum(feat[i] for feat in cluster_features) / len(cluster_features)
                                   for i in range(len(cluster_features[0]))]

                    cluster_characteristics[cluster_id] = {
                        'size': len(cluster_features),
                        'avg_security_score': mean_features[-2] if len(mean_features) > 1 else 0.0,
                        'avg_patch_level': mean_features[-1] if len(mean_features) > 0 else 0.0,
                        'feature_means': mean_features
                    }

        except Exception as e:
            self.logger.debug(f"Cluster characteristics analysis failed: {e}")

        return cluster_characteristics

    def _generate_cluster_optimization_strategy(self, characteristics: Dict[str, float], targets: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate optimization strategy for a specific cluster."""
        strategy = {
            'cluster_size': len(targets),
            'recommended_techniques': [],
            'risk_assessment': 'medium',
            'success_probability': 0.5,
            'target_analysis': {},
            'optimization_priority': 'medium'
        }

        try:
            security_score = characteristics.get('avg_security_score', 5.0)
            patch_level = characteristics.get('avg_patch_level', 5.0)
            cluster_size = characteristics.get('size', len(targets))

            # Analyze target diversity within cluster
            if targets:
                unique_os_types = set(t.get('target_data', {}).get('os_type', 'unknown') for t in targets)
                unique_architectures = set(t.get('target_data', {}).get('architecture', 'unknown') for t in targets)

                strategy['target_analysis'] = {
                    'os_diversity': len(unique_os_types),
                    'arch_diversity': len(unique_architectures),
                    'homogeneity_score': 1.0 - (len(unique_os_types) + len(unique_architectures)) / (2 * len(targets))
                }

                # Adjust strategy based on cluster homogeneity
                if strategy['target_analysis']['homogeneity_score'] > 0.8:
                    strategy['recommended_techniques'].append('standardized_payload')
                    strategy['optimization_priority'] = 'high'  # Homogeneous targets easier to optimize
                else:
                    strategy['recommended_techniques'].append('adaptive_payload')
                    strategy['optimization_priority'] = 'low'  # Diverse targets harder to optimize

            # Risk assessment based on security characteristics
            if security_score < 3.0 and patch_level < 3.0:
                strategy['risk_assessment'] = 'low'
                strategy['success_probability'] = 0.8
                strategy['recommended_techniques'].extend(['basic_exploitation', 'known_exploits'])

                # Low-security clusters get high priority
                if strategy['optimization_priority'] == 'medium':
                    strategy['optimization_priority'] = 'high'

            elif security_score > 7.0 or patch_level > 7.0:
                strategy['risk_assessment'] = 'high'
                strategy['success_probability'] = 0.3
                strategy['recommended_techniques'].extend(['zero_day', 'advanced_evasion', 'social_engineering'])

                # High-security clusters may not be worth the effort
                if cluster_size < 3:
                    strategy['optimization_priority'] = 'low'

            else:
                strategy['risk_assessment'] = 'medium'
                strategy['success_probability'] = 0.6
                strategy['recommended_techniques'].extend(['targeted_exploits', 'privilege_escalation'])

            # Adjust recommendations based on cluster size
            if cluster_size > 10:
                strategy['recommended_techniques'].append('mass_deployment')
                strategy['optimization_priority'] = 'high'  # Large clusters worth optimizing
            elif cluster_size < 3:
                strategy['recommended_techniques'].append('individual_targeting')
                # Small clusters get lower priority unless high-value
                if strategy['risk_assessment'] == 'low':
                    strategy['optimization_priority'] = 'medium'

            # Add resource efficiency recommendations
            efficiency_score = strategy['success_probability'] * cluster_size
            if efficiency_score > 5.0:
                strategy['recommended_techniques'].append('resource_efficient_approach')
            elif efficiency_score < 2.0:
                strategy['recommended_techniques'].append('minimal_resource_approach')

        except Exception as e:
            self.logger.debug(f"Cluster optimization strategy generation failed: {e}")

        return strategy

    def _get_top_features(self, feature_importance: Dict[str, float], top_n: int = 10) -> List[tuple]:
        """Get top N most important features."""
        try:
            sorted_features = sorted(feature_importance.items(),
                                   key=lambda x: x[1], reverse=True)
            return sorted_features[:top_n]
        except Exception as e:
            self.logger.debug(f"Top features extraction failed: {e}")
            return []

    def _analyze_adaptation_trends(self, adaptation_history: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze trends in adaptation performance over time."""
        trends = {
            'success_rate_trend': 'stable',
            'confidence_trend': 'stable',
            'strategy_effectiveness': {},
            'time_series_data': []
        }

        try:
            if len(adaptation_history) < 5:
                return trends

            # Analyze success rate trend
            recent_adaptations = adaptation_history[-20:]  # Last 20 adaptations
            success_rates = [adapt.get('success_rate', 0.5) for adapt in recent_adaptations]

            if len(success_rates) > 1:
                trend_slope = (success_rates[-1] - success_rates[0]) / len(success_rates)
                if trend_slope > 0.01:
                    trends['success_rate_trend'] = 'improving'
                elif trend_slope < -0.01:
                    trends['success_rate_trend'] = 'declining'

            # Analyze strategy effectiveness
            strategy_performance = {}
            for adaptation in recent_adaptations:
                strategy = adaptation.get('strategy', 'unknown')
                success_rate = adaptation.get('success_rate', 0.5)

                if strategy not in strategy_performance:
                    strategy_performance[strategy] = []
                strategy_performance[strategy].append(success_rate)

            for strategy, rates in strategy_performance.items():
                trends['strategy_effectiveness'][strategy] = sum(rates) / len(rates) if rates else 0.5

        except Exception as e:
            self.logger.debug(f"Adaptation trends analysis failed: {e}")

        return trends

    def _generate_strategic_recommendations(self, insights: Dict[str, Any],
                                          trends: Dict[str, Any]) -> List[str]:
        """Generate strategic recommendations based on analysis."""
        recommendations = []

        try:
            # Based on success rate trends
            if trends.get('success_rate_trend') == 'declining':
                recommendations.append("Success rate declining - review and update models")
                recommendations.append("Consider increasing training data collection")
            elif trends.get('success_rate_trend') == 'improving':
                recommendations.append("Success rate improving - continue current strategy")

            # Based on strategy effectiveness
            strategy_effectiveness = trends.get('strategy_effectiveness', {})
            if strategy_effectiveness:
                best_strategy = max(strategy_effectiveness.items(), key=lambda x: x[1])
                worst_strategy = min(strategy_effectiveness.items(), key=lambda x: x[1])

                recommendations.append(f"Most effective strategy: {best_strategy[0]} ({best_strategy[1]:.2f})")
                recommendations.append(f"Least effective strategy: {worst_strategy[0]} ({worst_strategy[1]:.2f})")

                if worst_strategy[1] < 0.4:
                    recommendations.append(f"Consider discontinuing or revising {worst_strategy[0]} strategy")

            # Model-specific recommendations
            if 'model_accuracy' in insights:
                accuracy = insights['model_accuracy']
                if isinstance(accuracy, dict):
                    for model_name, acc in accuracy.items():
                        if isinstance(acc, (int, float)) and acc < 0.7:
                            recommendations.append(f"Retrain {model_name} model - low accuracy ({acc:.2f})")

        except Exception as e:
            self.logger.debug(f"Strategic recommendation generation failed: {e}")

        return recommendations

    def _calculate_feature_similarity(self, features1: List[float], features2: List[float]) -> float:
        """Calculate similarity between two feature vectors."""
        try:
            if len(features1) != len(features2):
                return 0.0

            # Euclidean distance normalized to similarity
            distance = sum((f1 - f2) ** 2 for f1, f2 in zip(features1, features2)) ** 0.5
            max_distance = len(features1) ** 0.5  # Maximum possible distance

            similarity = 1.0 - (distance / max_distance) if max_distance > 0 else 0.0
            return max(0.0, min(1.0, similarity))

        except Exception as e:
            self.logger.debug(f"Feature similarity calculation failed: {e}")
            return 0.0
