"""Binary differ for Intellicrack vulnerability research.

This file is part of Intellicrack.
Copyright (C) 2025 Zachary Flint.

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see https://www.gnu.org/licenses/.
"""

import hashlib
import logging
import os
import shutil
import subprocess
import time
from enum import Enum
from typing import Any

from ...utils.analysis.analysis_stats import AnalysisStatsGenerator
from .base_analyzer import BaseAnalyzer
from .common_enums import SecurityRelevance

logger = logging.getLogger(__name__)

# Optional imports for advanced analysis
try:
    from intellicrack.handlers.capstone_handler import capstone

    CAPSTONE_AVAILABLE = True
except ImportError as e:
    logger.exception("Import error in binary_differ: %s", e)
    CAPSTONE_AVAILABLE = False

try:
    from intellicrack.handlers.lief_handler import HAS_LIEF, lief

    LIEF_AVAILABLE = HAS_LIEF
except ImportError as e:
    logger.exception("Import error in binary_differ: %s", e)
    LIEF_AVAILABLE = False
    HAS_LIEF = False
    lief = None  # type: ignore[assignment]

try:
    from intellicrack.handlers.pefile_handler import pefile

    PEFILE_AVAILABLE = True
except ImportError as e:
    logger.exception("Import error in binary_differ: %s", e)
    PEFILE_AVAILABLE = False


class DiffType(Enum):
    """Comprehensive enumeration of binary difference types for vulnerability research.

    This enumeration categorizes the various types of modifications that can occur
    between two binary files. Each difference type provides insights into potential
    security implications, code changes, and structural modifications.

    The available difference types include function changes (added/removed/modified),
    data modifications, import table changes, section alterations, and security
    feature modifications. Each type represents a potential security-relevant
    modification that warrants expert analysis.

    Note:
        - These difference types are crucial for security research and vulnerability assessment
        - Each type represents a potential security-relevant modification
        - Comprehensive analysis requires expert interpretation of these changes
        - Not all changes indicate a security risk, but they warrant careful investigation

    """

    FUNCTION_ADDED = "function_added"
    FUNCTION_REMOVED = "function_removed"
    FUNCTION_MODIFIED = "function_modified"
    DATA_CHANGED = "data_changed"
    IMPORT_ADDED = "import_added"
    IMPORT_REMOVED = "import_removed"
    SECTION_ADDED = "section_added"
    SECTION_REMOVED = "section_removed"
    SECTION_MODIFIED = "section_modified"
    SECURITY_FEATURE_ADDED = "security_feature_added"
    SECURITY_FEATURE_REMOVED = "security_feature_removed"


class BinaryDiffer(BaseAnalyzer):
    """Advanced binary diffing engine for vulnerability research."""

    def __init__(self) -> None:
        """Initialize the binary differ.

        Sets up the binary comparison and analysis system for vulnerability
        research. Configures security function patterns, mitigation detection,
        and analysis metrics for identifying security-relevant changes between
        binary versions.
        """
        super().__init__()
        self.logger = logging.getLogger("IntellicrackLogger.BinaryDiffer")

        # Security-relevant function patterns
        self.security_functions = {
            "memory": [
                "malloc",
                "free",
                "calloc",
                "realloc",
                "memcpy",
                "memset",
                "strcpy",
                "strcat",
                "sprintf",
            ],
            "crypto": ["encrypt", "decrypt", "hash", "random", "aes", "rsa", "sha", "md5"],
            "auth": ["login", "authenticate", "verify", "check", "validate", "authorize"],
            "network": ["socket", "connect", "bind", "listen", "send", "recv", "http", "ssl"],
            "file": ["fopen", "fread", "fwrite", "open", "read", "write", "chmod", "chown"],
            "process": ["exec", "system", "fork", "spawn", "thread", "process"],
        }

        # Security mitigations
        self.security_mitigations = {
            "stack_protection": ["__stack_chk_fail", "__stack_chk_guard"],
            "aslr": ["__randomize_layout", "ASLR"],
            "dep": ["NX", "DEP", "EXECUTE_PROTECT"],
            "cfg": ["__guard_dispatch", "CFG"],
            "cfi": ["__cfi_check", "__typeid"],
            "fortify": ["__fortify_function", "__builtin___memcpy_chk"],
        }

        # Vulnerability patterns
        self.vuln_patterns = {
            "buffer_overflow": [
                "strcpy",
                "strcat",
                "sprintf",
                "gets",
                "scanf",
                "memcpy_unsafe",
                "strncpy_unsafe",
            ],
            "format_string": ["printf", "sprintf", "fprintf", "snprintf"],
            "integer_overflow": ["add_overflow", "mul_overflow", "size_check"],
            "use_after_free": ["free", "delete", "use_after"],
            "double_free": ["double_free", "free_twice"],
            "null_deref": ["null_check", "deref_null"],
        }

        # Analysis cache
        self.analysis_cache: dict[str, Any] = {}

    def compare_binaries(self, old_binary: str, new_binary: str, analysis_level: str = "comprehensive") -> dict[str, Any]:
        """Perform a comprehensive comparison of two binary files to identify security-relevant differences.

        This method provides an in-depth analysis of binary changes, focusing on detecting modifications
        that could potentially introduce security vulnerabilities or impact the software's security posture.

        Args:
            old_binary (str): Absolute file path to the original binary file.
            new_binary (str): Absolute file path to the updated or modified binary file.
            analysis_level (str): Depth of analysis to perform. Determines the comprehensiveness
                of the comparison. Defaults to 'comprehensive'.

                - 'basic': Quick analysis of file size, hash, and basic section changes.
                - 'intermediate': Adds import table, string, and function signature comparisons.
                - 'comprehensive': Most detailed analysis, includes control flow, security mitigation,
                  and vulnerability pattern detection.

        Returns:
            dict[str, Any]: A comprehensive dictionary containing detailed analysis results with keys:
                - 'success': Boolean indicating successful analysis
                - 'old_binary': Path to the original binary
                - 'new_binary': Path to the updated binary
                - 'differences': List of detected binary differences
                - 'security_impact': Assessment of security implications
                - 'statistics': Quantitative analysis of changes
                - 'recommendations': Suggested actions based on analysis
                - 'analysis_time': Time taken for the entire analysis
                - 'error': Any error encountered during analysis

        Note:
            - Requires additional tools like objdump, LIEF, Capstone for comprehensive analysis
            - Performance and depth of analysis depend on available external tools
            - Analysis results are probabilistic and should be reviewed by security experts

        """
        result = {
            "success": False,
            "old_binary": old_binary,
            "new_binary": new_binary,
            "analysis_level": analysis_level,
            "differences": [],
            "security_impact": {},
            "statistics": {},
            "recommendations": [],
            "analysis_time": 0,
            "error": None,
        }

        start_time = time.time()

        try:
            self.logger.info("Comparing binaries: %s vs %s", old_binary, new_binary)

            # Validate input files
            if not os.path.exists(old_binary):
                result["error"] = f"Old binary not found: {old_binary}"
                return result

            if not os.path.exists(new_binary):
                result["error"] = f"New binary not found: {new_binary}"
                return result

            # Perform analysis based on level
            differences = []

            # Basic analysis - always performed
            basic_diffs = self._basic_analysis(old_binary, new_binary)
            differences.extend(basic_diffs)

            if analysis_level in {"intermediate", "comprehensive"}:
                # Intermediate analysis
                intermediate_diffs = self._intermediate_analysis(old_binary, new_binary)
                differences.extend(intermediate_diffs)

            if analysis_level == "comprehensive":
                # Comprehensive analysis
                comprehensive_diffs = self._comprehensive_analysis(old_binary, new_binary)
                differences.extend(comprehensive_diffs)

            # Analyze security impact
            security_impact = self._analyze_security_impact(differences)

            # Generate statistics
            statistics = self._generate_statistics(differences, old_binary, new_binary)

            # Generate recommendations
            recommendations = self._generate_recommendations(differences, security_impact)

            result["differences"] = differences
            result["security_impact"] = security_impact
            result["statistics"] = statistics
            result["recommendations"] = recommendations
            result["analysis_time"] = time.time() - start_time
            result["success"] = True

            self.logger.info("Binary comparison complete: %s differences found", len(differences))
            return result

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            logger.exception("Error in binary_differ: %s", e)
            return self.handle_analysis_error(result, e, start_time)

    def analyze_patch(self, patch_file: str, target_binary: str) -> dict[str, Any]:
        """Perform a comprehensive security analysis of a binary patch.

        This method examines a patch file and maps its potential security implications
        to a target binary, identifying possible modifications, vulnerabilities,
        and security-relevant changes.

        Args:
            patch_file (str): Absolute file path to the patch file to be analyzed.
            target_binary (str): Absolute file path to the binary that the patch targets.

        Returns:
            dict[str, Any]: A detailed dictionary containing patch analysis results with keys:
                - 'success': Boolean indicating successful patch analysis
                - 'patch_file': Path to the analyzed patch file
                - 'target_binary': Path to the target binary
                - 'patch_type': Classification of patch ('security_fix', 'regular', etc.)
                - 'security_fixes': List of potential security improvements
                - 'potential_vulnerabilities': List of potential security risks
                - 'impact_assessment': Detailed analysis of patch's security implications
                - 'affected_functions': Functions potentially modified by the patch
                - 'affected_sections': Binary sections impacted
                - 'binary_changes': Summary of binary modifications
                - 'error': Any error encountered during analysis

        Note:
            - Uses heuristic analysis to detect security-relevant patch content
            - Performs static analysis without actually applying the patch
            - Analysis depends on available system tools and libraries
            - Results are probabilistic and require expert review
            - Supports various patch formats (unified diff, binary patches)

        """
        result = self.create_analysis_result(
            patch_file=patch_file,
            target_binary=target_binary,
            patch_type=None,
            security_fixes=[],
            potential_vulnerabilities=[],
            impact_assessment={},
        )

        try:
            self.logger.info("Analyzing patch: %s", patch_file)

            if not os.path.exists(patch_file):
                result["error"] = f"Patch file not found: {patch_file}"
                return result

            # Read and parse patch
            with open(patch_file) as f:
                patch_content = f.read()

            # Analyze patch content
            patch_analysis = self._analyze_patch_content(patch_content)

            # Map to binary changes if target provided
            if target_binary and os.path.exists(target_binary):
                binary_impact = self._map_patch_to_binary(patch_content, target_binary)
                result.update(binary_impact)

            result.update(patch_analysis)
            result["success"] = True

            self.logger.info("Patch analysis complete")
            return result

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.exception("Patch analysis failed: %s", e)
            result["error"] = str(e)
            return result

    def find_similar_functions(self, binary_path: str, target_function: bytes, similarity_threshold: float = 0.8) -> list[dict[str, Any]]:
        """Locate and rank functions within a binary that are structurally similar to a target function.

        This method performs an advanced bytecode-level comparison of functions in a binary,
        identifying potential code reuse, library dependencies, or code cloning scenarios.
        Useful for vulnerability research, understanding code evolution, and detecting
        potential security risks arising from code similarities.

        Args:
            binary_path (str): Absolute file path to the binary to search for similar functions.
            target_function (bytes): Raw bytecode of the function to compare against.
            similarity_threshold (float): Minimum similarity score for considering
                a function a match. Range is 0.0 (no similarity) to 1.0 (exact match).
                Defaults to 0.8 for high structural similarity.

        Returns:
            list[dict[str, Any]]: A list of dictionaries representing similar functions, sorted
                by similarity in descending order. Each dictionary contains:

                - 'address': Memory address of the similar function
                - 'similarity': Numeric similarity score (0.0 - 1.0)
                - 'size': Size of the function in bytes
                - 'name': Function name (or generated identifier)
                - 'analysis': Additional security-relevant function metadata

        Note:
            * Uses byte-level comparison for function similarity
            * Performance depends on binary size and number of functions
            * Similarity calculation considers byte sequence matches
            * Recommended for vulnerability research and code analysis
            * May have different performance characteristics for different binary types
            * Requires external tools like objdump for function extraction

        """
        similar_functions = []

        try:
            self.logger.info("Finding similar functions in %s", binary_path)

            # Extract functions from binary
            functions = self._extract_functions(binary_path)

            # Compare each function to target
            for func_addr, func_data in functions.items():
                similarity = self._calculate_function_similarity(target_function, func_data["bytes"])

                if similarity >= similarity_threshold:
                    similar_functions.append(
                        {
                            "address": func_addr,
                            "similarity": similarity,
                            "size": len(func_data["bytes"]),
                            "name": func_data.get("name", f"sub_{func_addr:x}"),
                            "analysis": func_data.get("analysis", {}),
                        },
                    )

            # Sort by similarity
            similar_functions.sort(key=lambda x: x["similarity"], reverse=True)

            self.logger.info("Found %s similar functions", len(similar_functions))
            return similar_functions

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.exception("Similar function search failed: %s", e)
            return similar_functions

    def _basic_analysis(self, old_binary: str, new_binary: str) -> list[dict[str, Any]]:
        """Perform basic binary analysis.

        Compares file sizes, checksums, and section information between two binaries.

        Args:
            old_binary: Path to the original binary file.
            new_binary: Path to the modified binary file.

        Returns:
            list[dict[str, Any]]: List of detected differences with type and description.
        """
        differences = []

        try:
            # File size comparison
            old_size = os.path.getsize(old_binary)
            new_size = os.path.getsize(new_binary)

            if old_size != new_size:
                differences.append(
                    {
                        "type": DiffType.DATA_CHANGED.value,
                        "description": f"File size changed: {old_size} -> {new_size}",
                        "security_relevance": SecurityRelevance.LOW.value,
                        "old_value": old_size,
                        "new_value": new_size,
                    },
                )

            # Hash comparison
            old_hash = self._calculate_file_hash(old_binary)
            new_hash = self._calculate_file_hash(new_binary)

            if old_hash != new_hash:
                differences.append(
                    {
                        "type": DiffType.DATA_CHANGED.value,
                        "description": "Binary content changed",
                        "security_relevance": SecurityRelevance.MEDIUM.value,
                        "old_value": old_hash,
                        "new_value": new_hash,
                    },
                )

            # Section comparison using hexdump
            old_sections = self._get_basic_sections(old_binary)
            new_sections = self._get_basic_sections(new_binary)

            # Compare sections
            for section_name in set(old_sections) | set(new_sections):
                if section_name in old_sections and section_name not in new_sections:
                    differences.append(
                        {
                            "type": DiffType.SECTION_REMOVED.value,
                            "description": f"Section removed: {section_name}",
                            "security_relevance": SecurityRelevance.MEDIUM.value,
                            "section": section_name,
                        },
                    )
                elif section_name not in old_sections and section_name in new_sections:
                    differences.append(
                        {
                            "type": DiffType.SECTION_ADDED.value,
                            "description": f"Section added: {section_name}",
                            "security_relevance": SecurityRelevance.MEDIUM.value,
                            "section": section_name,
                        },
                    )
                elif section_name in old_sections:
                    old_content = old_sections[section_name]
                    new_content = new_sections[section_name]

                    if old_content != new_content:
                        # Calculate content change metrics
                        old_size = len(old_content) if isinstance(old_content, str) else 0
                        new_size = len(new_content) if isinstance(new_content, str) else 0
                        size_change = new_size - old_size

                        differences.append(
                            {
                                "type": DiffType.SECTION_MODIFIED.value,
                                "description": f"Section modified: {section_name} (size change: {size_change:+d})",
                                "security_relevance": SecurityRelevance.MEDIUM.value,
                                "section": section_name,
                                "old_size": old_size,
                                "new_size": new_size,
                                "size_change": size_change,
                            },
                        )

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Basic analysis error: %s", e)

        return differences

    def _intermediate_analysis(self, old_binary: str, new_binary: str) -> list[dict[str, Any]]:
        """Perform intermediate binary analysis.

        Analyzes import tables, string changes, and function signatures between binaries.

        Args:
            old_binary: Path to the original binary file.
            new_binary: Path to the modified binary file.

        Returns:
            list[dict[str, Any]]: List of detected intermediate-level differences.
        """
        differences = []

        try:
            # Import table comparison
            old_imports = self._extract_imports(old_binary)
            new_imports = self._extract_imports(new_binary)

            # Find import differences
            old_import_set = set(old_imports)
            new_import_set = set(new_imports)

            added_imports = new_import_set - old_import_set
            removed_imports = old_import_set - new_import_set

            for import_name in added_imports:
                security_relevance = self._assess_import_security_relevance(import_name)
                differences.append(
                    {
                        "type": DiffType.IMPORT_ADDED.value,
                        "description": f"Import added: {import_name}",
                        "security_relevance": security_relevance.value,
                        "import": import_name,
                    },
                )

            for import_name in removed_imports:
                security_relevance = self._assess_import_security_relevance(import_name)
                differences.append(
                    {
                        "type": DiffType.IMPORT_REMOVED.value,
                        "description": f"Import removed: {import_name}",
                        "security_relevance": security_relevance.value,
                        "import": import_name,
                    },
                )

            # String comparison
            old_strings = self._extract_strings(old_binary)
            new_strings = self._extract_strings(new_binary)

            string_diffs = self._compare_strings(old_strings, new_strings)
            differences.extend(string_diffs)

            # Function signature analysis (if tools available)
            if CAPSTONE_AVAILABLE:
                func_diffs = self._compare_function_signatures(old_binary, new_binary)
                differences.extend(func_diffs)

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Intermediate analysis error: %s", e)

        return differences

    def _comprehensive_analysis(self, old_binary: str, new_binary: str) -> list[dict[str, Any]]:
        """Perform comprehensive binary analysis.

        Analyzes control flow, security mitigations, vulnerability patterns, and binary structure.

        Args:
            old_binary: Path to the original binary file.
            new_binary: Path to the modified binary file.

        Returns:
            list[dict[str, Any]]: List of detected comprehensive differences.
        """
        differences = []

        try:
            # Control flow analysis
            if CAPSTONE_AVAILABLE:
                cfg_diffs = self._compare_control_flow(old_binary, new_binary)
                differences.extend(cfg_diffs)

            # Security mitigation analysis
            mitigation_diffs = self._compare_security_mitigations(old_binary, new_binary)
            differences.extend(mitigation_diffs)

            # Vulnerability pattern analysis
            vuln_diffs = self._compare_vulnerability_patterns(old_binary, new_binary)
            differences.extend(vuln_diffs)

            # Advanced binary structure analysis
            if LIEF_AVAILABLE:
                structure_diffs = self._compare_binary_structure(old_binary, new_binary)
                differences.extend(structure_diffs)

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Comprehensive analysis error: %s", e)

        return differences

    def _analyze_security_impact(self, differences: list[dict[str, Any]]) -> dict[str, Any]:
        """Analyze the security impact of identified differences.

        Categorizes differences, identifies critical changes, and assesses overall risk.

        Args:
            differences: List of binary differences to analyze.

        Returns:
            dict[str, Any]: Security impact assessment with risk levels and categorization.
        """
        impact = {
            "overall_risk": SecurityRelevance.LOW.value,
            "categories": {},
            "critical_changes": [],
            "mitigation_changes": [],
            "vulnerability_indicators": [],
        }

        try:
            # Categorize differences
            categories: dict[str, int] = {}
            critical_changes: list[dict[str, Any]] = []
            mitigation_changes: list[dict[str, Any]] = []
            vulnerability_indicators: list[dict[str, Any]] = []

            for diff in differences:
                diff_type = diff["type"]
                security_level = diff["security_relevance"]

                # Count by category
                if diff_type not in categories:
                    categories[diff_type] = 0
                categories[diff_type] += 1

                # Identify critical changes
                if security_level in ["critical", "high"]:
                    critical_changes.append(diff)

                # Identify mitigation changes
                if "mitigation" in diff.get("description", "").lower():
                    mitigation_changes.append(diff)

                # Identify vulnerability indicators
                if any(vuln in diff.get("description", "").lower() for vuln in ["overflow", "underflow", "injection", "bypass"]):
                    vulnerability_indicators.append(diff)

            # Determine overall risk
            if critical_changes:
                impact["overall_risk"] = SecurityRelevance.CRITICAL.value
            elif len([d for d in differences if d["security_relevance"] == "high"]) > 3:
                impact["overall_risk"] = SecurityRelevance.HIGH.value
            elif len([d for d in differences if d["security_relevance"] == "medium"]) > 5:
                impact["overall_risk"] = SecurityRelevance.MEDIUM.value

            impact["categories"] = categories
            impact["critical_changes"] = critical_changes  # type: ignore[assignment]
            impact["mitigation_changes"] = mitigation_changes  # type: ignore[assignment]
            impact["vulnerability_indicators"] = vulnerability_indicators  # type: ignore[assignment]

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Security impact analysis error: %s", e)

        return impact

    def _generate_statistics(self, differences: list[dict[str, Any]], old_binary: str, new_binary: str) -> dict[str, Any]:
        """Generate statistics about the binary comparison.

        Computes quantitative metrics about differences between binaries.

        Args:
            differences: List of detected binary differences.
            old_binary: Path to the original binary file.
            new_binary: Path to the modified binary file.

        Returns:
            dict[str, Any]: Statistics including counts by type, security level, and file info.
        """

        def _compute_stats() -> dict[str, Any]:
            stats = {
                "total_differences": len(differences),
                "by_type": AnalysisStatsGenerator.count_by_attribute(differences, "type"),
                "by_security_level": AnalysisStatsGenerator.count_by_attribute(differences, "security_relevance"),
                "file_info": {},
                "change_summary": {},
            }

            # File information
            stats["file_info"] = {
                "old_binary": {
                    "path": old_binary,
                    "size": os.path.getsize(old_binary),
                    "hash": self._calculate_file_hash(old_binary),
                },
                "new_binary": {
                    "path": new_binary,
                    "size": os.path.getsize(new_binary),
                    "hash": self._calculate_file_hash(new_binary),
                },
            }

            # Change summary
            stats["change_summary"] = {
                "additions": len([d for d in differences if "added" in d["type"]]),
                "removals": len([d for d in differences if "removed" in d["type"]]),
                "modifications": len([d for d in differences if "modified" in d["type"]]),
                "security_relevant": len([d for d in differences if d["security_relevance"] in ["high", "critical"]]),
            }

            return stats

        # Use safe statistics generation
        result = AnalysisStatsGenerator.safe_stats_generation(_compute_stats)
        if not isinstance(result, dict):
            # Return default stats on error
            return {
                "total_differences": 0,
                "by_type": {},
                "by_security_level": {},
                "file_info": {},
                "change_summary": {},
            }
        return result

    def _generate_recommendations(self, differences: list[dict[str, Any]], security_impact: dict[str, Any]) -> list[str]:
        """Generate recommendations based on analysis results.

        Produces actionable security recommendations from analysis findings.

        Args:
            differences: List of detected binary differences.
            security_impact: Security impact assessment from analysis.

        Returns:
            list[str]: List of recommended security actions.
        """

        def _compute_recommendations() -> list[str]:
            recommendations = []

            # Critical changes recommendations
            if security_impact["critical_changes"]:
                recommendations.append("CRITICAL: Review all critical security changes immediately")
                recommendations.append("Perform thorough security testing before deployment")

            # Mitigation changes
            if security_impact["mitigation_changes"]:
                recommendations.append("Review security mitigation changes for potential bypasses")

            # Vulnerability indicators
            if security_impact["vulnerability_indicators"]:
                recommendations.append("Investigate potential vulnerability indicators")
                recommendations.append("Consider additional security testing and code review")

            # Import changes
            import_changes = [d for d in differences if "import" in d["type"]]
            if import_changes:
                recommendations.append("Review import table changes for security implications")

            # Function changes
            function_changes = [d for d in differences if "function" in d["type"]]
            if function_changes:
                recommendations.append("Analyze function changes for logic vulnerabilities")

            # General recommendations
            if len(differences) > 20:
                recommendations.append("Large number of changes detected - consider staged testing")

            if not recommendations:
                recommendations.append("Changes appear minimal - standard testing recommended")

            return recommendations

        # Use safe recommendation generation
        return AnalysisStatsGenerator.safe_recommendation_generation(_compute_recommendations)

    # Helper methods for binary analysis

    def _calculate_file_hash(self, file_path: str) -> str:
        """Calculate SHA256 hash of file.

        Computes the SHA256 checksum of a binary file for integrity verification.

        Args:
            file_path: Path to the file to hash.

        Returns:
            str: Hexadecimal representation of SHA256 hash, or empty string on error.
        """
        try:
            hasher = hashlib.sha256()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except (OSError, ValueError, RuntimeError) as e:
            self.logger.exception("Error in binary_differ: %s", e)
            return ""

    def _get_basic_sections(self, binary_path: str) -> dict[str, str]:
        """Extract basic section information using hexdump.

        Retrieves binary section names and content using objdump or file inspection.

        Args:
            binary_path: Path to the binary file.

        Returns:
            dict[str, str]: Dictionary mapping section names to their hex content.
        """
        sections = {}

        try:
            if objdump_path := shutil.which("objdump"):
                result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis
                    [objdump_path, "-h", binary_path],
                    check=False,
                    capture_output=True,
                    text=True,
                )
            else:
                result = None

            if result and result.returncode == 0:
                lines = result.stdout.split("\n")
                for line in lines:
                    if "CONTENTS" in line:
                        parts = line.split()
                        if len(parts) >= 2:
                            section_name = parts[1]
                            sections[section_name] = line.strip()
            else:
                # Fallback to basic file analysis
                with open(binary_path, "rb") as f:
                    data = f.read(1024)  # Read first 1KB
                    sections["header"] = data.hex()

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Section extraction error: %s", e)

        return sections

    def _extract_imports(self, binary_path: str) -> list[str]:
        """Extract import table from binary.

        Retrieves imported libraries and symbols from the binary using objdump or nm.

        Args:
            binary_path: Path to the binary file.

        Returns:
            list[str]: List of unique imported function/library names.
        """
        imports = []

        try:
            if objdump_path := shutil.which("objdump"):
                result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis
                    [objdump_path, "-T", binary_path],
                    check=False,
                    capture_output=True,
                    text=True,
                )
            else:
                result = None

            if result and result.returncode == 0:
                lines = result.stdout.split("\n")
                for line in lines:
                    if "DF" in line or "*UND*" in line:
                        parts = line.split()
                        if len(parts) >= 7:
                            import_name = parts[-1]
                            imports.append(import_name)
            else:
                if nm_path := shutil.which("nm"):
                    result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis
                        [nm_path, "-D", binary_path],
                        check=False,
                        capture_output=True,
                        text=True,
                    )
                else:
                    result = None
                if result and result.returncode == 0:
                    lines = result.stdout.split("\n")
                    for line in lines:
                        if "U " in line:
                            parts = line.split()
                            if len(parts) >= 2:
                                imports.append(parts[-1])

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Import extraction error: %s", e)

        return list(set(imports))  # Remove duplicates

    def _extract_strings(self, binary_path: str) -> list[str]:
        """Extract strings from binary.

        Retrieves readable string constants from the binary file.

        Args:
            binary_path: Path to the binary file.

        Returns:
            list[str]: List of extracted strings with minimum length of 4 characters.
        """
        strings = []

        try:
            if strings_path := shutil.which("strings"):
                result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis
                    [strings_path, "-n", "4", binary_path],
                    check=False,
                    capture_output=True,
                    text=True,
                )
            else:
                result = None

            if result and result.returncode == 0:
                strings = result.stdout.strip().split("\n")
                # Filter for meaningful strings
                strings = [s for s in strings if len(s) >= 4 and len(s) <= 100]

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("String extraction error: %s", e)

        return strings

    def _compare_strings(self, old_strings: list[str], new_strings: list[str]) -> list[dict[str, Any]]:
        """Compare string tables between binaries.

        Identifies added and removed strings with security keyword matching.

        Args:
            old_strings: List of strings from the original binary.
            new_strings: List of strings from the modified binary.

        Returns:
            list[dict[str, Any]]: List of string differences with security relevance.
        """
        differences = []

        try:
            old_set = set(old_strings)
            new_set = set(new_strings)

            added_strings = new_set - old_set
            removed_strings = old_set - new_set

            # Look for security-relevant strings
            security_keywords = ["password", "secret", "key", "token", "admin", "root", "auth"]

            # Analyze added strings for security impact
            for string in added_strings:
                security_relevance = SecurityRelevance.LOW

                # Enhance security analysis with keyword matching
                string_lower = string.lower()
                matched_keywords = [kw for kw in security_keywords if kw in string_lower]

                if matched_keywords:
                    security_relevance = SecurityRelevance.MEDIUM
                    # Log matched security keywords for investigation
                    self.logger.info("Added string contains security keywords %s: %s...", matched_keywords, string[:30])

                differences.append(
                    {
                        "type": "string_added",
                        "description": f"String added: {string[:50]}...",
                        "security_relevance": security_relevance.value,
                        "string": string,
                        "matched_keywords": matched_keywords,
                    },
                )

            # Analyze removed strings for security impact
            for string in removed_strings:
                security_relevance = SecurityRelevance.LOW

                # Enhance security analysis with keyword matching
                string_lower = string.lower()
                matched_keywords = [kw for kw in security_keywords if kw in string_lower]

                if matched_keywords:
                    security_relevance = SecurityRelevance.MEDIUM
                    # Log matched security keywords for investigation
                    self.logger.info("Removed string contains security keywords %s: %s...", matched_keywords, string[:30])

                differences.append(
                    {
                        "type": "string_removed",
                        "description": f"String removed: {string[:50]}...",
                        "security_relevance": security_relevance.value,
                        "string": string,
                        "matched_keywords": matched_keywords,
                    },
                )

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("String comparison error: %s", e)

        return differences

    def _assess_import_security_relevance(self, import_name: str) -> SecurityRelevance:
        """Assess security relevance of an import.

        Evaluates the security significance of an imported library or function.

        Args:
            import_name: Name of the imported function or library.

        Returns:
            SecurityRelevance: Security relevance level (LOW, MEDIUM, HIGH, CRITICAL).
        """
        import_lower = import_name.lower()

        # Check against security function categories
        for category, functions in self.security_functions.items():
            if any(func in import_lower for func in functions):
                if category in ["crypto", "auth"]:
                    return SecurityRelevance.HIGH
                else:
                    return SecurityRelevance.MEDIUM if category in ["memory", "network"] else SecurityRelevance.LOW
        return SecurityRelevance.LOW

    def _assess_function_security_relevance(self, function_name: str) -> SecurityRelevance:
        """Assess security relevance of a function based on its name.

        Evaluates the security significance of a function through pattern matching.

        Args:
            function_name: Name of the function to assess.

        Returns:
            SecurityRelevance: Security relevance level (LOW, MEDIUM, HIGH, CRITICAL).
        """
        func_lower = function_name.lower()

        # Check against security function categories
        for category, functions in self.security_functions.items():
            if any(func in func_lower for func in functions):
                if category in ["crypto", "auth"]:
                    return SecurityRelevance.HIGH
                else:
                    return SecurityRelevance.MEDIUM if category in ["memory", "network", "process"] else SecurityRelevance.LOW
        # Check for vulnerability patterns
        for patterns in self.vuln_patterns.values():
            if any(pattern in func_lower for pattern in patterns):
                return SecurityRelevance.HIGH

        return next(
            (
                SecurityRelevance.MEDIUM
                for _mitigation, indicators in self.security_mitigations.items()
                if any(indicator.lower() in func_lower for indicator in indicators)
            ),
            SecurityRelevance.LOW,
        )

    def _compare_function_signatures(self, old_binary: str, new_binary: str) -> list[dict[str, Any]]:
        """Compare function signatures using Capstone.

        Identifies added, removed, and modified functions between binaries.

        Args:
            old_binary: Path to the original binary file.
            new_binary: Path to the modified binary file.

        Returns:
            list[dict[str, Any]]: List of function signature differences.
        """
        differences: list[dict[str, Any]] = []

        if not CAPSTONE_AVAILABLE:
            self.logger.debug("Capstone not available for function signature comparison of %s vs %s", old_binary, new_binary)
            return differences

        try:
            # Log the binaries being compared
            self.logger.debug("Comparing function signatures: %s vs %s", old_binary, new_binary)

            # Extract functions from both binaries
            old_functions = self._extract_functions(old_binary)
            new_functions = self._extract_functions(new_binary)

            # Compare function counts and basic metrics
            if len(old_functions) != len(new_functions):
                differences.append(
                    {
                        "type": DiffType.FUNCTION_MODIFIED.value,
                        "description": f"Function count changed: {len(old_functions)} -> {len(new_functions)}",
                        "security_relevance": SecurityRelevance.MEDIUM.value,
                        "old_count": len(old_functions),
                        "new_count": len(new_functions),
                    },
                )

            # Find added/removed functions
            old_names = {f.get("name", f"sub_{addr:x}") for addr, f in old_functions.items()}
            new_names = {f.get("name", f"sub_{addr:x}") for addr, f in new_functions.items()}

            added_funcs = new_names - old_names
            removed_funcs = old_names - new_names

            for func_name in added_funcs:
                security_relevance = self._assess_function_security_relevance(func_name)
                differences.append(
                    {
                        "type": DiffType.FUNCTION_ADDED.value,
                        "description": f"Function added: {func_name}",
                        "security_relevance": security_relevance.value,
                        "function": func_name,
                    },
                )

            for func_name in removed_funcs:
                security_relevance = self._assess_function_security_relevance(func_name)
                differences.append(
                    {
                        "type": DiffType.FUNCTION_REMOVED.value,
                        "description": f"Function removed: {func_name}",
                        "security_relevance": security_relevance.value,
                        "function": func_name,
                    },
                )

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Function signature comparison error for %s vs %s: %s", old_binary, new_binary, e)

        return differences

    def _compare_control_flow(self, old_binary: str, new_binary: str) -> list[dict[str, Any]]:
        """Compare control flow graphs.

        Analyzes and compares control flow patterns including jumps, conditions, and loops.

        Args:
            old_binary: Path to the original binary file.
            new_binary: Path to the modified binary file.

        Returns:
            list[dict[str, Any]]: List of control flow differences with security impact.
        """
        differences = []

        try:
            self.logger.debug("Comparing control flow: %s vs %s", old_binary, new_binary)

            # If Capstone is available, use it for detailed control flow analysis
            if CAPSTONE_AVAILABLE:
                capstone_diffs = self._compare_control_flow_with_capstone(old_binary, new_binary)
                differences.extend(capstone_diffs)

            # Basic control flow indicators through string analysis (fallback/additional)
            old_strings = self._extract_strings(old_binary)
            new_strings = self._extract_strings(new_binary)

            # Look for control flow related patterns
            control_flow_patterns = {
                "jumps": ["jmp", "je", "jne", "jz", "jnz", "call", "ret"],
                "conditions": ["if", "else", "switch", "case", "cmp", "test"],
                "loops": ["for", "while", "loop", "jmp"],
                "error_handlers": ["catch", "except", "finally", "error", "fail"],
            }

            old_cf_indicators = dict.fromkeys(control_flow_patterns, 0)
            new_cf_indicators = dict.fromkeys(control_flow_patterns, 0)

            # Count control flow indicators
            for string in old_strings:
                for category, patterns in control_flow_patterns.items():
                    if any(pattern in string.lower() for pattern in patterns):
                        old_cf_indicators[category] += 1

            for string in new_strings:
                for category, patterns in control_flow_patterns.items():
                    if any(pattern in string.lower() for pattern in patterns):
                        new_cf_indicators[category] += 1

            # Compare indicators
            for category in control_flow_patterns:
                if old_cf_indicators[category] != new_cf_indicators[category]:
                    change = new_cf_indicators[category] - old_cf_indicators[category]
                    differences.append(
                        {
                            "type": "control_flow_change",
                            "description": f"Control flow {category} changed by {change:+d} ({old_cf_indicators[category]} -> {new_cf_indicators[category]})",
                            "security_relevance": SecurityRelevance.MEDIUM.value if abs(change) > 5 else SecurityRelevance.LOW.value,
                            "category": category,
                            "old_count": old_cf_indicators[category],
                            "new_count": new_cf_indicators[category],
                        },
                    )

            # If we have significant changes in error handlers, that's security relevant
            if abs(new_cf_indicators["error_handlers"] - old_cf_indicators["error_handlers"]) > 3:
                differences.append(
                    {
                        "type": "error_handling_change",
                        "description": "Significant change in error handling detected",
                        "security_relevance": SecurityRelevance.HIGH.value,
                    },
                )

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Control flow comparison error for %s vs %s: %s", old_binary, new_binary, e)

        return differences

    def _compare_security_mitigations(self, old_binary: str, new_binary: str) -> list[dict[str, Any]]:
        """Compare security mitigations between binaries.

        Detects changes in security features like stack protection, ASLR, and DEP.

        Args:
            old_binary: Path to the original binary file.
            new_binary: Path to the modified binary file.

        Returns:
            list[dict[str, Any]]: List of security mitigation changes with critical relevance.
        """
        differences = []

        try:
            old_mitigations = self._detect_security_mitigations(old_binary)
            new_mitigations = self._detect_security_mitigations(new_binary)

            # Compare mitigation presence
            for mitigation in self.security_mitigations:
                old_present = old_mitigations.get(mitigation, False)
                new_present = new_mitigations.get(mitigation, False)

                if old_present and not new_present:
                    differences.append(
                        {
                            "type": DiffType.SECURITY_FEATURE_REMOVED.value,
                            "description": f"Security mitigation removed: {mitigation}",
                            "security_relevance": SecurityRelevance.CRITICAL.value,
                            "mitigation": mitigation,
                        },
                    )
                elif not old_present and new_present:
                    differences.append(
                        {
                            "type": DiffType.SECURITY_FEATURE_ADDED.value,
                            "description": f"Security mitigation added: {mitigation}",
                            "security_relevance": SecurityRelevance.HIGH.value,
                            "mitigation": mitigation,
                        },
                    )

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Security mitigation comparison error: %s", e)

        return differences

    def _detect_security_mitigations(self, binary_path: str) -> dict[str, bool]:
        """Detect security mitigations in binary.

        Checks for presence of security features by scanning strings and imports.

        Args:
            binary_path: Path to the binary file to analyze.

        Returns:
            dict[str, bool]: Dictionary mapping mitigation types to their presence status.
        """
        mitigations = {}

        try:
            # Extract strings and symbols
            strings = self._extract_strings(binary_path)
            imports = self._extract_imports(binary_path)

            all_symbols = strings + imports
            all_text = " ".join(all_symbols).lower()

            # Check for each mitigation
            for mitigation, indicators in self.security_mitigations.items():
                mitigations[mitigation] = any(indicator.lower() in all_text for indicator in indicators)

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Mitigation detection error: %s", e)

        return mitigations

    def _compare_vulnerability_patterns(self, old_binary: str, new_binary: str) -> list[dict[str, Any]]:
        """Compare vulnerability patterns between binaries.

        Identifies changes in vulnerability-related function calls and patterns.

        Args:
            old_binary: Path to the original binary file.
            new_binary: Path to the modified binary file.

        Returns:
            list[dict[str, Any]]: List of vulnerability pattern changes with relevance.
        """
        differences = []

        try:
            old_patterns = self._detect_vulnerability_patterns(old_binary)
            new_patterns = self._detect_vulnerability_patterns(new_binary)

            # Compare pattern presence
            for vuln_type in self.vuln_patterns:
                old_count = old_patterns.get(vuln_type, 0)
                new_count = new_patterns.get(vuln_type, 0)

                if new_count > old_count:
                    differences.append(
                        {
                            "type": "vulnerability_pattern_increased",
                            "description": f"Potential {vuln_type} patterns increased: {old_count} -> {new_count}",
                            "security_relevance": SecurityRelevance.HIGH.value,
                            "vulnerability_type": vuln_type,
                            "old_count": old_count,
                            "new_count": new_count,
                        },
                    )
                elif new_count < old_count:
                    differences.append(
                        {
                            "type": "vulnerability_pattern_decreased",
                            "description": f"Potential {vuln_type} patterns decreased: {old_count} -> {new_count}",
                            "security_relevance": SecurityRelevance.MEDIUM.value,
                            "vulnerability_type": vuln_type,
                            "old_count": old_count,
                            "new_count": new_count,
                        },
                    )

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Vulnerability pattern comparison error: %s", e)

        return differences

    def _detect_vulnerability_patterns(self, binary_path: str) -> dict[str, int]:
        """Detect vulnerability patterns in binary.

        Counts occurrences of vulnerability-related patterns in binary.

        Args:
            binary_path: Path to the binary file to analyze.

        Returns:
            dict[str, int]: Dictionary mapping vulnerability types to occurrence counts.
        """
        patterns = {}

        try:
            # Extract strings and imports
            strings = self._extract_strings(binary_path)
            imports = self._extract_imports(binary_path)

            all_symbols = strings + imports
            all_text = " ".join(all_symbols).lower()

            # Count patterns for each vulnerability type
            for vuln_type, pattern_list in self.vuln_patterns.items():
                count = sum(all_text.count(pattern.lower()) for pattern in pattern_list)
                patterns[vuln_type] = count

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Vulnerability pattern detection error: %s", e)

        return patterns

    def _compare_binary_structure(self, old_binary: str, new_binary: str) -> list[dict[str, Any]]:
        """Compare binary structure using LIEF.

        Performs deep analysis of binary headers, sections, and security features.

        Args:
            old_binary: Path to the original binary file.
            new_binary: Path to the modified binary file.

        Returns:
            list[dict[str, Any]]: List of binary structure differences with security implications.
        """
        differences = []

        if not LIEF_AVAILABLE:
            self.logger.debug("LIEF not available for binary structure comparison of %s vs %s", old_binary, new_binary)
            # Fallback to basic structure analysis
            try:
                # Compare file headers
                with open(old_binary, "rb") as f:
                    old_header = f.read(1024)
                with open(new_binary, "rb") as f:
                    new_header = f.read(1024)

                # Check for PE/ELF/Mach-O signatures
                if old_header[:2] != new_header[:2]:
                    differences.append(
                        {
                            "type": "binary_format_change",
                            "description": "Binary format signature changed",
                            "security_relevance": SecurityRelevance.HIGH.value,
                        },
                    )

                # Check for architecture changes (basic heuristic)
                if old_header[4:8] != new_header[4:8]:
                    differences.append(
                        {
                            "type": "architecture_change",
                            "description": "Binary architecture indicators changed",
                            "security_relevance": SecurityRelevance.MEDIUM.value,
                        },
                    )

            except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
                self.logger.debug("Fallback binary structure analysis error: %s", e)
            return differences

        try:
            self.logger.info("Comparing binary structure using LIEF: %s vs %s", old_binary, new_binary)

            # Parse binaries with LIEF
            if not hasattr(lief, "parse"):
                self.logger.warning("lief.parse not available")
                return differences

            # Use lief.parse with proper error handling
            old_bin = lief.parse(old_binary)
            new_bin = lief.parse(new_binary)

            if not old_bin or not new_bin:
                self.logger.warning("Failed to parse one or both binaries with LIEF")
                return differences

            # Compare binary types
            if type(old_bin) is not type(new_bin):
                differences.append(
                    {
                        "type": "binary_type_change",
                        "description": f"Binary type changed: {type(old_bin).__name__} -> {type(new_bin).__name__}",
                        "security_relevance": SecurityRelevance.CRITICAL.value,
                    },
                )
                return differences

            # PE specific comparisons
            if hasattr(old_bin, "optional_header") and hasattr(new_bin, "optional_header"):
                old_opt = old_bin.optional_header
                new_opt = new_bin.optional_header

                # Check security features
                if hasattr(old_opt, "dll_characteristics") and hasattr(new_opt, "dll_characteristics"):
                    old_chars = old_opt.dll_characteristics
                    new_chars = new_opt.dll_characteristics

                    # Check for DEP/NX
                    old_dep = bool(old_chars & 0x0100)  # IMAGE_DLLCHARACTERISTICS_NX_COMPAT
                    new_dep = bool(new_chars & 0x0100)
                    if old_dep != new_dep:
                        differences.append(
                            {
                                "type": DiffType.SECURITY_FEATURE_ADDED.value if new_dep else DiffType.SECURITY_FEATURE_REMOVED.value,
                                "description": f"DEP/NX {'enabled' if new_dep else 'disabled'}",
                                "security_relevance": SecurityRelevance.CRITICAL.value,
                            },
                        )

                    # Check for ASLR
                    old_aslr = bool(old_chars & 0x0040)  # IMAGE_DLLCHARACTERISTICS_DYNAMIC_BASE
                    new_aslr = bool(new_chars & 0x0040)
                    if old_aslr != new_aslr:
                        differences.append(
                            {
                                "type": DiffType.SECURITY_FEATURE_ADDED.value if new_aslr else DiffType.SECURITY_FEATURE_REMOVED.value,
                                "description": f"ASLR {'enabled' if new_aslr else 'disabled'}",
                                "security_relevance": SecurityRelevance.CRITICAL.value,
                            },
                        )

                # Check entry point
                if (
                    hasattr(old_opt, "addressof_entrypoint")
                    and hasattr(new_opt, "addressof_entrypoint")
                    and old_opt.addressof_entrypoint != new_opt.addressof_entrypoint
                ):
                    differences.append(
                        {
                            "type": "entry_point_change",
                            "description": f"Entry point changed: 0x{old_opt.addressof_entrypoint:x} -> 0x{new_opt.addressof_entrypoint:x}",
                            "security_relevance": SecurityRelevance.HIGH.value,
                        },
                    )

            # Section comparisons
            if hasattr(old_bin, "sections") and hasattr(new_bin, "sections"):
                old_sections: dict[str, Any] = {(s.name.decode() if isinstance(s.name, bytes) else s.name): s for s in old_bin.sections}
                new_sections: dict[str, Any] = {(s.name.decode() if isinstance(s.name, bytes) else s.name): s for s in new_bin.sections}

                # Check for section changes
                for name in set(old_sections) | set(new_sections):
                    if name in old_sections and name not in new_sections:
                        differences.append(
                            {
                                "type": DiffType.SECTION_REMOVED.value,
                                "description": f"Section removed: {name}",
                                "security_relevance": SecurityRelevance.MEDIUM.value,
                            },
                        )
                    elif name not in old_sections and name in new_sections:
                        new_sec = new_sections[name]
                        new_size_raw = getattr(new_sec, "size", 0)
                        new_size_int = new_size_raw if isinstance(new_size_raw, int) else int(new_size_raw)

                        differences.append(
                            {
                                "type": DiffType.SECTION_ADDED.value,
                                "description": f"Section added: {name} (size: {new_size_int})",
                                "security_relevance": SecurityRelevance.MEDIUM.value,
                                "section_size": str(new_size_int),
                            },
                        )
                    elif name in old_sections:
                        old_sec = old_sections[name]
                        new_sec = new_sections[name]

                        # Check section characteristics
                        if hasattr(old_sec, "characteristics") and hasattr(new_sec, "characteristics"):
                            old_chars_raw = old_sec.characteristics
                            new_chars_raw = new_sec.characteristics
                            old_chars_int = old_chars_raw if isinstance(old_chars_raw, int) else int(old_chars_raw)
                            new_chars_int = new_chars_raw if isinstance(new_chars_raw, int) else int(new_chars_raw)

                            if old_chars_int != new_chars_int:
                                differences.append(
                                    {
                                        "type": "section_permissions_change",
                                        "description": f"Section {name} permissions changed (0x{old_chars_int:x} -> 0x{new_chars_int:x})",
                                        "security_relevance": SecurityRelevance.HIGH.value,
                                        "old_characteristics": str(old_chars_int),
                                        "new_characteristics": str(new_chars_int),
                                    },
                                )

                        # Check section size changes
                        old_size_raw = getattr(old_sec, "size", 0)
                        new_size_raw = getattr(new_sec, "size", 0)
                        old_size_int = old_size_raw if isinstance(old_size_raw, int) else int(old_size_raw)
                        new_size_int = new_size_raw if isinstance(new_size_raw, int) else int(new_size_raw)

                        if old_size_int != new_size_int:
                            size_change = new_size_int - old_size_int
                            differences.append(
                                {
                                    "type": "section_size_change",
                                    "description": f"Section {name} size changed: {old_size_int} -> {new_size_int} ({size_change:+d})",
                                    "security_relevance": SecurityRelevance.LOW.value
                                    if abs(size_change) < 1024
                                    else SecurityRelevance.MEDIUM.value,
                                    "old_size": str(old_size_int),
                                    "new_size": str(new_size_int),
                                    "size_change": str(size_change),
                                },
                            )

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            if isinstance(e, AttributeError):
                self.logger.warning("lief.parse not available - lief may not be properly installed")
            else:
                self.logger.debug("Binary structure comparison error for %s vs %s: %s", old_binary, new_binary, e)

        return differences

    def _analyze_patch_content(self, patch_content: str) -> dict[str, Any]:
        """Analyze patch file content for security implications.

        Examines patch content for security keywords and vulnerability indicators.

        Args:
            patch_content: Complete text content of the patch file.

        Returns:
            dict[str, Any]: Analysis results including patch type and security findings.
        """
        analysis: dict[str, Any] = {
            "patch_type": "unknown",
            "security_fixes": [],
            "potential_vulnerabilities": [],
            "impact_assessment": {},
        }

        try:
            # Analyze patch for security keywords
            security_keywords = {
                "fixes": ["fix", "patch", "security", "vulnerability", "cve"],
                "dangerous": ["buffer", "overflow", "injection", "bypass", "privilege"],
                "crypto": ["encrypt", "decrypt", "hash", "random", "crypto"],
                "auth": ["auth", "login", "password", "token", "access"],
            }

            patch_lower = patch_content.lower()

            security_fixes: list[str] = []
            potential_vulnerabilities: list[str] = []

            for category, keywords in security_keywords.items():
                for keyword in keywords:
                    if keyword in patch_lower:
                        if category == "fixes":
                            security_fixes.append(keyword)
                        elif category == "dangerous":
                            potential_vulnerabilities.append(keyword)

            analysis["security_fixes"] = security_fixes
            analysis["potential_vulnerabilities"] = potential_vulnerabilities

            # Determine patch type
            if analysis["security_fixes"]:
                analysis["patch_type"] = "security_fix"
            elif analysis["potential_vulnerabilities"]:
                analysis["patch_type"] = "potentially_dangerous"
            else:
                analysis["patch_type"] = "regular"

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Patch content analysis error: %s", e)

        return analysis

    def _map_patch_to_binary(self, patch_content: str, binary_path: str) -> dict[str, Any]:
        """Map patch changes to binary file.

        Correlates patch content to affected functions and sections in the binary.

        Args:
            patch_content: Complete text content of the patch file.
            binary_path: Path to the target binary file for patch mapping.

        Returns:
            dict[str, Any]: Mapping results including affected functions and sections.
        """
        mapping: dict[str, Any] = {
            "affected_functions": [],
            "affected_sections": [],
            "binary_changes": [],
            "patch_size": len(patch_content),
            "binary_info": {},
        }

        try:
            self.logger.debug("Mapping patch to binary: %s (patch size: %s bytes)", binary_path, len(patch_content))

            # Get binary information
            if os.path.exists(binary_path):
                mapping["binary_info"] = {
                    "path": binary_path,
                    "size": os.path.getsize(binary_path),
                    "hash": self._calculate_file_hash(binary_path),
                }

            # Parse patch content for addresses and function names
            patch_lines = patch_content.split("\n")

            # Look for hex addresses (common in patches)
            import re

            hex_pattern = r"0x[0-9a-fA-F]+"
            func_pattern = r"(?:function|func|sub_|FUN_)[0-9a-zA-Z_]+"

            addresses = []
            functions = []

            for line in patch_lines:
                # Find hex addresses
                hex_matches = re.findall(hex_pattern, line)
                addresses.extend(hex_matches)

                # Find function references
                func_matches = re.findall(func_pattern, line, re.IGNORECASE)
                functions.extend(func_matches)

            # Extract functions from binary to cross-reference
            binary_functions = self._extract_functions(binary_path)
            binary_func_names = {f.get("name", f"sub_{addr:x}").lower() for addr, f in binary_functions.items()}

            # Map patch references to binary functions
            matched_functions = 0
            affected_functions: list[dict[str, Any]] = []
            for func_ref in set(functions):
                func_lower = func_ref.lower()
                if matching_funcs := [bin_func for bin_func in binary_func_names if func_lower in bin_func]:
                    security_relevance = self._assess_function_security_relevance(func_ref)
                    affected_functions.append(
                        {
                            "name": func_ref,
                            "type": "direct_reference",
                            "security_relevance": security_relevance.value,
                            "matches": matching_funcs[:5],
                        },
                    )
                    matched_functions += 1

            mapping["affected_functions"] = affected_functions

            if matched_functions > 0:
                self.logger.debug("Patch references %s functions in binary", matched_functions)

            # Analyze patch for section references
            section_keywords = [
                ".text",
                ".data",
                ".rdata",
                ".bss",
                ".reloc",
                ".rsrc",
                ".init",
                ".plt",
                ".got",
            ]
            section_references: dict[str, int] = {}
            affected_sections: list[str] = []

            for line in patch_lines:
                for section in section_keywords:
                    if section in line:
                        if section not in section_references:
                            section_references[section] = 0
                        section_references[section] += 1
                        affected_sections.append(section)

            mapping["affected_sections"] = list(set(affected_sections))

            # Log section reference statistics
            if section_references:
                self.logger.debug("Patch contains section references: %s", section_references)
                mapping["section_reference_counts"] = section_references

            # Determine patch type based on content
            binary_changes: list[dict[str, Any]] = []

            if addresses:
                binary_changes.append(
                    {
                        "description": f"Patch contains {len(set(addresses))} unique address references",
                        "type": "address_patch",
                        "addresses": list(set(addresses))[:10],
                    },
                )

            if affected_functions:
                binary_changes.append(
                    {
                        "description": f"Patch affects {len(affected_functions)} functions",
                        "type": "function_patch",
                    },
                )

            if affected_sections:
                binary_changes.append(
                    {
                        "description": f"Patch references {len(affected_sections)} sections",
                        "type": "section_patch",
                    },
                )

            # Analyze patch patterns
            patch_lower = patch_content.lower()
            if "nop" in patch_lower or "0x90" in patch_lower:
                binary_changes.append(
                    {
                        "description": "Patch contains NOP instructions (possible bypass)",
                        "type": "nop_patch",
                        "security_relevance": SecurityRelevance.HIGH.value,
                    },
                )

            if "jmp" in patch_lower or "call" in patch_lower:
                binary_changes.append(
                    {
                        "description": "Patch modifies control flow (JMP/CALL)",
                        "type": "control_flow_patch",
                        "security_relevance": SecurityRelevance.MEDIUM.value,
                    },
                )

            if not binary_changes:
                binary_changes.append(
                    {
                        "description": "Patch analysis complete",
                        "type": "generic_patch",
                    },
                )

            mapping["binary_changes"] = binary_changes

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Patch-to-binary mapping error for %s: %s", binary_path, e)
            mapping["binary_changes"].append(
                {
                    "description": f"Mapping error: {e!s}",
                    "type": "error",
                },
            )

        return mapping

    def _extract_functions(self, binary_path: str) -> dict[int, dict[str, Any]]:
        """Extract function information from binary.

        Extracts function symbols, addresses, and metadata from binary using objdump or nm.

        Args:
            binary_path: Path to the binary file to analyze.

        Returns:
            dict[int, dict[str, Any]]: Dictionary mapping function addresses to their metadata.
        """
        functions = {}

        try:
            if objdump_path := shutil.which("objdump"):
                result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis
                    [objdump_path, "-t", binary_path],
                    capture_output=True,
                    text=True,
                    check=False,
                )
            else:
                result = None

            if result and result.returncode == 0:
                lines = result.stdout.split("\n")
                functions_found = 0

                for line in lines:
                    if "F .text" in line:
                        parts = line.split()
                        if len(parts) >= 6:
                            try:
                                addr = int(parts[0], 16)
                                size = int(parts[4], 16) if parts[4] and parts[4].isdigit() else 0
                                name = parts[-1]

                                # Enhanced function analysis
                                security_relevance = self._assess_function_security_relevance(name)

                                # Extract actual function bytes if size is known
                                func_bytes = b""
                                if size > 0 and CAPSTONE_AVAILABLE:
                                    func_bytes = self._extract_function_bytes_with_capstone(binary_path, addr, size)

                                functions[addr] = {
                                    "name": name,
                                    "size": size,
                                    "bytes": func_bytes,
                                    "analysis": {
                                        "security_relevance": security_relevance.value,
                                        "extracted_from": binary_path,
                                        "has_disassembly": len(func_bytes) > 0,
                                    },
                                }
                                functions_found += 1

                            except (ValueError, IndexError) as parse_error:
                                self.logger.debug("Failed to parse function line '%s': %s", line, parse_error)
                                continue

                self.logger.debug("Extracted %s functions from %s", functions_found, binary_path)
            else:
                if nm_path := shutil.which("nm"):
                    nm_result = subprocess.run(  # nosec S603 - Using nm for symbol analysis
                        [nm_path, "-C", "-D", binary_path],
                        capture_output=True,
                        text=True,
                        check=False,
                    )
                else:
                    nm_result = None
                if nm_result and nm_result.returncode == 0:
                    nm_lines = nm_result.stdout.split("\n")
                    nm_functions_found = 0

                    for line in nm_lines:
                        if " T " in line or " t " in line:  # Text symbols
                            parts = line.split()
                            if len(parts) >= 3:
                                try:
                                    addr = int(parts[0], 16)
                                    name = parts[-1]

                                    security_relevance = self._assess_function_security_relevance(name)

                                    functions[addr] = {
                                        "name": name,
                                        "size": 0,  # nm doesn't provide size
                                        "bytes": b"",
                                        "analysis": {
                                            "security_relevance": security_relevance.value,
                                            "extracted_from": binary_path,
                                            "extraction_method": "nm",
                                        },
                                    }
                                    nm_functions_found += 1

                                except (ValueError, IndexError) as parse_error:
                                    self.logger.debug("Failed to parse nm line '%s': %s", line, parse_error)
                                    continue

                    self.logger.debug("Extracted %s functions from %s using nm", nm_functions_found, binary_path)

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug("Function extraction error for %s: %s", binary_path, e)

        return functions

    def _calculate_function_similarity(self, func1: bytes, func2: bytes) -> float:
        """Calculate similarity between two functions.

        Computes byte-level similarity score between two function bytecodes.

        Args:
            func1: Raw bytecode of the first function.
            func2: Raw bytecode of the second function.

        Returns:
            float: Similarity score from 0.0 (no similarity) to 1.0 (identical).
        """
        try:
            if not func1 or not func2:
                return 0.0

            max_len = max(len(func1), len(func2))

            if max_len == 0:
                return 1.0

            # Simple byte-based similarity
            min_len = min(len(func1), len(func2))
            return sum(func1[i] == func2[i] for i in range(min_len)) / max_len
        except (OSError, ValueError, RuntimeError) as e:
            self.logger.exception("Error in binary_differ: %s", e)
            return 0.0

    def _extract_function_bytes_with_capstone(self, binary_path: str, func_addr: int, func_size: int) -> bytes:
        """Extract and disassemble function bytes using Capstone.

        Extracts raw bytecode for a function from the binary file.

        Args:
            binary_path: Path to the binary file.
            func_addr: Virtual address of the function.
            func_size: Size of the function in bytes.

        Returns:
            bytes: Raw bytecode of the function, or empty bytes if extraction fails.
        """
        if not CAPSTONE_AVAILABLE:
            return b""

        try:
            # Read the binary file
            with open(binary_path, "rb") as f:
                # Read the entire file to handle address mappings
                binary_data = f.read()

            # Try to use LIEF to get proper file offsets
            if LIEF_AVAILABLE:
                try:
                    if binary := lief.parse(binary_path):
                        # Find the section containing the function
                        for section in binary.sections:
                            if hasattr(section, "virtual_address") and hasattr(section, "size"):
                                sec_start = section.virtual_address
                                sec_end = sec_start + section.size

                                if sec_start <= func_addr < sec_end:
                                    # Calculate offset within section
                                    offset_in_section = func_addr - sec_start
                                    file_offset = section.offset + offset_in_section

                                    # Extract function bytes
                                    if file_offset + func_size <= len(binary_data):
                                        func_bytes = binary_data[file_offset : file_offset + func_size]

                                        # Use Capstone to verify it's valid code
                                        if self._verify_code_with_capstone(func_bytes, func_addr):
                                            return func_bytes
                                        break
                except Exception as e:
                    self.logger.debug("LIEF parsing failed: %s", e)

            # Fallback: Try direct offset (works for some simple binaries)
            if func_addr < len(binary_data):
                func_bytes = binary_data[func_addr : func_addr + func_size]
                if self._verify_code_with_capstone(func_bytes, func_addr):
                    return func_bytes

            return b""

        except Exception as e:
            self.logger.debug("Failed to extract function bytes at 0x%x: %s", func_addr, e)
            return b""

    def _verify_code_with_capstone(self, code_bytes: bytes, base_addr: int) -> bool:
        """Verify that bytes are valid code using Capstone disassembler.

        Validates that extracted bytes can be successfully disassembled as code.

        Args:
            code_bytes: Raw bytes to verify as valid code.
            base_addr: Base address for disassembly context.

        Returns:
            bool: True if bytes represent valid code, False otherwise.
        """
        if not CAPSTONE_AVAILABLE or not code_bytes:
            return False

        try:
            arch_info = self._detect_binary_architecture("")
            if (arch_info["arch"] == "x86" and arch_info["bits"] == 64) or arch_info["arch"] not in ["x86", "arm"]:
                md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_64)
            elif arch_info["arch"] == "x86":
                md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_32)
            elif arch_info["bits"] == 64:
                md = capstone.Cs(capstone.CS_ARCH_ARM64, capstone.CS_MODE_ARM)
            else:
                md = capstone.Cs(capstone.CS_ARCH_ARM, capstone.CS_MODE_ARM)
            if instructions := list(md.disasm(code_bytes[: min(32, len(code_bytes))], base_addr)):
                self.logger.debug("Verified code at 0x%x - First instruction: %s", base_addr, instructions[0].mnemonic)
                return True

            return False

        except Exception as e:
            self.logger.debug("Capstone verification failed: %s", e)
            return False

    def _compare_control_flow_with_capstone(self, old_binary: str, new_binary: str) -> list[dict[str, Any]]:
        """Compare control flow using Capstone disassembly.

        Performs detailed control flow analysis using Capstone instruction disassembly.

        Args:
            old_binary: Path to the original binary file.
            new_binary: Path to the modified binary file.

        Returns:
            list[dict[str, Any]]: List of control flow differences detected.
        """
        differences: list[dict[str, Any]] = []

        if not CAPSTONE_AVAILABLE:
            return differences

        try:
            # Extract functions from both binaries
            old_functions = self._extract_functions(old_binary)
            new_functions = self._extract_functions(new_binary)

            # Analyze control flow patterns using Capstone
            old_cf_stats = self._analyze_control_flow_patterns(old_binary, old_functions)
            new_cf_stats = self._analyze_control_flow_patterns(new_binary, new_functions)

            # Compare control flow statistics
            for metric, old_value in old_cf_stats.items():
                new_value = new_cf_stats.get(metric, 0)
                if old_value != new_value:
                    change = new_value - old_value

                    # Determine security relevance based on metric type
                    relevance = SecurityRelevance.LOW
                    if metric in ["indirect_jumps", "indirect_calls"]:
                        relevance = SecurityRelevance.HIGH if abs(change) > 10 else SecurityRelevance.MEDIUM
                    elif metric in ["conditional_jumps", "function_calls"]:
                        relevance = SecurityRelevance.MEDIUM if abs(change) > 20 else SecurityRelevance.LOW

                    differences.append(
                        {
                            "type": "control_flow_capstone",
                            "description": f"Control flow {metric}: {old_value} -> {new_value} (change: {change:+d})",
                            "security_relevance": relevance.value,
                            "metric": metric,
                            "old_value": old_value,
                            "new_value": new_value,
                            "change": change,
                        },
                    )

            # Check for new indirect control flow (potential security concern)
            if new_cf_stats.get("indirect_jumps", 0) > old_cf_stats.get("indirect_jumps", 0):
                differences.append(
                    {
                        "type": "security_concern",
                        "description": "Increased indirect jumps detected - potential security implications",
                        "security_relevance": SecurityRelevance.HIGH.value,
                    },
                )

        except Exception as e:
            self.logger.debug("Capstone control flow comparison failed: %s", e)

        return differences

    def _analyze_control_flow_patterns(self, binary_path: str, functions: dict[int, dict[str, Any]]) -> dict[str, int]:
        """Analyze control flow patterns in binary using Capstone with architecture detection.

        Counts control flow instructions like jumps, calls, and returns in the binary.

        Args:
            binary_path: Path to the binary file to analyze.
            functions: Dictionary of functions extracted from the binary.

        Returns:
            dict[str, int]: Statistics of control flow patterns and instruction counts.
        """
        stats = {
            "conditional_jumps": 0,
            "unconditional_jumps": 0,
            "function_calls": 0,
            "indirect_jumps": 0,
            "indirect_calls": 0,
            "returns": 0,
            "syscalls": 0,
            "interrupts": 0,
        }

        if not CAPSTONE_AVAILABLE:
            return stats

        try:
            # Determine architecture from binary path/metadata
            arch_info = self._detect_binary_architecture(binary_path)

            # Setup Capstone disassembler based on detected architecture
            if (arch_info["arch"] == "x86" and arch_info["bits"] == 64) or arch_info["arch"] not in ["x86", "arm"]:
                md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_64)
            elif arch_info["arch"] == "x86":
                md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_32)
            elif arch_info["bits"] == 64:
                md = capstone.Cs(capstone.CS_ARCH_ARM64, capstone.CS_MODE_ARM)
            else:
                md = capstone.Cs(capstone.CS_ARCH_ARM, capstone.CS_MODE_ARM)
            md.detail = True  # Enable detailed instruction info

            # Add binary path to stats for context - using Any since stats dict has int values
            # These are metadata fields that don't fit the int pattern
            stats["binary_path"] = binary_path  # type: ignore[assignment]
            stats["detected_arch"] = f"{arch_info['arch']}_{arch_info['bits']}"  # type: ignore[assignment]

            # Analyze each function
            for func_addr, func_info in functions.items():
                func_bytes = func_info.get("bytes", b"")
                if not func_bytes:
                    continue

                # Disassemble function
                for insn in md.disasm(func_bytes, func_addr):
                    # Categorize instructions
                    if insn.group(capstone.x86.X86_GRP_JUMP):
                        # Check if conditional or unconditional jump
                        if insn.mnemonic in ["jmp", "ljmp"]:
                            stats["unconditional_jumps"] += 1
                            # Check for indirect jump
                            if len(insn.operands) > 0 and insn.operands[0].type == capstone.x86.X86_OP_REG:
                                stats["indirect_jumps"] += 1
                        else:
                            stats["conditional_jumps"] += 1

                    elif insn.group(capstone.x86.X86_GRP_CALL):
                        stats["function_calls"] += 1
                        # Check for indirect call
                        if len(insn.operands) > 0 and insn.operands[0].type == capstone.x86.X86_OP_REG:
                            stats["indirect_calls"] += 1

                    elif insn.group(capstone.x86.X86_GRP_RET):
                        stats["returns"] += 1

                    elif insn.mnemonic in ["syscall", "sysenter"]:
                        stats["syscalls"] += 1

                    elif insn.mnemonic.startswith("int"):
                        stats["interrupts"] += 1

        except Exception as e:
            self.logger.debug("Control flow pattern analysis failed: %s", e)

        return stats

    def get_analysis_cache(self) -> dict[str, Any]:
        """Get current analysis cache.

        Returns a copy of the cached analysis results.

        Returns:
            dict[str, Any]: Copy of the analysis cache dictionary.
        """
        return self.analysis_cache.copy()

    def clear_analysis_cache(self) -> None:
        """Clear analysis cache.

        Removes all cached analysis results from memory.
        """
        self.analysis_cache.clear()

    def _detect_binary_architecture(self, binary_path: str) -> dict[str, Any]:
        """Detect binary architecture and bitness from file.

        Identifies the processor architecture and bitness of a binary.

        Args:
            binary_path: Path to the binary file to analyze.

        Returns:
            dict[str, Any]: Architecture information with keys 'arch', 'bits', 'endian'.
        """
        arch_info = {
            "arch": "x86",
            "bits": 64,
            "endian": "little",
        }

        try:
            # Try to use pefile for PE files
            if PEFILE_AVAILABLE and binary_path.lower().endswith((".exe", ".dll", ".sys")):
                pe = pefile.PE(binary_path)
                if pe.FILE_HEADER.Machine == pefile.MACHINE_TYPE["IMAGE_FILE_MACHINE_I386"]:
                    arch_info |= {"arch": "x86", "bits": 32}
                elif pe.FILE_HEADER.Machine == pefile.MACHINE_TYPE["IMAGE_FILE_MACHINE_AMD64"]:
                    arch_info |= {"arch": "x86", "bits": 64}
                elif pe.FILE_HEADER.Machine == pefile.MACHINE_TYPE["IMAGE_FILE_MACHINE_ARM64"]:
                    arch_info |= {"arch": "arm", "bits": 64}
                pe.close()
                return arch_info

            # Try basic file header analysis for ELF files
            with open(binary_path, "rb") as f:
                header = f.read(64)
                if len(header) >= 16 and header[:4] == b"\x7fELF":
                    ei_class = header[4]  # 1=32-bit, 2=64-bit
                    ei_data = header[5]  # 1=little-endian, 2=big-endian
                    e_machine = int.from_bytes(header[18:20], "little" if ei_data == 1 else "big")

                    arch_info["bits"] = 64 if ei_class == 2 else 32
                    arch_info["endian"] = "little" if ei_data == 1 else "big"

                    if e_machine in {62, 3}:  # EM_X86_64
                        arch_info["arch"] = "x86"
                    elif e_machine in {40, 183}:  # EM_ARM
                        arch_info["arch"] = "arm"

        except Exception as e:
            self.logger.debug("Architecture detection failed for %s: %s", binary_path, e)

        return arch_info

    def export_analysis(self, result: dict[str, Any], output_file: str, format: str = "json") -> bool:
        """Export analysis results to file.

        Saves analysis results in the specified format to an output file.

        Args:
            result: Analysis result dictionary to export.
            output_file: Path to the output file for exported results.
            format: Output format (json, html, xml, etc.). Defaults to 'json'.

        Returns:
            bool: True if export was successful, False otherwise.
        """
        from ...utils.analysis.analysis_exporter import AnalysisExporter

        return AnalysisExporter.export_analysis(result, output_file, format, "binary_diff")
