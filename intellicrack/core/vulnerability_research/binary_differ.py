"""
This file is part of Intellicrack.
Copyright (C) 2025 Zachary Flint

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see <https://www.gnu.org/licenses/>.
"""

"""
Binary Diffing Engine

Automated binary comparison and patch analysis for vulnerability research.
Identifies security-relevant changes between binary versions.
"""

import hashlib
import logging
import os
import subprocess
import time
from enum import Enum
from typing import Any, Dict, List

from ...utils.analysis.analysis_stats import AnalysisStatsGenerator
from .base_analyzer import BaseAnalyzer
from .common_enums import SecurityRelevance

logger = logging.getLogger(__name__)

# Optional imports for advanced analysis
try:
    import capstone
    CAPSTONE_AVAILABLE = True
except ImportError as e:
    logger.error("Import error in binary_differ: %s", e)
    CAPSTONE_AVAILABLE = False

try:
    import lief
    LIEF_AVAILABLE = True
except ImportError as e:
    logger.error("Import error in binary_differ: %s", e)
    LIEF_AVAILABLE = False

try:
    import pefile
    PEFILE_AVAILABLE = True
except ImportError as e:
    logger.error("Import error in binary_differ: %s", e)
    PEFILE_AVAILABLE = False


class DiffType(Enum):
    """Types of binary differences"""
    FUNCTION_ADDED = "function_added"
    FUNCTION_REMOVED = "function_removed"
    FUNCTION_MODIFIED = "function_modified"
    DATA_CHANGED = "data_changed"
    IMPORT_ADDED = "import_added"
    IMPORT_REMOVED = "import_removed"
    SECTION_ADDED = "section_added"
    SECTION_REMOVED = "section_removed"
    SECTION_MODIFIED = "section_modified"
    SECURITY_FEATURE_ADDED = "security_feature_added"
    SECURITY_FEATURE_REMOVED = "security_feature_removed"


class BinaryDiffer(BaseAnalyzer):
    """
    Advanced binary diffing engine for vulnerability research.
    """

    def __init__(self):
        """Initialize the binary differ.

        Sets up the binary comparison and analysis system for vulnerability
        research. Configures security function patterns, mitigation detection,
        and analysis metrics for identifying security-relevant changes between
        binary versions.
        """
        super().__init__()
        self.logger = logging.getLogger("IntellicrackLogger.BinaryDiffer")

        # Security-relevant function patterns
        self.security_functions = {
            "memory": ["malloc", "free", "calloc", "realloc", "memcpy", "memset", "strcpy", "strcat", "sprintf"],
            "crypto": ["encrypt", "decrypt", "hash", "random", "aes", "rsa", "sha", "md5"],
            "auth": ["login", "authenticate", "verify", "check", "validate", "authorize"],
            "network": ["socket", "connect", "bind", "listen", "send", "recv", "http", "ssl"],
            "file": ["fopen", "fread", "fwrite", "open", "read", "write", "chmod", "chown"],
            "process": ["exec", "system", "fork", "spawn", "thread", "process"]
        }

        # Security mitigations
        self.security_mitigations = {
            "stack_protection": ["__stack_chk_fail", "__stack_chk_guard"],
            "aslr": ["__randomize_layout", "ASLR"],
            "dep": ["NX", "DEP", "EXECUTE_PROTECT"],
            "cfg": ["__guard_dispatch", "CFG"],
            "cfi": ["__cfi_check", "__typeid"],
            "fortify": ["__fortify_function", "__builtin___memcpy_chk"]
        }

        # Vulnerability patterns
        self.vuln_patterns = {
            "buffer_overflow": [
                "strcpy", "strcat", "sprintf", "gets", "scanf",
                "memcpy_unsafe", "strncpy_unsafe"
            ],
            "format_string": ["printf", "sprintf", "fprintf", "snprintf"],
            "integer_overflow": ["add_overflow", "mul_overflow", "size_check"],
            "use_after_free": ["free", "delete", "use_after"],
            "double_free": ["double_free", "free_twice"],
            "null_deref": ["null_check", "deref_null"]
        }

        # Analysis cache
        self.analysis_cache = {}

    def compare_binaries(self,
                        old_binary: str,
                        new_binary: str,
                        analysis_level: str = "comprehensive") -> Dict[str, Any]:
        """
        Compare two binary files and identify security-relevant differences.

        Args:
            old_binary: Path to original binary
            new_binary: Path to updated binary
            analysis_level: Level of analysis ('basic', 'intermediate', 'comprehensive')

        Returns:
            Comprehensive diff analysis results
        """
        result = {
            "success": False,
            "old_binary": old_binary,
            "new_binary": new_binary,
            "analysis_level": analysis_level,
            "differences": [],
            "security_impact": {},
            "statistics": {},
            "recommendations": [],
            "analysis_time": 0,
            "error": None
        }

        start_time = time.time()

        try:
            self.logger.info(f"Comparing binaries: {old_binary} vs {new_binary}")

            # Validate input files
            if not os.path.exists(old_binary):
                result["error"] = f"Old binary not found: {old_binary}"
                return result

            if not os.path.exists(new_binary):
                result["error"] = f"New binary not found: {new_binary}"
                return result

            # Perform analysis based on level
            differences = []

            # Basic analysis - always performed
            basic_diffs = self._basic_analysis(old_binary, new_binary)
            differences.extend(basic_diffs)

            if analysis_level in ["intermediate", "comprehensive"]:
                # Intermediate analysis
                intermediate_diffs = self._intermediate_analysis(old_binary, new_binary)
                differences.extend(intermediate_diffs)

            if analysis_level == "comprehensive":
                # Comprehensive analysis
                comprehensive_diffs = self._comprehensive_analysis(old_binary, new_binary)
                differences.extend(comprehensive_diffs)

            # Analyze security impact
            security_impact = self._analyze_security_impact(differences)

            # Generate statistics
            statistics = self._generate_statistics(differences, old_binary, new_binary)

            # Generate recommendations
            recommendations = self._generate_recommendations(differences, security_impact)

            result["differences"] = differences
            result["security_impact"] = security_impact
            result["statistics"] = statistics
            result["recommendations"] = recommendations
            result["analysis_time"] = time.time() - start_time
            result["success"] = True

            self.logger.info(f"Binary comparison complete: {len(differences)} differences found")
            return result

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            logger.error("Error in binary_differ: %s", e)
            return self.handle_analysis_error(result, e, start_time)

    def analyze_patch(self,
                     patch_file: str,
                     target_binary: str) -> Dict[str, Any]:
        """
        Analyze a patch file for security implications.

        Args:
            patch_file: Path to patch file
            target_binary: Path to target binary

        Returns:
            Patch analysis results
        """
        result = self.create_analysis_result(
            patch_file=patch_file,
            target_binary=target_binary,
            patch_type=None,
            security_fixes=[],
            potential_vulnerabilities=[],
            impact_assessment={}
        )

        try:
            self.logger.info(f"Analyzing patch: {patch_file}")

            if not os.path.exists(patch_file):
                result["error"] = f"Patch file not found: {patch_file}"
                return result

            # Read and parse patch
            with open(patch_file, "r") as f:
                patch_content = f.read()

            # Analyze patch content
            patch_analysis = self._analyze_patch_content(patch_content)

            # Map to binary changes if target provided
            if target_binary and os.path.exists(target_binary):
                binary_impact = self._map_patch_to_binary(patch_content, target_binary)
                result.update(binary_impact)

            result.update(patch_analysis)
            result["success"] = True

            self.logger.info("Patch analysis complete")
            return result

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.error(f"Patch analysis failed: {e}")
            result["error"] = str(e)
            return result

    def find_similar_functions(self,
                              binary_path: str,
                              target_function: bytes,
                              similarity_threshold: float = 0.8) -> List[Dict[str, Any]]:
        """
        Find functions similar to a target function in a binary.

        Args:
            binary_path: Path to binary to search
            target_function: Target function bytes
            similarity_threshold: Minimum similarity score

        Returns:
            List of similar functions found
        """
        similar_functions = []

        try:
            self.logger.info(f"Finding similar functions in {binary_path}")

            # Extract functions from binary
            functions = self._extract_functions(binary_path)

            # Compare each function to target
            for func_addr, func_data in functions.items():
                similarity = self._calculate_function_similarity(target_function, func_data["bytes"])

                if similarity >= similarity_threshold:
                    similar_functions.append({
                        "address": func_addr,
                        "similarity": similarity,
                        "size": len(func_data["bytes"]),
                        "name": func_data.get("name", f"sub_{func_addr:x}"),
                        "analysis": func_data.get("analysis", {})
                    })

            # Sort by similarity
            similar_functions.sort(key=lambda x: x["similarity"], reverse=True)

            self.logger.info(f"Found {len(similar_functions)} similar functions")
            return similar_functions

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.error(f"Similar function search failed: {e}")
            return similar_functions

    def _basic_analysis(self, old_binary: str, new_binary: str) -> List[Dict[str, Any]]:
        """Perform basic binary analysis."""
        differences = []

        try:
            # File size comparison
            old_size = os.path.getsize(old_binary)
            new_size = os.path.getsize(new_binary)

            if old_size != new_size:
                differences.append({
                    "type": DiffType.DATA_CHANGED.value,
                    "description": f"File size changed: {old_size} -> {new_size}",
                    "security_relevance": SecurityRelevance.LOW.value,
                    "old_value": old_size,
                    "new_value": new_size
                })

            # Hash comparison
            old_hash = self._calculate_file_hash(old_binary)
            new_hash = self._calculate_file_hash(new_binary)

            if old_hash != new_hash:
                differences.append({
                    "type": DiffType.DATA_CHANGED.value,
                    "description": "Binary content changed",
                    "security_relevance": SecurityRelevance.MEDIUM.value,
                    "old_value": old_hash,
                    "new_value": new_hash
                })

            # Section comparison using hexdump
            old_sections = self._get_basic_sections(old_binary)
            new_sections = self._get_basic_sections(new_binary)

            # Compare sections
            for section_name in set(old_sections.keys()) | set(new_sections.keys()):
                if section_name in old_sections and section_name not in new_sections:
                    differences.append({
                        "type": DiffType.SECTION_REMOVED.value,
                        "description": f"Section removed: {section_name}",
                        "security_relevance": SecurityRelevance.MEDIUM.value,
                        "section": section_name
                    })
                elif section_name not in old_sections and section_name in new_sections:
                    differences.append({
                        "type": DiffType.SECTION_ADDED.value,
                        "description": f"Section added: {section_name}",
                        "security_relevance": SecurityRelevance.MEDIUM.value,
                        "section": section_name
                    })
                else:
                    # Both binaries have this section - check if content changed
                    if section_name in old_sections and section_name in new_sections:
                        old_content = old_sections[section_name]
                        new_content = new_sections[section_name]

                        if old_content != new_content:
                            # Calculate content change metrics
                            old_size = len(old_content) if isinstance(old_content, str) else 0
                            new_size = len(new_content) if isinstance(new_content, str) else 0
                            size_change = new_size - old_size

                            differences.append({
                                "type": DiffType.SECTION_MODIFIED.value,
                                "description": f"Section modified: {section_name} (size change: {size_change:+d})",
                                "security_relevance": SecurityRelevance.MEDIUM.value,
                                "section": section_name,
                                "old_size": old_size,
                                "new_size": new_size,
                                "size_change": size_change
                            })

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Basic analysis error: {e}")

        return differences

    def _intermediate_analysis(self, old_binary: str, new_binary: str) -> List[Dict[str, Any]]:
        """Perform intermediate binary analysis."""
        differences = []

        try:
            # Import table comparison
            old_imports = self._extract_imports(old_binary)
            new_imports = self._extract_imports(new_binary)

            # Find import differences
            old_import_set = set(old_imports)
            new_import_set = set(new_imports)

            added_imports = new_import_set - old_import_set
            removed_imports = old_import_set - new_import_set

            for import_name in added_imports:
                security_relevance = self._assess_import_security_relevance(import_name)
                differences.append({
                    "type": DiffType.IMPORT_ADDED.value,
                    "description": f"Import added: {import_name}",
                    "security_relevance": security_relevance.value,
                    "import": import_name
                })

            for import_name in removed_imports:
                security_relevance = self._assess_import_security_relevance(import_name)
                differences.append({
                    "type": DiffType.IMPORT_REMOVED.value,
                    "description": f"Import removed: {import_name}",
                    "security_relevance": security_relevance.value,
                    "import": import_name
                })

            # String comparison
            old_strings = self._extract_strings(old_binary)
            new_strings = self._extract_strings(new_binary)

            string_diffs = self._compare_strings(old_strings, new_strings)
            differences.extend(string_diffs)

            # Function signature analysis (if tools available)
            if CAPSTONE_AVAILABLE:
                func_diffs = self._compare_function_signatures(old_binary, new_binary)
                differences.extend(func_diffs)

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Intermediate analysis error: {e}")

        return differences

    def _comprehensive_analysis(self, old_binary: str, new_binary: str) -> List[Dict[str, Any]]:
        """Perform comprehensive binary analysis."""
        differences = []

        try:
            # Control flow analysis
            if CAPSTONE_AVAILABLE:
                cfg_diffs = self._compare_control_flow(old_binary, new_binary)
                differences.extend(cfg_diffs)

            # Security mitigation analysis
            mitigation_diffs = self._compare_security_mitigations(old_binary, new_binary)
            differences.extend(mitigation_diffs)

            # Vulnerability pattern analysis
            vuln_diffs = self._compare_vulnerability_patterns(old_binary, new_binary)
            differences.extend(vuln_diffs)

            # Advanced binary structure analysis
            if LIEF_AVAILABLE:
                structure_diffs = self._compare_binary_structure(old_binary, new_binary)
                differences.extend(structure_diffs)

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Comprehensive analysis error: {e}")

        return differences
    def _analyze_security_impact(self, differences: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze the security impact of identified differences."""
        impact = {
            "overall_risk": SecurityRelevance.LOW.value,
            "categories": {},
            "critical_changes": [],
            "mitigation_changes": [],
            "vulnerability_indicators": []
        }

        try:
            # Categorize differences
            categories = {}
            critical_changes = []
            mitigation_changes = []
            vulnerability_indicators = []

            for diff in differences:
                diff_type = diff["type"]
                security_level = diff["security_relevance"]

                # Count by category
                if diff_type not in categories:
                    categories[diff_type] = 0
                categories[diff_type] += 1

                # Identify critical changes
                if security_level in ["critical", "high"]:
                    critical_changes.append(diff)

                # Identify mitigation changes
                if "mitigation" in diff.get("description", "").lower():
                    mitigation_changes.append(diff)

                # Identify vulnerability indicators
                if any(vuln in diff.get("description", "").lower()
                       for vuln in ["overflow", "underflow", "injection", "bypass"]):
                    vulnerability_indicators.append(diff)

            # Determine overall risk
            if critical_changes:
                impact["overall_risk"] = SecurityRelevance.CRITICAL.value
            elif len([d for d in differences if d["security_relevance"] == "high"]) > 3:
                impact["overall_risk"] = SecurityRelevance.HIGH.value
            elif len([d for d in differences if d["security_relevance"] == "medium"]) > 5:
                impact["overall_risk"] = SecurityRelevance.MEDIUM.value

            impact["categories"] = categories
            impact["critical_changes"] = critical_changes
            impact["mitigation_changes"] = mitigation_changes
            impact["vulnerability_indicators"] = vulnerability_indicators

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Security impact analysis error: {e}")

        return impact

    def _generate_statistics(self, differences: List[Dict[str, Any]],
                           old_binary: str, new_binary: str) -> Dict[str, Any]:
        """Generate statistics about the binary comparison."""
        def _compute_stats():
            stats = {
                "total_differences": len(differences),
                "by_type": AnalysisStatsGenerator.count_by_attribute(differences, "type"),
                "by_security_level": AnalysisStatsGenerator.count_by_attribute(differences, "security_relevance"),
                "file_info": {},
                "change_summary": {}
            }

            # File information
            stats["file_info"] = {
                "old_binary": {
                    "path": old_binary,
                    "size": os.path.getsize(old_binary),
                    "hash": self._calculate_file_hash(old_binary)
                },
                "new_binary": {
                    "path": new_binary,
                    "size": os.path.getsize(new_binary),
                    "hash": self._calculate_file_hash(new_binary)
                }
            }

            # Change summary
            stats["change_summary"] = {
                "additions": len([d for d in differences if "added" in d["type"]]),
                "removals": len([d for d in differences if "removed" in d["type"]]),
                "modifications": len([d for d in differences if "modified" in d["type"]]),
                "security_relevant": len([d for d in differences if d["security_relevance"] in ["high", "critical"]])
            }

            return stats

        # Use safe statistics generation
        result = AnalysisStatsGenerator.safe_stats_generation(_compute_stats)
        if not result:
            # Return default stats on error
            return {
                "total_differences": 0,
                "by_type": {},
                "by_security_level": {},
                "file_info": {},
                "change_summary": {}
            }
        return result

    def _generate_recommendations(self, differences: List[Dict[str, Any]],
                                security_impact: Dict[str, Any]) -> List[str]:
        """Generate recommendations based on analysis results."""
        def _compute_recommendations():
            recommendations = []

            # Critical changes recommendations
            if security_impact["critical_changes"]:
                recommendations.append("CRITICAL: Review all critical security changes immediately")
                recommendations.append("Perform thorough security testing before deployment")

            # Mitigation changes
            if security_impact["mitigation_changes"]:
                recommendations.append("Review security mitigation changes for potential bypasses")

            # Vulnerability indicators
            if security_impact["vulnerability_indicators"]:
                recommendations.append("Investigate potential vulnerability indicators")
                recommendations.append("Consider additional security testing and code review")

            # Import changes
            import_changes = [d for d in differences if "import" in d["type"]]
            if import_changes:
                recommendations.append("Review import table changes for security implications")

            # Function changes
            function_changes = [d for d in differences if "function" in d["type"]]
            if function_changes:
                recommendations.append("Analyze function changes for logic vulnerabilities")

            # General recommendations
            if len(differences) > 20:
                recommendations.append("Large number of changes detected - consider staged testing")

            if not recommendations:
                recommendations.append("Changes appear minimal - standard testing recommended")

            return recommendations

        # Use safe recommendation generation
        return AnalysisStatsGenerator.safe_recommendation_generation(_compute_recommendations)

    # Helper methods for binary analysis

    def _calculate_file_hash(self, file_path: str) -> str:
        """Calculate SHA256 hash of file."""
        try:
            hasher = hashlib.sha256()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except (OSError, ValueError, RuntimeError) as e:
            self.logger.error("Error in binary_differ: %s", e)
            return ""

    def _get_basic_sections(self, binary_path: str) -> Dict[str, str]:
        """Extract basic section information using hexdump."""
        sections = {}

        try:
            # Use objdump if available
            result = subprocess.run(["objdump", "-h", binary_path],
                                  capture_output=True, text=True)

            if result.returncode == 0:
                lines = result.stdout.split("\n")
                for line in lines:
                    if "CONTENTS" in line:
                        parts = line.split()
                        if len(parts) >= 2:
                            section_name = parts[1]
                            sections[section_name] = line.strip()
            else:
                # Fallback to basic file analysis
                with open(binary_path, "rb") as f:
                    data = f.read(1024)  # Read first 1KB
                    sections["header"] = data.hex()

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Section extraction error: {e}")

        return sections

    def _extract_imports(self, binary_path: str) -> List[str]:
        """Extract import table from binary."""
        imports = []

        try:
            # Try objdump first
            result = subprocess.run(["objdump", "-T", binary_path],
                                  capture_output=True, text=True)

            if result.returncode == 0:
                lines = result.stdout.split("\n")
                for line in lines:
                    if "DF" in line or "*UND*" in line:
                        parts = line.split()
                        if len(parts) >= 7:
                            import_name = parts[-1]
                            imports.append(import_name)
            else:
                # Try nm command
                result = subprocess.run(["nm", "-D", binary_path],
                                      capture_output=True, text=True)
                if result.returncode == 0:
                    lines = result.stdout.split("\n")
                    for line in lines:
                        if "U " in line:
                            parts = line.split()
                            if len(parts) >= 2:
                                imports.append(parts[-1])

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Import extraction error: {e}")

        return list(set(imports))  # Remove duplicates

    def _extract_strings(self, binary_path: str) -> List[str]:
        """Extract strings from binary."""
        strings = []

        try:
            # Use strings command
            result = subprocess.run(["strings", "-n", "4", binary_path],
                                  capture_output=True, text=True)

            if result.returncode == 0:
                strings = result.stdout.strip().split("\n")
                # Filter for meaningful strings
                strings = [s for s in strings if len(s) >= 4 and len(s) <= 100]

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"String extraction error: {e}")

        return strings

    def _compare_strings(self, old_strings: List[str], new_strings: List[str]) -> List[Dict[str, Any]]:
        """Compare string tables between binaries."""
        differences = []

        try:
            old_set = set(old_strings)
            new_set = set(new_strings)

            added_strings = new_set - old_set
            removed_strings = old_set - new_set

            # Look for security-relevant strings
            security_keywords = ["password", "secret", "key", "token", "admin", "root", "auth"]

            # Analyze added strings for security impact
            for string in added_strings:
                security_relevance = SecurityRelevance.LOW

                # Enhance security analysis with keyword matching
                string_lower = string.lower()
                matched_keywords = [kw for kw in security_keywords if kw in string_lower]

                if matched_keywords:
                    security_relevance = SecurityRelevance.MEDIUM
                    # Log matched security keywords for investigation
                    self.logger.info(f"Added string contains security keywords {matched_keywords}: {string[:30]}...")

                differences.append({
                    "type": "string_added",
                    "description": f"String added: {string[:50]}...",
                    "security_relevance": security_relevance.value,
                    "string": string,
                    "matched_keywords": matched_keywords
                })

            # Analyze removed strings for security impact
            for string in removed_strings:
                security_relevance = SecurityRelevance.LOW

                # Enhance security analysis with keyword matching
                string_lower = string.lower()
                matched_keywords = [kw for kw in security_keywords if kw in string_lower]

                if matched_keywords:
                    security_relevance = SecurityRelevance.MEDIUM
                    # Log matched security keywords for investigation
                    self.logger.info(f"Removed string contains security keywords {matched_keywords}: {string[:30]}...")

                differences.append({
                    "type": "string_removed",
                    "description": f"String removed: {string[:50]}...",
                    "security_relevance": security_relevance.value,
                    "string": string,
                    "matched_keywords": matched_keywords
                })

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"String comparison error: {e}")

        return differences

    def _assess_import_security_relevance(self, import_name: str) -> SecurityRelevance:
        """Assess security relevance of an import."""
        import_lower = import_name.lower()

        # Check against security function categories
        for category, functions in self.security_functions.items():
            if any(func in import_lower for func in functions):
                if category in ["crypto", "auth"]:
                    return SecurityRelevance.HIGH
                elif category in ["memory", "network"]:
                    return SecurityRelevance.MEDIUM
                else:
                    return SecurityRelevance.LOW

        return SecurityRelevance.LOW

    def _assess_function_security_relevance(self, function_name: str) -> SecurityRelevance:
        """Assess security relevance of a function based on its name."""
        func_lower = function_name.lower()

        # Check against security function categories
        for category, functions in self.security_functions.items():
            if any(func in func_lower for func in functions):
                if category in ["crypto", "auth"]:
                    return SecurityRelevance.HIGH
                elif category in ["memory", "network", "process"]:
                    return SecurityRelevance.MEDIUM
                else:
                    return SecurityRelevance.LOW

        # Check for vulnerability patterns
        for _vuln_type, patterns in self.vuln_patterns.items():
            if any(pattern in func_lower for pattern in patterns):
                return SecurityRelevance.HIGH

        # Check for security mitigations
        for _mitigation, indicators in self.security_mitigations.items():
            if any(indicator.lower() in func_lower for indicator in indicators):
                return SecurityRelevance.MEDIUM

        return SecurityRelevance.LOW

    def _compare_function_signatures(self, old_binary: str, new_binary: str) -> List[Dict[str, Any]]:
        """Compare function signatures using Capstone."""
        differences = []

        if not CAPSTONE_AVAILABLE:
            self.logger.debug(f"Capstone not available for function signature comparison of {old_binary} vs {new_binary}")
            return differences

        try:
            # Log the binaries being compared
            self.logger.debug(f"Comparing function signatures: {old_binary} vs {new_binary}")

            # Extract functions from both binaries
            old_functions = self._extract_functions(old_binary)
            new_functions = self._extract_functions(new_binary)

            # Compare function counts and basic metrics
            if len(old_functions) != len(new_functions):
                differences.append({
                    "type": DiffType.FUNCTION_MODIFIED.value,
                    "description": f"Function count changed: {len(old_functions)} -> {len(new_functions)}",
                    "security_relevance": SecurityRelevance.MEDIUM.value,
                    "old_count": len(old_functions),
                    "new_count": len(new_functions)
                })

            # Find added/removed functions
            old_names = {f.get("name", f"sub_{addr:x}") for addr, f in old_functions.items()}
            new_names = {f.get("name", f"sub_{addr:x}") for addr, f in new_functions.items()}

            added_funcs = new_names - old_names
            removed_funcs = old_names - new_names

            for func_name in added_funcs:
                security_relevance = self._assess_function_security_relevance(func_name)
                differences.append({
                    "type": DiffType.FUNCTION_ADDED.value,
                    "description": f"Function added: {func_name}",
                    "security_relevance": security_relevance.value,
                    "function": func_name
                })

            for func_name in removed_funcs:
                security_relevance = self._assess_function_security_relevance(func_name)
                differences.append({
                    "type": DiffType.FUNCTION_REMOVED.value,
                    "description": f"Function removed: {func_name}",
                    "security_relevance": security_relevance.value,
                    "function": func_name
                })

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Function signature comparison error for {old_binary} vs {new_binary}: {e}")

        return differences

    def _compare_control_flow(self, old_binary: str, new_binary: str) -> List[Dict[str, Any]]:
        """Compare control flow graphs."""
        differences = []

        try:
            self.logger.debug(f"Comparing control flow: {old_binary} vs {new_binary}")

            # If Capstone is available, use it for detailed control flow analysis
            if CAPSTONE_AVAILABLE:
                capstone_diffs = self._compare_control_flow_with_capstone(old_binary, new_binary)
                differences.extend(capstone_diffs)

            # Basic control flow indicators through string analysis (fallback/additional)
            old_strings = self._extract_strings(old_binary)
            new_strings = self._extract_strings(new_binary)

            # Look for control flow related patterns
            control_flow_patterns = {
                "jumps": ["jmp", "je", "jne", "jz", "jnz", "call", "ret"],
                "conditions": ["if", "else", "switch", "case", "cmp", "test"],
                "loops": ["for", "while", "loop", "jmp"],
                "error_handlers": ["catch", "except", "finally", "error", "fail"]
            }

            old_cf_indicators = {cat: 0 for cat in control_flow_patterns}
            new_cf_indicators = {cat: 0 for cat in control_flow_patterns}

            # Count control flow indicators
            for string in old_strings:
                for category, patterns in control_flow_patterns.items():
                    if any(pattern in string.lower() for pattern in patterns):
                        old_cf_indicators[category] += 1

            for string in new_strings:
                for category, patterns in control_flow_patterns.items():
                    if any(pattern in string.lower() for pattern in patterns):
                        new_cf_indicators[category] += 1

            # Compare indicators
            for category in control_flow_patterns:
                if old_cf_indicators[category] != new_cf_indicators[category]:
                    change = new_cf_indicators[category] - old_cf_indicators[category]
                    differences.append({
                        "type": "control_flow_change",
                        "description": f"Control flow {category} changed by {change:+d} ({old_cf_indicators[category]} -> {new_cf_indicators[category]})",
                        "security_relevance": SecurityRelevance.MEDIUM.value if abs(change) > 5 else SecurityRelevance.LOW.value,
                        "category": category,
                        "old_count": old_cf_indicators[category],
                        "new_count": new_cf_indicators[category]
                    })

            # If we have significant changes in error handlers, that's security relevant
            if abs(new_cf_indicators["error_handlers"] - old_cf_indicators["error_handlers"]) > 3:
                differences.append({
                    "type": "error_handling_change",
                    "description": "Significant change in error handling detected",
                    "security_relevance": SecurityRelevance.HIGH.value
                })

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Control flow comparison error for {old_binary} vs {new_binary}: {e}")

        return differences

    def _compare_security_mitigations(self, old_binary: str, new_binary: str) -> List[Dict[str, Any]]:
        """Compare security mitigations between binaries."""
        differences = []

        try:
            old_mitigations = self._detect_security_mitigations(old_binary)
            new_mitigations = self._detect_security_mitigations(new_binary)

            # Compare mitigation presence
            for mitigation, _indicators in self.security_mitigations.items():
                old_present = old_mitigations.get(mitigation, False)
                new_present = new_mitigations.get(mitigation, False)

                if old_present and not new_present:
                    differences.append({
                        "type": DiffType.SECURITY_FEATURE_REMOVED.value,
                        "description": f"Security mitigation removed: {mitigation}",
                        "security_relevance": SecurityRelevance.CRITICAL.value,
                        "mitigation": mitigation
                    })
                elif not old_present and new_present:
                    differences.append({
                        "type": DiffType.SECURITY_FEATURE_ADDED.value,
                        "description": f"Security mitigation added: {mitigation}",
                        "security_relevance": SecurityRelevance.HIGH.value,
                        "mitigation": mitigation
                    })

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Security mitigation comparison error: {e}")

        return differences

    def _detect_security_mitigations(self, binary_path: str) -> Dict[str, bool]:
        """Detect security mitigations in binary."""
        mitigations = {}

        try:
            # Extract strings and symbols
            strings = self._extract_strings(binary_path)
            imports = self._extract_imports(binary_path)

            all_symbols = strings + imports
            all_text = " ".join(all_symbols).lower()

            # Check for each mitigation
            for mitigation, indicators in self.security_mitigations.items():
                mitigations[mitigation] = any(
                    indicator.lower() in all_text for indicator in indicators
                )

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Mitigation detection error: {e}")

        return mitigations

    def _compare_vulnerability_patterns(self, old_binary: str, new_binary: str) -> List[Dict[str, Any]]:
        """Compare vulnerability patterns between binaries."""
        differences = []

        try:
            old_patterns = self._detect_vulnerability_patterns(old_binary)
            new_patterns = self._detect_vulnerability_patterns(new_binary)

            # Compare pattern presence
            for vuln_type, _patterns in self.vuln_patterns.items():
                old_count = old_patterns.get(vuln_type, 0)
                new_count = new_patterns.get(vuln_type, 0)

                if new_count > old_count:
                    differences.append({
                        "type": "vulnerability_pattern_increased",
                        "description": f"Potential {vuln_type} patterns increased: {old_count} -> {new_count}",
                        "security_relevance": SecurityRelevance.HIGH.value,
                        "vulnerability_type": vuln_type,
                        "old_count": old_count,
                        "new_count": new_count
                    })
                elif new_count < old_count:
                    differences.append({
                        "type": "vulnerability_pattern_decreased",
                        "description": f"Potential {vuln_type} patterns decreased: {old_count} -> {new_count}",
                        "security_relevance": SecurityRelevance.MEDIUM.value,
                        "vulnerability_type": vuln_type,
                        "old_count": old_count,
                        "new_count": new_count
                    })

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Vulnerability pattern comparison error: {e}")

        return differences

    def _detect_vulnerability_patterns(self, binary_path: str) -> Dict[str, int]:
        """Detect vulnerability patterns in binary."""
        patterns = {}

        try:
            # Extract strings and imports
            strings = self._extract_strings(binary_path)
            imports = self._extract_imports(binary_path)

            all_symbols = strings + imports
            all_text = " ".join(all_symbols).lower()

            # Count patterns for each vulnerability type
            for vuln_type, pattern_list in self.vuln_patterns.items():
                count = sum(
                    all_text.count(pattern.lower()) for pattern in pattern_list
                )
                patterns[vuln_type] = count

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Vulnerability pattern detection error: {e}")

        return patterns

    def _compare_binary_structure(self, old_binary: str, new_binary: str) -> List[Dict[str, Any]]:
        """Compare binary structure using LIEF."""
        differences = []

        if not LIEF_AVAILABLE:
            self.logger.debug(f"LIEF not available for binary structure comparison of {old_binary} vs {new_binary}")
            # Fallback to basic structure analysis
            try:
                # Compare file headers
                with open(old_binary, "rb") as f:
                    old_header = f.read(1024)
                with open(new_binary, "rb") as f:
                    new_header = f.read(1024)

                # Check for PE/ELF/Mach-O signatures
                if old_header[:2] != new_header[:2]:
                    differences.append({
                        "type": "binary_format_change",
                        "description": "Binary format signature changed",
                        "security_relevance": SecurityRelevance.HIGH.value
                    })

                # Check for architecture changes (basic heuristic)
                if old_header[4:8] != new_header[4:8]:
                    differences.append({
                        "type": "architecture_change",
                        "description": "Binary architecture indicators changed",
                        "security_relevance": SecurityRelevance.MEDIUM.value
                    })

            except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
                self.logger.debug(f"Fallback binary structure analysis error: {e}")
            return differences

        try:
            self.logger.info(f"Comparing binary structure using LIEF: {old_binary} vs {new_binary}")

            # Parse binaries with LIEF
            if not hasattr(lief, "parse"):
                self.logger.warning("lief.parse not available")
                return differences

            # Use lief.parse with proper error handling
            try:
                old_bin = lief.parse(old_binary)
                new_bin = lief.parse(new_binary)
            except AttributeError:
                self.logger.warning("lief.parse not available - lief may not be properly installed")
                return differences
            except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
                self.logger.warning(f"Failed to parse binaries with LIEF: {e}")
                return differences

            if not old_bin or not new_bin:
                self.logger.warning("Failed to parse one or both binaries with LIEF")
                return differences

            # Compare binary types
            if type(old_bin) != type(new_bin):
                differences.append({
                    "type": "binary_type_change",
                    "description": f"Binary type changed: {type(old_bin).__name__} -> {type(new_bin).__name__}",
                    "security_relevance": SecurityRelevance.CRITICAL.value
                })
                return differences

            # PE specific comparisons
            if hasattr(old_bin, "optional_header") and hasattr(new_bin, "optional_header"):
                old_opt = old_bin.optional_header
                new_opt = new_bin.optional_header

                # Check security features
                if hasattr(old_opt, "dll_characteristics") and hasattr(new_opt, "dll_characteristics"):
                    old_chars = old_opt.dll_characteristics
                    new_chars = new_opt.dll_characteristics

                    # Check for DEP/NX
                    old_dep = bool(old_chars & 0x0100)  # IMAGE_DLLCHARACTERISTICS_NX_COMPAT
                    new_dep = bool(new_chars & 0x0100)
                    if old_dep != new_dep:
                        differences.append({
                            "type": DiffType.SECURITY_FEATURE_ADDED.value if new_dep else DiffType.SECURITY_FEATURE_REMOVED.value,
                            "description": f'DEP/NX {"enabled" if new_dep else "disabled"}',
                            "security_relevance": SecurityRelevance.CRITICAL.value
                        })

                    # Check for ASLR
                    old_aslr = bool(old_chars & 0x0040)  # IMAGE_DLLCHARACTERISTICS_DYNAMIC_BASE
                    new_aslr = bool(new_chars & 0x0040)
                    if old_aslr != new_aslr:
                        differences.append({
                            "type": DiffType.SECURITY_FEATURE_ADDED.value if new_aslr else DiffType.SECURITY_FEATURE_REMOVED.value,
                            "description": f'ASLR {"enabled" if new_aslr else "disabled"}',
                            "security_relevance": SecurityRelevance.CRITICAL.value
                        })

                # Check entry point
                if hasattr(old_opt, "addressof_entrypoint") and hasattr(new_opt, "addressof_entrypoint"):
                    if old_opt.addressof_entrypoint != new_opt.addressof_entrypoint:
                        differences.append({
                            "type": "entry_point_change",
                            "description": f"Entry point changed: 0x{old_opt.addressof_entrypoint:x} -> 0x{new_opt.addressof_entrypoint:x}",
                            "security_relevance": SecurityRelevance.HIGH.value
                        })

            # Section comparisons
            if hasattr(old_bin, "sections") and hasattr(new_bin, "sections"):
                old_sections = {s.name: s for s in old_bin.sections}
                new_sections = {s.name: s for s in new_bin.sections}

                # Check for section changes
                for name in set(old_sections.keys()) | set(new_sections.keys()):
                    if name in old_sections and name not in new_sections:
                        differences.append({
                            "type": DiffType.SECTION_REMOVED.value,
                            "description": f"Section removed: {name}",
                            "security_relevance": SecurityRelevance.MEDIUM.value
                        })
                    elif name not in old_sections and name in new_sections:
                        new_sec = new_sections[name]
                        new_size = getattr(new_sec, "size", 0)

                        differences.append({
                            "type": DiffType.SECTION_ADDED.value,
                            "description": f"Section added: {name} (size: {new_size})",
                            "security_relevance": SecurityRelevance.MEDIUM.value,
                            "section_size": new_size
                        })
                    elif name in old_sections and name in new_sections:
                        old_sec = old_sections[name]
                        new_sec = new_sections[name]

                        # Check section characteristics
                        if hasattr(old_sec, "characteristics") and hasattr(new_sec, "characteristics"):
                            old_chars = old_sec.characteristics
                            new_chars = new_sec.characteristics

                            if old_chars != new_chars:
                                differences.append({
                                    "type": "section_permissions_change",
                                    "description": f"Section {name} permissions changed (0x{old_chars:x} -> 0x{new_chars:x})",
                                    "security_relevance": SecurityRelevance.HIGH.value,
                                    "old_characteristics": old_chars,
                                    "new_characteristics": new_chars
                                })

                        # Check section size changes
                        old_size = getattr(old_sec, "size", 0)
                        new_size = getattr(new_sec, "size", 0)

                        if old_size != new_size:
                            size_change = new_size - old_size
                            differences.append({
                                "type": "section_size_change",
                                "description": f"Section {name} size changed: {old_size} -> {new_size} ({size_change:+d})",
                                "security_relevance": SecurityRelevance.LOW.value if abs(size_change) < 1024 else SecurityRelevance.MEDIUM.value,
                                "old_size": old_size,
                                "new_size": new_size,
                                "size_change": size_change
                            })

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Binary structure comparison error for {old_binary} vs {new_binary}: {e}")

        return differences

    def _analyze_patch_content(self, patch_content: str) -> Dict[str, Any]:
        """Analyze patch file content for security implications."""
        analysis = {
            "patch_type": "unknown",
            "security_fixes": [],
            "potential_vulnerabilities": [],
            "impact_assessment": {}
        }

        try:
            # Analyze patch for security keywords
            security_keywords = {
                "fixes": ["fix", "patch", "security", "vulnerability", "cve"],
                "dangerous": ["buffer", "overflow", "injection", "bypass", "privilege"],
                "crypto": ["encrypt", "decrypt", "hash", "random", "crypto"],
                "auth": ["auth", "login", "password", "token", "access"]
            }

            patch_lower = patch_content.lower()

            for category, keywords in security_keywords.items():
                for keyword in keywords:
                    if keyword in patch_lower:
                        if category == "fixes":
                            analysis["security_fixes"].append(keyword)
                        elif category == "dangerous":
                            analysis["potential_vulnerabilities"].append(keyword)

            # Determine patch type
            if analysis["security_fixes"]:
                analysis["patch_type"] = "security_fix"
            elif analysis["potential_vulnerabilities"]:
                analysis["patch_type"] = "potentially_dangerous"
            else:
                analysis["patch_type"] = "regular"

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Patch content analysis error: {e}")

        return analysis

    def _map_patch_to_binary(self, patch_content: str, binary_path: str) -> Dict[str, Any]:
        """Map patch changes to binary file."""
        mapping = {
            "affected_functions": [],
            "affected_sections": [],
            "binary_changes": [],
            "patch_size": len(patch_content),
            "binary_info": {}
        }

        try:
            self.logger.debug(f"Mapping patch to binary: {binary_path} (patch size: {len(patch_content)} bytes)")

            # Get binary information
            if os.path.exists(binary_path):
                mapping["binary_info"] = {
                    "path": binary_path,
                    "size": os.path.getsize(binary_path),
                    "hash": self._calculate_file_hash(binary_path)
                }

            # Parse patch content for addresses and function names
            patch_lines = patch_content.split("\n")

            # Look for hex addresses (common in patches)
            import re
            hex_pattern = r"0x[0-9a-fA-F]+"
            func_pattern = r"(?:function|func|sub_|FUN_)[0-9a-zA-Z_]+"

            addresses = []
            functions = []

            for line in patch_lines:
                # Find hex addresses
                hex_matches = re.findall(hex_pattern, line)
                addresses.extend(hex_matches)

                # Find function references
                func_matches = re.findall(func_pattern, line, re.IGNORECASE)
                functions.extend(func_matches)

            # Extract functions from binary to cross-reference
            binary_functions = self._extract_functions(binary_path)
            binary_func_names = {f.get("name", f"sub_{addr:x}").lower() for addr, f in binary_functions.items()}

            # Map patch references to binary functions
            matched_functions = 0
            for func_ref in set(functions):
                func_lower = func_ref.lower()
                matching_funcs = [bin_func for bin_func in binary_func_names if func_lower in bin_func]

                if matching_funcs:
                    security_relevance = self._assess_function_security_relevance(func_ref)
                    mapping["affected_functions"].append({
                        "name": func_ref,
                        "type": "direct_reference",
                        "security_relevance": security_relevance.value,
                        "matches": matching_funcs[:5]  # First 5 matches
                    })
                    matched_functions += 1

            if matched_functions > 0:
                self.logger.debug(f"Patch references {matched_functions} functions in binary")

            # Analyze patch for section references
            section_keywords = [".text", ".data", ".rdata", ".bss", ".reloc", ".rsrc", ".init", ".plt", ".got"]
            section_references = {}

            for line in patch_lines:
                for section in section_keywords:
                    if section in line:
                        if section not in section_references:
                            section_references[section] = 0
                        section_references[section] += 1
                        mapping["affected_sections"].append(section)

            mapping["affected_sections"] = list(set(mapping["affected_sections"]))

            # Log section reference statistics
            if section_references:
                self.logger.debug(f"Patch contains section references: {section_references}")
                mapping["section_reference_counts"] = section_references

            # Determine patch type based on content
            if addresses:
                mapping["binary_changes"].append({
                    "description": f"Patch contains {len(set(addresses))} unique address references",
                    "type": "address_patch",
                    "addresses": list(set(addresses))[:10]  # First 10 unique addresses
                })

            if mapping["affected_functions"]:
                mapping["binary_changes"].append({
                    "description": f'Patch affects {len(mapping["affected_functions"])} functions',
                    "type": "function_patch"
                })

            if mapping["affected_sections"]:
                mapping["binary_changes"].append({
                    "description": f'Patch references {len(mapping["affected_sections"])} sections',
                    "type": "section_patch"
                })

            # Analyze patch patterns
            patch_lower = patch_content.lower()
            if "nop" in patch_lower or "0x90" in patch_lower:
                mapping["binary_changes"].append({
                    "description": "Patch contains NOP instructions (possible bypass)",
                    "type": "nop_patch",
                    "security_relevance": SecurityRelevance.HIGH.value
                })

            if "jmp" in patch_lower or "call" in patch_lower:
                mapping["binary_changes"].append({
                    "description": "Patch modifies control flow (JMP/CALL)",
                    "type": "control_flow_patch",
                    "security_relevance": SecurityRelevance.MEDIUM.value
                })

            if not mapping["binary_changes"]:
                mapping["binary_changes"].append({
                    "description": "Patch analysis complete",
                    "type": "generic_patch"
                })

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Patch-to-binary mapping error for {binary_path}: {e}")
            mapping["binary_changes"].append({
                "description": f"Mapping error: {str(e)}",
                "type": "error"
            })

        return mapping

    def _extract_functions(self, binary_path: str) -> Dict[int, Dict[str, Any]]:
        """Extract function information from binary."""
        functions = {}

        try:
            # Use objdump to get function symbols
            result = subprocess.run(["objdump", "-t", binary_path],
                                  capture_output=True, text=True, check=False)

            if result.returncode == 0:
                lines = result.stdout.split("\n")
                functions_found = 0

                for line in lines:
                    if "F .text" in line:
                        parts = line.split()
                        if len(parts) >= 6:
                            try:
                                addr = int(parts[0], 16)
                                size = int(parts[4], 16) if parts[4] != "" and parts[4].isdigit() else 0
                                name = parts[-1]

                                # Enhanced function analysis
                                security_relevance = self._assess_function_security_relevance(name)

                                # Extract actual function bytes if size is known
                                func_bytes = b""
                                if size > 0 and CAPSTONE_AVAILABLE:
                                    func_bytes = self._extract_function_bytes_with_capstone(binary_path, addr, size)

                                functions[addr] = {
                                    "name": name,
                                    "size": size,
                                    "bytes": func_bytes,
                                    "analysis": {
                                        "security_relevance": security_relevance.value,
                                        "extracted_from": binary_path,
                                        "has_disassembly": len(func_bytes) > 0
                                    }
                                }
                                functions_found += 1

                            except (ValueError, IndexError) as parse_error:
                                self.logger.debug(f"Failed to parse function line '{line}': {parse_error}")
                                continue

                self.logger.debug(f"Extracted {functions_found} functions from {binary_path}")
            else:
                # Try alternative approach with nm if objdump fails
                nm_result = subprocess.run(["nm", "-C", "-D", binary_path],
                                         capture_output=True, text=True, check=False)
                if nm_result.returncode == 0:
                    nm_lines = nm_result.stdout.split("\n")
                    nm_functions_found = 0

                    for line in nm_lines:
                        if " T " in line or " t " in line:  # Text symbols
                            parts = line.split()
                            if len(parts) >= 3:
                                try:
                                    addr = int(parts[0], 16)
                                    name = parts[-1]

                                    security_relevance = self._assess_function_security_relevance(name)

                                    functions[addr] = {
                                        "name": name,
                                        "size": 0,  # nm doesn't provide size
                                        "bytes": b"",
                                        "analysis": {
                                            "security_relevance": security_relevance.value,
                                            "extracted_from": binary_path,
                                            "extraction_method": "nm"
                                        }
                                    }
                                    nm_functions_found += 1

                                except (ValueError, IndexError) as parse_error:
                                    self.logger.debug(f"Failed to parse nm line '{line}': {parse_error}")
                                    continue

                    self.logger.debug(f"Extracted {nm_functions_found} functions from {binary_path} using nm")

        except (OSError, ValueError, RuntimeError, AttributeError, KeyError) as e:
            self.logger.debug(f"Function extraction error for {binary_path}: {e}")

        return functions

    def _calculate_function_similarity(self, func1: bytes, func2: bytes) -> float:
        """Calculate similarity between two functions."""
        try:
            if not func1 or not func2:
                return 0.0

            # Simple byte-based similarity
            min_len = min(len(func1), len(func2))
            max_len = max(len(func1), len(func2))

            if max_len == 0:
                return 1.0

            matches = sum(1 for i in range(min_len) if func1[i] == func2[i])
            similarity = matches / max_len

            return similarity

        except (OSError, ValueError, RuntimeError) as e:
            self.logger.error("Error in binary_differ: %s", e)
            return 0.0

    def _extract_function_bytes_with_capstone(self, binary_path: str, func_addr: int, func_size: int) -> bytes:
        """Extract and disassemble function bytes using Capstone."""
        if not CAPSTONE_AVAILABLE:
            return b""

        try:
            # Read the binary file
            with open(binary_path, "rb") as f:
                # Read the entire file to handle address mappings
                binary_data = f.read()

            # Try to use LIEF to get proper file offsets
            if LIEF_AVAILABLE:
                try:
                    binary = lief.parse(binary_path)
                    if binary:
                        # Find the section containing the function
                        for section in binary.sections:
                            if hasattr(section, "virtual_address") and hasattr(section, "size"):
                                sec_start = section.virtual_address
                                sec_end = sec_start + section.size

                                if sec_start <= func_addr < sec_end:
                                    # Calculate offset within section
                                    offset_in_section = func_addr - sec_start
                                    file_offset = section.offset + offset_in_section

                                    # Extract function bytes
                                    if file_offset + func_size <= len(binary_data):
                                        func_bytes = binary_data[file_offset:file_offset + func_size]

                                        # Use Capstone to verify it's valid code
                                        if self._verify_code_with_capstone(func_bytes, func_addr):
                                            return func_bytes
                                        break
                except Exception as e:
                    self.logger.debug(f"LIEF parsing failed: {e}")

            # Fallback: Try direct offset (works for some simple binaries)
            if func_addr < len(binary_data):
                func_bytes = binary_data[func_addr:func_addr + func_size]
                if self._verify_code_with_capstone(func_bytes, func_addr):
                    return func_bytes

            return b""

        except Exception as e:
            self.logger.debug(f"Failed to extract function bytes at 0x{func_addr:x}: {e}")
            return b""

    def _verify_code_with_capstone(self, code_bytes: bytes, base_addr: int) -> bool:
        """Verify that bytes are valid code using Capstone disassembler."""
        if not CAPSTONE_AVAILABLE or not code_bytes:
            return False

        try:
            # Determine architecture (simplified - assumes x86_64)
            # In a real implementation, would detect from binary headers
            md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_64)

            # Try to disassemble first few instructions
            instructions = list(md.disasm(code_bytes[:min(32, len(code_bytes))], base_addr))

            # If we can disassemble at least one valid instruction, consider it code
            if instructions:
                self.logger.debug(f"Verified code at 0x{base_addr:x} - First instruction: {instructions[0].mnemonic}")
                return True

            return False

        except Exception as e:
            self.logger.debug(f"Capstone verification failed: {e}")
            return False

    def _compare_control_flow_with_capstone(self, old_binary: str, new_binary: str) -> List[Dict[str, Any]]:
        """Compare control flow using Capstone disassembly."""
        differences = []

        if not CAPSTONE_AVAILABLE:
            return differences

        try:
            # Extract functions from both binaries
            old_functions = self._extract_functions(old_binary)
            new_functions = self._extract_functions(new_binary)

            # Analyze control flow patterns using Capstone
            old_cf_stats = self._analyze_control_flow_patterns(old_binary, old_functions)
            new_cf_stats = self._analyze_control_flow_patterns(new_binary, new_functions)

            # Compare control flow statistics
            for metric, old_value in old_cf_stats.items():
                new_value = new_cf_stats.get(metric, 0)
                if old_value != new_value:
                    change = new_value - old_value

                    # Determine security relevance based on metric type
                    relevance = SecurityRelevance.LOW
                    if metric in ["indirect_jumps", "indirect_calls"]:
                        relevance = SecurityRelevance.HIGH if abs(change) > 10 else SecurityRelevance.MEDIUM
                    elif metric in ["conditional_jumps", "function_calls"]:
                        relevance = SecurityRelevance.MEDIUM if abs(change) > 20 else SecurityRelevance.LOW

                    differences.append({
                        "type": "control_flow_capstone",
                        "description": f"Control flow {metric}: {old_value} -> {new_value} (change: {change:+d})",
                        "security_relevance": relevance.value,
                        "metric": metric,
                        "old_value": old_value,
                        "new_value": new_value,
                        "change": change
                    })

            # Check for new indirect control flow (potential security concern)
            if new_cf_stats.get("indirect_jumps", 0) > old_cf_stats.get("indirect_jumps", 0):
                differences.append({
                    "type": "security_concern",
                    "description": "Increased indirect jumps detected - potential security implications",
                    "security_relevance": SecurityRelevance.HIGH.value
                })

        except Exception as e:
            self.logger.debug(f"Capstone control flow comparison failed: {e}")

        return differences

    def _analyze_control_flow_patterns(self, binary_path: str, functions: Dict[int, Dict[str, Any]]) -> Dict[str, int]:
        """Analyze control flow patterns in binary using Capstone with architecture detection."""
        stats = {
            "conditional_jumps": 0,
            "unconditional_jumps": 0,
            "function_calls": 0,
            "indirect_jumps": 0,
            "indirect_calls": 0,
            "returns": 0,
            "syscalls": 0,
            "interrupts": 0
        }

        if not CAPSTONE_AVAILABLE:
            return stats

        try:
            # Determine architecture from binary path/metadata
            arch_info = self._detect_binary_architecture(binary_path)

            # Setup Capstone disassembler based on detected architecture
            if arch_info["arch"] == "x86":
                if arch_info["bits"] == 64:
                    md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_64)
                else:
                    md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_32)
            elif arch_info["arch"] == "arm":
                if arch_info["bits"] == 64:
                    md = capstone.Cs(capstone.CS_ARCH_ARM64, capstone.CS_MODE_ARM)
                else:
                    md = capstone.Cs(capstone.CS_ARCH_ARM, capstone.CS_MODE_ARM)
            else:
                # Default to x86_64 if architecture detection fails
                md = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_64)

            md.detail = True  # Enable detailed instruction info

            # Add binary path to stats for context
            stats["binary_path"] = binary_path
            stats["detected_arch"] = f"{arch_info['arch']}_{arch_info['bits']}"

            # Analyze each function
            for func_addr, func_info in functions.items():
                func_bytes = func_info.get("bytes", b"")
                if not func_bytes:
                    continue

                # Disassemble function
                for insn in md.disasm(func_bytes, func_addr):
                    # Categorize instructions
                    if insn.group(capstone.x86.X86_GRP_JUMP):
                        # Check if conditional or unconditional jump
                        if insn.mnemonic in ["jmp", "ljmp"]:
                            stats["unconditional_jumps"] += 1
                            # Check for indirect jump
                            if len(insn.operands) > 0 and insn.operands[0].type == capstone.x86.X86_OP_REG:
                                stats["indirect_jumps"] += 1
                        else:
                            stats["conditional_jumps"] += 1

                    elif insn.group(capstone.x86.X86_GRP_CALL):
                        stats["function_calls"] += 1
                        # Check for indirect call
                        if len(insn.operands) > 0 and insn.operands[0].type == capstone.x86.X86_OP_REG:
                            stats["indirect_calls"] += 1

                    elif insn.group(capstone.x86.X86_GRP_RET):
                        stats["returns"] += 1

                    elif insn.mnemonic in ["syscall", "sysenter"]:
                        stats["syscalls"] += 1

                    elif insn.mnemonic.startswith("int"):
                        stats["interrupts"] += 1

        except Exception as e:
            self.logger.debug(f"Control flow pattern analysis failed: {e}")

        return stats

    def get_analysis_cache(self) -> Dict[str, Any]:
        """Get current analysis cache."""
        return self.analysis_cache.copy()

    def clear_analysis_cache(self):
        """Clear analysis cache."""
        self.analysis_cache.clear()

    def _detect_binary_architecture(self, binary_path: str) -> Dict[str, Any]:
        """Detect binary architecture and bitness from file."""
        arch_info = {
            "arch": "x86",
            "bits": 64,
            "endian": "little"
        }

        try:
            # Try to use pefile for PE files
            if PEFILE_AVAILABLE and binary_path.lower().endswith((".exe", ".dll", ".sys")):
                pe = pefile.PE(binary_path)
                if pe.FILE_HEADER.Machine == pefile.MACHINE_TYPE["IMAGE_FILE_MACHINE_I386"]:
                    arch_info.update({"arch": "x86", "bits": 32})
                elif pe.FILE_HEADER.Machine == pefile.MACHINE_TYPE["IMAGE_FILE_MACHINE_AMD64"]:
                    arch_info.update({"arch": "x86", "bits": 64})
                elif pe.FILE_HEADER.Machine == pefile.MACHINE_TYPE["IMAGE_FILE_MACHINE_ARM64"]:
                    arch_info.update({"arch": "arm", "bits": 64})
                pe.close()
                return arch_info

            # Try basic file header analysis for ELF files
            with open(binary_path, "rb") as f:
                header = f.read(64)
                if len(header) >= 16:
                    # ELF magic number check
                    if header[:4] == b"\x7fELF":
                        ei_class = header[4]  # 1=32-bit, 2=64-bit
                        ei_data = header[5]   # 1=little-endian, 2=big-endian
                        e_machine = int.from_bytes(header[18:20], "little" if ei_data == 1 else "big")

                        arch_info["bits"] = 64 if ei_class == 2 else 32
                        arch_info["endian"] = "little" if ei_data == 1 else "big"

                        if e_machine == 0x3E:  # EM_X86_64
                            arch_info["arch"] = "x86"
                        elif e_machine == 0x03:  # EM_386
                            arch_info["arch"] = "x86"
                        elif e_machine == 0x28:  # EM_ARM
                            arch_info["arch"] = "arm"
                        elif e_machine == 0xB7:  # EM_AARCH64
                            arch_info["arch"] = "arm"

        except (OSError, IOError, Exception) as e:
            self.logger.debug(f"Architecture detection failed for {binary_path}: {e}")

        return arch_info

    def export_analysis(self, result: Dict[str, Any], output_file: str, format: str = "json") -> bool:
        """Export analysis results to file."""
        from ...utils.analysis.analysis_exporter import AnalysisExporter
        return AnalysisExporter.export_analysis(result, output_file, format, "binary_diff")
