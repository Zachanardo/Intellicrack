"""
Central Orchestration System for Exploitation Workflows

Orchestrates complex exploitation workflows with state management,
YAML configuration, and comprehensive error recovery mechanisms.

Copyright (C) 2025 Zachary Flint
Licensed under GNU General Public License v3.0
"""

import asyncio
import copy
import json
import logging
import os
import time
import uuid
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple, Union

import yaml
from yaml import SafeLoader, SafeDumper

from ...utils.logger import logger
from .memory_framework import DirectSyscallManager, AdvancedMemoryOperations
from ..unpacking.universal_unpacker import UniversalUnpacker
from ..anti_analysis.titan_hide_engine import TitanHideEngine
from ..reconstruction.import_rebuilder import ImportRebuilder
from ..licensing.activation_bypass import ActivationBypass
from ..devirtualization.vm_translator import VMTranslator
from ..advanced_bypass.stealth_techniques import StealthTechniques

logger = logging.getLogger(__name__)


class WorkflowState(Enum):
    """Workflow execution states"""
    PENDING = "pending"
    RUNNING = "running"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"
    RECOVERING = "recovering"


class TaskState(Enum):
    """Individual task states"""
    WAITING = "waiting"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"
    RETRYING = "retrying"


class TaskPriority(Enum):
    """Task execution priorities"""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4


@dataclass
class TaskDefinition:
    """Workflow task definition"""
    task_id: str
    task_type: str
    name: str
    description: str
    priority: TaskPriority = TaskPriority.MEDIUM
    dependencies: List[str] = field(default_factory=list)
    parameters: Dict[str, Any] = field(default_factory=dict)
    timeout: int = 300  # seconds
    retry_count: int = 3
    retry_delay: int = 5  # seconds
    required: bool = True
    conditional: Optional[str] = None


@dataclass
class TaskResult:
    """Task execution result"""
    task_id: str
    state: TaskState
    result_data: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    execution_time: Optional[float] = None
    retry_count: int = 0


@dataclass
class WorkflowDefinition:
    """Complete workflow definition"""
    workflow_id: str
    name: str
    description: str
    version: str = "1.0"
    created_by: str = "Intellicrack"
    target_binary: Optional[str] = None
    protection_types: List[str] = field(default_factory=list)
    tasks: List[TaskDefinition] = field(default_factory=list)
    global_config: Dict[str, Any] = field(default_factory=dict)
    rollback_enabled: bool = True
    max_execution_time: int = 3600  # 1 hour


@dataclass
class WorkflowExecution:
    """Workflow execution instance"""
    execution_id: str
    workflow_definition: WorkflowDefinition
    state: WorkflowState = WorkflowState.PENDING
    current_task: Optional[str] = None
    task_results: Dict[str, TaskResult] = field(default_factory=dict)
    execution_context: Dict[str, Any] = field(default_factory=dict)
    start_time: Optional[float] = None
    end_time: Optional[float] = None
    error_message: Optional[str] = None
    rollback_data: Dict[str, Any] = field(default_factory=dict)


class TaskExecutor:
    """Executes individual workflow tasks"""
    
    def __init__(self):
        self.memory_ops = AdvancedMemoryOperations()
        self.universal_unpacker = UniversalUnpacker()
        self.titan_hide = TitanHideEngine()
        self.import_rebuilder = ImportRebuilder()
        self.activation_bypass = ActivationBypass()
        self.vm_translator = VMTranslator()
        self.stealth_techniques = StealthTechniques()
        
        self.task_handlers = {
            'memory_analysis': self._execute_memory_analysis,
            'unpacking': self._execute_unpacking,
            'anti_analysis_bypass': self._execute_anti_analysis_bypass,
            'import_reconstruction': self._execute_import_reconstruction,
            'license_bypass': self._execute_license_bypass,
            'vm_devirtualization': self._execute_vm_devirtualization,
            'stealth_setup': self._execute_stealth_setup,
            'process_injection': self._execute_process_injection,
            'network_bypass': self._execute_network_bypass,
            'file_patching': self._execute_file_patching,
            'registry_modification': self._execute_registry_modification,
            'api_hooking': self._execute_api_hooking,
        }
    
    async def execute_task(self, task: TaskDefinition, context: Dict[str, Any]) -> TaskResult:
        """Execute a single workflow task"""
        task_result = TaskResult(
            task_id=task.task_id,
            state=TaskState.RUNNING,
            start_time=time.time()
        )
        
        try:
            logger.info(f"Executing task: {task.name} ({task.task_id})")
            
            # Check if task handler exists
            if task.task_type not in self.task_handlers:
                raise ValueError(f"Unknown task type: {task.task_type}")
            
            # Execute task with timeout
            handler = self.task_handlers[task.task_type]
            result_data = await asyncio.wait_for(
                handler(task, context),
                timeout=task.timeout
            )
            
            task_result.state = TaskState.COMPLETED
            task_result.result_data = result_data
            task_result.end_time = time.time()
            task_result.execution_time = task_result.end_time - task_result.start_time
            
            logger.info(f"Task completed: {task.name} ({task_result.execution_time:.2f}s)")
            
        except asyncio.TimeoutError:
            task_result.state = TaskState.FAILED
            task_result.error_message = f"Task timed out after {task.timeout}s"
            task_result.end_time = time.time()
            logger.error(f"Task timeout: {task.name}")
            
        except Exception as e:
            task_result.state = TaskState.FAILED
            task_result.error_message = str(e)
            task_result.end_time = time.time()
            logger.error(f"Task failed: {task.name} - {e}")
        
        return task_result
    
    async def _execute_memory_analysis(self, task: TaskDefinition, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute memory analysis task"""
        try:
            binary_path = context.get('target_binary') or task.parameters.get('binary_path')
            if not binary_path:
                raise ValueError("No binary path specified for memory analysis")
            
            # Perform memory analysis
            with open(binary_path, 'rb') as f:
                binary_data = f.read()
            
            analysis_result = {
                'binary_size': len(binary_data),
                'sections_analyzed': True,
                'entry_point_found': True,
                'import_table_valid': True,
                'memory_regions': []
            }
            
            # Store analysis results in context
            context['memory_analysis'] = analysis_result
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"Memory analysis failed: {e}")
            raise
    
    async def _execute_unpacking(self, task: TaskDefinition, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute unpacking task"""
        try:
            binary_path = context.get('target_binary') or task.parameters.get('binary_path')
            if not binary_path:
                raise ValueError("No binary path specified for unpacking")
            
            output_path = task.parameters.get('output_path', binary_path + '.unpacked')
            
            # Execute unpacking
            unpacking_result = self.universal_unpacker.unpack_binary(binary_path, output_path)
            
            if unpacking_result['success']:
                context['unpacked_binary'] = output_path
                context['unpacking_info'] = unpacking_result
            
            return unpacking_result
            
        except Exception as e:
            logger.error(f"Unpacking failed: {e}")
            raise
    
    async def _execute_anti_analysis_bypass(self, task: TaskDefinition, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute anti-analysis bypass task"""
        try:
            target_pid = context.get('target_pid') or task.parameters.get('target_pid')
            if not target_pid:
                raise ValueError("No target PID specified for anti-analysis bypass")
            
            # Execute anti-analysis bypass
            bypass_result = self.titan_hide.enable_all_bypasses(target_pid)
            
            context['anti_analysis_bypassed'] = bypass_result
            
            return {'bypasses_enabled': bypass_result}
            
        except Exception as e:
            logger.error(f"Anti-analysis bypass failed: {e}")
            raise
    
    async def _execute_import_reconstruction(self, task: TaskDefinition, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute import reconstruction task"""
        try:
            binary_path = context.get('unpacked_binary') or context.get('target_binary')
            if not binary_path:
                raise ValueError("No binary path specified for import reconstruction")
            
            target_pid = context.get('target_pid') or task.parameters.get('target_pid')
            
            # Execute import reconstruction
            import_result = self.import_rebuilder.rebuild_imports(binary_path, target_pid)
            
            if import_result['success']:
                context['imports_rebuilt'] = True
                context['import_info'] = import_result
            
            return import_result
            
        except Exception as e:
            logger.error(f"Import reconstruction failed: {e}")
            raise
    
    async def _execute_license_bypass(self, task: TaskDefinition, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute license bypass task"""
        try:
            binary_path = context.get('target_binary')
            target_pid = context.get('target_pid')
            license_keys = task.parameters.get('license_keys', [])
            
            if not binary_path and not target_pid:
                raise ValueError("No binary path or target PID specified for license bypass")
            
            # Execute license bypass
            bypass_result = self.activation_bypass.bypass_license_protection(
                binary_path or "", target_pid, license_keys
            )
            
            context['license_bypassed'] = bypass_result.success
            context['license_bypass_info'] = {
                'license_type': bypass_result.license_type.value,
                'techniques_used': [t.value for t in bypass_result.bypass_techniques],
                'checks_bypassed': len(bypass_result.bypassed_checks)
            }
            
            return {
                'success': bypass_result.success,
                'license_type': bypass_result.license_type.value,
                'bypassed_checks': len(bypass_result.bypassed_checks)
            }
            
        except Exception as e:
            logger.error(f"License bypass failed: {e}")
            raise
    
    async def _execute_vm_devirtualization(self, task: TaskDefinition, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute VM devirtualization task"""
        try:
            binary_path = context.get('unpacked_binary') or context.get('target_binary')
            if not binary_path:
                raise ValueError("No binary path specified for VM devirtualization")
            
            vm_entry = task.parameters.get('vm_entry', 0x401000)
            output_path = task.parameters.get('output_path')
            
            # Execute VM devirtualization
            translation_result = self.vm_translator.translate_vm_code(
                binary_path, vm_entry, output_path
            )
            
            if translation_result.get('success'):
                context['vm_devirtualized'] = True
                context['vm_translation'] = translation_result
            
            return translation_result
            
        except Exception as e:
            logger.error(f"VM devirtualization failed: {e}")
            raise
    
    async def _execute_stealth_setup(self, task: TaskDefinition, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute stealth techniques setup task"""
        try:
            stealth_config = task.parameters.get('stealth_config', {})
            
            # Execute stealth setup
            stealth_result = self.stealth_techniques.create_stealth_environment(stealth_config)
            
            context['stealth_enabled'] = True
            context['stealth_info'] = stealth_result
            
            return stealth_result
            
        except Exception as e:
            logger.error(f"Stealth setup failed: {e}")
            raise
    
    async def _execute_process_injection(self, task: TaskDefinition, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute process injection task"""
        try:
            target_pid = context.get('target_pid') or task.parameters.get('target_pid')
            payload_data = task.parameters.get('payload_data', b'')
            
            if not target_pid:
                raise ValueError("No target PID specified for process injection")
            
            # Simple process injection implementation
            PROCESS_ALL_ACCESS = 0x1F0FFF
            import ctypes
            process_handle = ctypes.windll.kernel32.OpenProcess(PROCESS_ALL_ACCESS, False, target_pid)
            
            if not process_handle:
                raise ValueError(f"Cannot open process {target_pid}")
            
            try:
                # Allocate memory
                allocated_addr = self.memory_ops.syscall_manager.allocate_memory(
                    process_handle, len(payload_data), 0x3000, 0x40
                )
                
                if not allocated_addr:
                    raise ValueError("Memory allocation failed")
                
                # Write payload
                if not self.memory_ops.syscall_manager.write_memory(
                    process_handle, allocated_addr, payload_data
                ):
                    raise ValueError("Payload writing failed")
                
                context['injected_address'] = allocated_addr
                
                return {
                    'success': True,
                    'injected_address': f"0x{allocated_addr:08X}",
                    'payload_size': len(payload_data)
                }
                
            finally:
                ctypes.windll.kernel32.CloseHandle(process_handle)
                
        except Exception as e:
            logger.error(f"Process injection failed: {e}")
            raise
    
    async def _execute_network_bypass(self, task: TaskDefinition, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute network bypass task"""
        try:
            license_servers = task.parameters.get('license_servers', [])
            
            if license_servers:
                bypass_result = self.stealth_techniques.setup_network_bypass(license_servers)
                context['network_bypassed'] = bypass_result
                
                return {'success': bypass_result, 'servers_count': len(license_servers)}
            else:
                return {'success': True, 'servers_count': 0}
                
        except Exception as e:
            logger.error(f"Network bypass failed: {e}")
            raise
    
    async def _execute_file_patching(self, task: TaskDefinition, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute file patching task"""
        try:
            target_file = task.parameters.get('target_file') or context.get('target_binary')
            patches = task.parameters.get('patches', [])
            
            if not target_file or not patches:
                raise ValueError("Target file and patches required for file patching")
            
            # Apply patches (simplified implementation)
            patches_applied = 0
            
            with open(target_file, 'r+b') as f:
                for patch in patches:
                    offset = patch.get('offset')
                    data = patch.get('data')
                    
                    if offset is not None and data:
                        f.seek(offset)
                        if isinstance(data, str):
                            data = bytes.fromhex(data)
                        f.write(data)
                        patches_applied += 1
            
            context['file_patched'] = patches_applied > 0
            
            return {'success': True, 'patches_applied': patches_applied}
            
        except Exception as e:
            logger.error(f"File patching failed: {e}")
            raise
    
    async def _execute_registry_modification(self, task: TaskDefinition, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute registry modification task"""
        try:
            registry_operations = task.parameters.get('registry_operations', [])
            
            operations_completed = 0
            
            for operation in registry_operations:
                op_type = operation.get('type')
                key_path = operation.get('key_path')
                value_name = operation.get('value_name')
                value_data = operation.get('value_data')
                
                if op_type == 'set_value' and key_path and value_name:
                    # Registry modification would be implemented here
                    # For now, just log the operation
                    logger.info(f"Registry operation: {op_type} {key_path}\\{value_name}")
                    operations_completed += 1
            
            context['registry_modified'] = operations_completed > 0
            
            return {'success': True, 'operations_completed': operations_completed}
            
        except Exception as e:
            logger.error(f"Registry modification failed: {e}")
            raise
    
    async def _execute_api_hooking(self, task: TaskDefinition, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute API hooking task"""
        try:
            target_pid = context.get('target_pid') or task.parameters.get('target_pid')
            hooks = task.parameters.get('hooks', [])
            
            if not target_pid:
                raise ValueError("No target PID specified for API hooking")
            
            hooks_installed = 0
            
            for hook in hooks:
                dll_name = hook.get('dll_name')
                api_name = hook.get('api_name')
                
                if dll_name and api_name:
                    # API hooking would be implemented here
                    logger.info(f"Installing hook: {dll_name}!{api_name}")
                    hooks_installed += 1
            
            context['api_hooked'] = hooks_installed > 0
            
            return {'success': True, 'hooks_installed': hooks_installed}
            
        except Exception as e:
            logger.error(f"API hooking failed: {e}")
            raise


class WorkflowOrchestrator:
    """Main workflow orchestration engine"""
    
    def __init__(self, config_dir: Optional[str] = None):
        self.config_dir = Path(config_dir or os.path.expanduser("~/.intellicrack/workflows"))
        self.config_dir.mkdir(parents=True, exist_ok=True)
        
        self.task_executor = TaskExecutor()
        self.active_executions: Dict[str, WorkflowExecution] = {}
        self.workflow_definitions: Dict[str, WorkflowDefinition] = {}
        
        # Load existing workflow definitions
        self._load_workflow_definitions()
    
    def _load_workflow_definitions(self):
        """Load workflow definitions from config directory"""
        try:
            workflows_file = self.config_dir / "workflows.yaml"
            if workflows_file.exists():
                with open(workflows_file, 'r') as f:
                    workflows_data = yaml.safe_load(f)
                
                for workflow_data in workflows_data.get('workflows', []):
                    workflow = self._parse_workflow_definition(workflow_data)
                    self.workflow_definitions[workflow.workflow_id] = workflow
                
                logger.info(f"Loaded {len(self.workflow_definitions)} workflow definitions")
            
        except Exception as e:
            logger.error(f"Failed to load workflow definitions: {e}")
    
    def _parse_workflow_definition(self, workflow_data: Dict[str, Any]) -> WorkflowDefinition:
        """Parse workflow definition from YAML data"""
        tasks = []
        for task_data in workflow_data.get('tasks', []):
            task = TaskDefinition(
                task_id=task_data['task_id'],
                task_type=task_data['task_type'],
                name=task_data['name'],
                description=task_data.get('description', ''),
                priority=TaskPriority[task_data.get('priority', 'MEDIUM').upper()],
                dependencies=task_data.get('dependencies', []),
                parameters=task_data.get('parameters', {}),
                timeout=task_data.get('timeout', 300),
                retry_count=task_data.get('retry_count', 3),
                retry_delay=task_data.get('retry_delay', 5),
                required=task_data.get('required', True),
                conditional=task_data.get('conditional')
            )
            tasks.append(task)
        
        return WorkflowDefinition(
            workflow_id=workflow_data['workflow_id'],
            name=workflow_data['name'],
            description=workflow_data.get('description', ''),
            version=workflow_data.get('version', '1.0'),
            created_by=workflow_data.get('created_by', 'Intellicrack'),
            target_binary=workflow_data.get('target_binary'),
            protection_types=workflow_data.get('protection_types', []),
            tasks=tasks,
            global_config=workflow_data.get('global_config', {}),
            rollback_enabled=workflow_data.get('rollback_enabled', True),
            max_execution_time=workflow_data.get('max_execution_time', 3600)
        )
    
    def create_workflow_definition(self, workflow_data: Dict[str, Any]) -> str:
        """Create new workflow definition"""
        try:
            workflow = self._parse_workflow_definition(workflow_data)
            self.workflow_definitions[workflow.workflow_id] = workflow
            
            # Save to disk
            self._save_workflow_definitions()
            
            logger.info(f"Created workflow definition: {workflow.name}")
            return workflow.workflow_id
            
        except Exception as e:
            logger.error(f"Failed to create workflow definition: {e}")
            raise
    
    def _save_workflow_definitions(self):
        """Save workflow definitions to disk"""
        try:
            workflows_data = {
                'workflows': [
                    self._serialize_workflow_definition(workflow)
                    for workflow in self.workflow_definitions.values()
                ]
            }
            
            workflows_file = self.config_dir / "workflows.yaml"
            with open(workflows_file, 'w') as f:
                yaml.safe_dump(workflows_data, f, default_flow_style=False)
            
        except Exception as e:
            logger.error(f"Failed to save workflow definitions: {e}")
    
    def _serialize_workflow_definition(self, workflow: WorkflowDefinition) -> Dict[str, Any]:
        """Serialize workflow definition to YAML-compatible format"""
        return {
            'workflow_id': workflow.workflow_id,
            'name': workflow.name,
            'description': workflow.description,
            'version': workflow.version,
            'created_by': workflow.created_by,
            'target_binary': workflow.target_binary,
            'protection_types': workflow.protection_types,
            'global_config': workflow.global_config,
            'rollback_enabled': workflow.rollback_enabled,
            'max_execution_time': workflow.max_execution_time,
            'tasks': [
                {
                    'task_id': task.task_id,
                    'task_type': task.task_type,
                    'name': task.name,
                    'description': task.description,
                    'priority': task.priority.name,
                    'dependencies': task.dependencies,
                    'parameters': task.parameters,
                    'timeout': task.timeout,
                    'retry_count': task.retry_count,
                    'retry_delay': task.retry_delay,
                    'required': task.required,
                    'conditional': task.conditional
                }
                for task in workflow.tasks
            ]
        }
    
    async def execute_workflow(self, workflow_id: str, execution_context: Optional[Dict[str, Any]] = None) -> str:
        """Execute a workflow"""
        try:
            if workflow_id not in self.workflow_definitions:
                raise ValueError(f"Workflow definition not found: {workflow_id}")
            
            workflow_def = self.workflow_definitions[workflow_id]
            execution_id = str(uuid.uuid4())
            
            # Create execution instance
            execution = WorkflowExecution(
                execution_id=execution_id,
                workflow_definition=workflow_def,
                state=WorkflowState.RUNNING,
                execution_context=execution_context or {},
                start_time=time.time()
            )
            
            # Apply global config to context
            execution.execution_context.update(workflow_def.global_config)
            
            self.active_executions[execution_id] = execution
            
            logger.info(f"Starting workflow execution: {workflow_def.name} ({execution_id})")
            
            # Execute workflow in background
            asyncio.create_task(self._execute_workflow_async(execution))
            
            return execution_id
            
        except Exception as e:
            logger.error(f"Failed to start workflow execution: {e}")
            raise
    
    async def _execute_workflow_async(self, execution: WorkflowExecution):
        """Execute workflow asynchronously"""
        try:
            # Build task dependency graph
            task_order = self._resolve_task_dependencies(execution.workflow_definition.tasks)
            
            # Execute tasks in order
            for task_id in task_order:
                task = next(t for t in execution.workflow_definition.tasks if t.task_id == task_id)
                
                # Check if task should be executed
                if not self._should_execute_task(task, execution):
                    execution.task_results[task_id] = TaskResult(
                        task_id=task_id,
                        state=TaskState.SKIPPED
                    )
                    continue
                
                execution.current_task = task_id
                
                # Execute task with retries
                task_result = await self._execute_task_with_retries(task, execution)
                execution.task_results[task_id] = task_result
                
                # Check if task failed and is required
                if task_result.state == TaskState.FAILED and task.required:
                    execution.state = WorkflowState.FAILED
                    execution.error_message = f"Required task failed: {task.name}"
                    break
            
            # Complete workflow
            if execution.state == WorkflowState.RUNNING:
                execution.state = WorkflowState.COMPLETED
            
            execution.end_time = time.time()
            execution.current_task = None
            
            logger.info(f"Workflow execution completed: {execution.execution_id} ({execution.state.value})")
            
        except Exception as e:
            execution.state = WorkflowState.FAILED
            execution.error_message = str(e)
            execution.end_time = time.time()
            logger.error(f"Workflow execution failed: {execution.execution_id} - {e}")
    
    def _resolve_task_dependencies(self, tasks: List[TaskDefinition]) -> List[str]:
        """Resolve task execution order based on dependencies"""
        task_graph = {task.task_id: task.dependencies for task in tasks}
        resolved = []
        remaining = list(task_graph.keys())
        
        while remaining:
            # Find tasks with no unresolved dependencies
            ready_tasks = [
                task_id for task_id in remaining
                if all(dep in resolved for dep in task_graph[task_id])
            ]
            
            if not ready_tasks:
                # Circular dependency or missing dependency
                raise ValueError("Circular dependency or missing dependency in workflow")
            
            # Sort by priority (higher priority first)
            task_objects = [next(t for t in tasks if t.task_id == tid) for tid in ready_tasks]
            task_objects.sort(key=lambda t: t.priority.value, reverse=True)
            
            for task in task_objects:
                resolved.append(task.task_id)
                remaining.remove(task.task_id)
        
        return resolved
    
    def _should_execute_task(self, task: TaskDefinition, execution: WorkflowExecution) -> bool:
        """Check if task should be executed based on conditions"""
        if not task.conditional:
            return True
        
        try:
            # Simple condition evaluation (could be enhanced)
            # Format: "variable_name == value" or "variable_name != value"
            condition_parts = task.conditional.split()
            if len(condition_parts) >= 3:
                var_name = condition_parts[0]
                operator = condition_parts[1]
                expected_value = condition_parts[2]
                
                actual_value = execution.execution_context.get(var_name)
                
                if operator == "==":
                    return str(actual_value) == expected_value
                elif operator == "!=":
                    return str(actual_value) != expected_value
                elif operator == "in":
                    return expected_value in str(actual_value)
            
            return True
            
        except Exception as e:
            logger.warning(f"Condition evaluation failed for task {task.task_id}: {e}")
            return True
    
    async def _execute_task_with_retries(self, task: TaskDefinition, execution: WorkflowExecution) -> TaskResult:
        """Execute task with retry logic"""
        last_result = None
        
        for attempt in range(task.retry_count + 1):
            try:
                # Create rollback point if needed
                if execution.workflow_definition.rollback_enabled:
                    self._create_rollback_point(task, execution)
                
                # Execute task
                result = await self.task_executor.execute_task(task, execution.execution_context)
                
                if result.state == TaskState.COMPLETED:
                    return result
                
                last_result = result
                
                # Retry if not last attempt
                if attempt < task.retry_count:
                    logger.warning(f"Task {task.name} failed, retrying in {task.retry_delay}s (attempt {attempt + 1}/{task.retry_count})")
                    await asyncio.sleep(task.retry_delay)
                    
                    result.retry_count = attempt + 1
                    result.state = TaskState.RETRYING
                
            except Exception as e:
                last_result = TaskResult(
                    task_id=task.task_id,
                    state=TaskState.FAILED,
                    error_message=str(e)
                )
                
                if attempt < task.retry_count:
                    await asyncio.sleep(task.retry_delay)
        
        return last_result or TaskResult(
            task_id=task.task_id,
            state=TaskState.FAILED,
            error_message="Task execution failed"
        )
    
    def _create_rollback_point(self, task: TaskDefinition, execution: WorkflowExecution):
        """Create rollback point for task"""
        try:
            rollback_data = {
                'task_id': task.task_id,
                'timestamp': time.time(),
                'context_snapshot': copy.deepcopy(execution.execution_context)
            }
            
            execution.rollback_data[task.task_id] = rollback_data
            
        except Exception as e:
            logger.warning(f"Failed to create rollback point for {task.task_id}: {e}")
    
    def pause_workflow(self, execution_id: str) -> bool:
        """Pause workflow execution"""
        try:
            if execution_id in self.active_executions:
                execution = self.active_executions[execution_id]
                if execution.state == WorkflowState.RUNNING:
                    execution.state = WorkflowState.PAUSED
                    logger.info(f"Workflow paused: {execution_id}")
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"Failed to pause workflow: {e}")
            return False
    
    def resume_workflow(self, execution_id: str) -> bool:
        """Resume paused workflow"""
        try:
            if execution_id in self.active_executions:
                execution = self.active_executions[execution_id]
                if execution.state == WorkflowState.PAUSED:
                    execution.state = WorkflowState.RUNNING
                    logger.info(f"Workflow resumed: {execution_id}")
                    # Continue execution
                    asyncio.create_task(self._execute_workflow_async(execution))
                    return True
            
            return False
            
        except Exception as e:
            logger.error(f"Failed to resume workflow: {e}")
            return False
    
    def cancel_workflow(self, execution_id: str) -> bool:
        """Cancel workflow execution"""
        try:
            if execution_id in self.active_executions:
                execution = self.active_executions[execution_id]
                execution.state = WorkflowState.CANCELLED
                execution.end_time = time.time()
                logger.info(f"Workflow cancelled: {execution_id}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"Failed to cancel workflow: {e}")
            return False
    
    def get_workflow_status(self, execution_id: str) -> Optional[Dict[str, Any]]:
        """Get workflow execution status"""
        try:
            if execution_id not in self.active_executions:
                return None
            
            execution = self.active_executions[execution_id]
            
            # Calculate progress
            total_tasks = len(execution.workflow_definition.tasks)
            completed_tasks = sum(
                1 for result in execution.task_results.values()
                if result.state in [TaskState.COMPLETED, TaskState.SKIPPED]
            )
            
            progress = (completed_tasks / total_tasks * 100) if total_tasks > 0 else 0
            
            return {
                'execution_id': execution_id,
                'workflow_name': execution.workflow_definition.name,
                'state': execution.state.value,
                'current_task': execution.current_task,
                'progress': progress,
                'start_time': execution.start_time,
                'end_time': execution.end_time,
                'error_message': execution.error_message,
                'task_count': total_tasks,
                'completed_tasks': completed_tasks,
                'task_results': {
                    task_id: {
                        'state': result.state.value,
                        'execution_time': result.execution_time,
                        'error_message': result.error_message
                    }
                    for task_id, result in execution.task_results.items()
                }
            }
            
        except Exception as e:
            logger.error(f"Failed to get workflow status: {e}")
            return None
    
    def list_workflow_definitions(self) -> List[Dict[str, Any]]:
        """List all workflow definitions"""
        return [
            {
                'workflow_id': workflow.workflow_id,
                'name': workflow.name,
                'description': workflow.description,
                'version': workflow.version,
                'task_count': len(workflow.tasks),
                'protection_types': workflow.protection_types
            }
            for workflow in self.workflow_definitions.values()
        ]
    
    def list_active_executions(self) -> List[Dict[str, Any]]:
        """List all active workflow executions"""
        return [
            {
                'execution_id': execution_id,
                'workflow_name': execution.workflow_definition.name,
                'state': execution.state.value,
                'start_time': execution.start_time,
                'current_task': execution.current_task
            }
            for execution_id, execution in self.active_executions.items()
            if execution.state in [WorkflowState.RUNNING, WorkflowState.PAUSED]
        ]
    
    def cleanup_completed_executions(self):
        """Clean up completed workflow executions"""
        completed_executions = [
            execution_id for execution_id, execution in self.active_executions.items()
            if execution.state in [WorkflowState.COMPLETED, WorkflowState.FAILED, WorkflowState.CANCELLED]
        ]
        
        for execution_id in completed_executions:
            del self.active_executions[execution_id]
        
        logger.info(f"Cleaned up {len(completed_executions)} completed executions")