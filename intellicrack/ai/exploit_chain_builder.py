"""Automated Exploit Chain Builder

Copyright (C) 2025 Zachary Flint

This file is part of Intellicrack.

Intellicrack is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Intellicrack is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Intellicrack.  If not, see <https://www.gnu.org/licenses/>.
"""

import hashlib
import uuid
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any

from ..utils.logger import get_logger
from .learning_engine_simple import get_learning_engine
from .llm_backends import LLMManager, LLMMessage
from .performance_monitor import profile_ai_operation

logger = get_logger(__name__)


class ExploitType(Enum):
    """Types of exploit primitives."""

    BUFFER_OVERFLOW = "buffer_overflow"
    FORMAT_STRING = "format_string"
    USE_AFTER_FREE = "use_after_free"
    DOUBLE_FREE = "double_free"
    INTEGER_OVERFLOW = "integer_overflow"
    NULL_POINTER_DEREF = "null_pointer_deref"
    PRIVILEGE_ESCALATION = "privilege_escalation"
    CODE_INJECTION = "code_injection"
    COMMAND_INJECTION = "command_injection"
    SQL_INJECTION = "sql_injection"
    CROSS_SITE_SCRIPTING = "cross_site_scripting"
    RACE_CONDITION = "race_condition"
    AUTHENTICATION_BYPASS = "authentication_bypass"
    AUTHORIZATION_BYPASS = "authorization_bypass"
    CRYPTOGRAPHIC_WEAKNESS = "cryptographic_weakness"
    DESERIALIZATION = "deserialization"
    PATH_TRAVERSAL = "path_traversal"
    REMOTE_CODE_EXECUTION = "remote_code_execution"


class ExploitPrimitive(Enum):
    """Basic exploit building blocks."""

    MEMORY_CORRUPTION = "memory_corruption"
    CONTROL_FLOW_HIJACK = "control_flow_hijack"
    DATA_LEAK = "data_leak"
    ARBITRARY_WRITE = "arbitrary_write"
    ARBITRARY_READ = "arbitrary_read"
    HEAP_MANIPULATION = "heap_manipulation"
    STACK_MANIPULATION = "stack_manipulation"
    ROP_GADGET = "rop_gadget"
    JOP_GADGET = "jop_gadget"
    SHELLCODE_INJECTION = "shellcode_injection"
    BYPASS_ASLR = "bypass_aslr"
    BYPASS_DEP = "bypass_dep"
    BYPASS_STACK_CANARY = "bypass_stack_canary"
    INFORMATION_DISCLOSURE = "information_disclosure"
    TIMING_ATTACK = "timing_attack"


class ChainComplexity(Enum):
    """Complexity levels of exploit chains."""

    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"
    ADVANCED = "advanced"
    EXPERT = "expert"


@dataclass
class Vulnerability:
    """Vulnerability information for exploit development."""

    vuln_id: str
    vuln_type: ExploitType
    severity: str  # low, medium, high, critical
    description: str
    location: dict[str, Any]  # file, function, line, etc.
    prerequisites: list[str] = field(default_factory=list)
    impact: dict[str, str] = field(default_factory=dict)
    cve_id: str | None = None
    metadata: dict[str, Any] = field(default_factory=dict)
    confidence: float = 0.0
    exploitability: float = 0.0
    discovery_method: str = "unknown"


@dataclass
class ExploitStep:
    """Individual step in an exploit chain."""

    step_id: str
    step_type: ExploitPrimitive
    description: str
    code_template: str
    prerequisites: list[str] = field(default_factory=list)
    outputs: list[str] = field(default_factory=list)
    success_criteria: list[str] = field(default_factory=list)
    failure_modes: list[str] = field(default_factory=list)
    reliability: float = 0.0
    execution_time: float = 0.0
    memory_requirements: int = 0
    payload_data: str | None = None
    verification_code: str | None = None


@dataclass
class ExploitChain:
    """Complete exploit chain."""

    chain_id: str
    name: str
    target_vulnerability: str
    steps: list[ExploitStep]
    complexity: ChainComplexity
    success_probability: float
    total_execution_time: float
    memory_footprint: int
    stealth_rating: float
    stability_rating: float
    created_at: datetime = field(default_factory=datetime.now)
    tested: bool = False
    test_results: dict[str, Any] = field(default_factory=dict)
    safety_verified: bool = False
    safety_report: dict[str, Any] = field(default_factory=dict)


@dataclass
class ChainValidationResult:
    """Results of exploit chain validation."""

    is_valid: bool
    confidence: float
    issues: list[str] = field(default_factory=list)
    warnings: list[str] = field(default_factory=list)
    optimizations: list[str] = field(default_factory=list)
    estimated_success_rate: float = 0.0
    risk_assessment: dict[str, str] = field(default_factory=dict)


class ExploitPrimitiveLibrary:
    """Library of exploit primitives and templates."""

    def __init__(self):
        """Initialize the exploit primitive library.

        Sets up collections for storing exploit primitives, templates,
        and gadgets, then populates with common exploitation techniques.
        """
        self.primitives: dict[ExploitPrimitive,
                              list[dict[str, Any]]] = defaultdict(list)
        self.templates: dict[str, str] = {}
        self.gadgets: dict[str, list[str]] = defaultdict(list)
        self._initialize_library()

    def _initialize_library(self):
        """Initialize with common exploit primitives."""
        # Buffer overflow primitives
        self.primitives[ExploitPrimitive.MEMORY_CORRUPTION].extend([
            {
                "name": "stack_buffer_overflow",
                "description": "Classic stack buffer overflow",
                "template": """
// Stack buffer overflow exploit
char payload[BUFFER_SIZE];
memset(payload, 'A', OVERFLOW_OFFSET);
*(void**)(payload + OVERFLOW_OFFSET) = (void*)TARGET_ADDRESS;
""",
                "prerequisites": ["buffer_size_known", "return_address_offset"],
                "reliability": 0.8,
            },
            {
                "name": "heap_buffer_overflow",
                "description": "Heap buffer overflow with metadata corruption",
                "template": """
// Heap buffer overflow
char* heap_buffer = malloc(BUFFER_SIZE);
memset(heap_buffer, 'B', OVERFLOW_SIZE);
// Corrupt heap metadata
*(size_t*)(heap_buffer + BUFFER_SIZE) = FAKE_SIZE;
""",
                "prerequisites": ["heap_layout_known", "allocator_knowledge"],
                "reliability": 0.6,
            },
        ])

        # Control flow hijacking
        self.primitives[ExploitPrimitive.CONTROL_FLOW_HIJACK].extend([
            {
                "name": "return_address_overwrite",
                "description": "Overwrite return address on stack",
                "template": """
// Return address overwrite
void exploit_function() {
    char buffer[BUFFER_SIZE];
    // Overflow to overwrite return address
    strcpy(buffer, MALICIOUS_INPUT);
}
""",
                "prerequisites": ["stack_layout", "return_offset"],
                "reliability": 0.9,
            },
            {
                "name": "function_pointer_overwrite",
                "description": "Overwrite function pointer",
                "template": """
// Function pointer overwrite
struct vtable {
    void (*func_ptr)(void);
};
struct vtable* obj = (struct vtable*)TARGET_OBJECT;
obj->func_ptr = (void*)SHELLCODE_ADDRESS;
""",
                "prerequisites": ["object_layout", "function_pointer_location"],
                "reliability": 0.7,
            },
        ])

        # ROP gadgets
        self.primitives[ExploitPrimitive.ROP_GADGET].extend([
            {
                "name": "pop_ret_gadget",
                "description": "POP instruction followed by RET",
                "template": "pop rax; ret",
                "prerequisites": ["gadget_address"],
                "reliability": 0.95,
            },
            {
                "name": "syscall_gadget",
                "description": "System call gadget",
                "template": "syscall; ret",
                "prerequisites": ["syscall_number", "register_setup"],
                "reliability": 0.85,
            },
        ])

        # Shellcode injection
        self.primitives[ExploitPrimitive.SHELLCODE_INJECTION].extend([
            {
                "name": "nop_sled_shellcode",
                "description": "NOP sled with shellcode",
                "template": """
// NOP sled + shellcode
unsigned char exploit[] = {
    // NOP sled
    0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90,
    // Shellcode starts here
    SHELLCODE_BYTES
};
""",
                "prerequisites": ["executable_memory", "shellcode"],
                "reliability": 0.8,
            },
        ])

        # ASLR bypass techniques
        self.primitives[ExploitPrimitive.BYPASS_ASLR].extend([
            {
                "name": "information_leak",
                "description": "Leak addresses to bypass ASLR",
                "template": """
// Information leak to bypass ASLR
void* leaked_address = get_leaked_pointer();
void* base_address = (void*)((uintptr_t)leaked_address & ~0xfff);
void* target_function = base_address + FUNCTION_OFFSET;
""",
                "prerequisites": ["leak_primitive", "base_calculation"],
                "reliability": 0.7,
            },
        ])

    @profile_ai_operation("get_primitive")
    def get_primitive(self, primitive_type: ExploitPrimitive,
                      requirements: list[str] | None = None) -> dict[str, Any] | None:
        """Get best matching primitive for requirements."""
        candidates = self.primitives.get(primitive_type, [])

        if not candidates:
            return None

        if not requirements:
            return candidates[0]  # Return first available

        # Score candidates based on requirement matching
        best_candidate = None
        best_score = 0.0

        for candidate in candidates:
            score = self._score_primitive(candidate, requirements)
            if score > best_score:
                best_score = score
                best_candidate = candidate

        return best_candidate

    def _score_primitive(self, primitive: dict[str, Any], requirements: list[str]) -> float:
        """Score primitive based on requirement matching."""
        prereqs = set(primitive.get("prerequisites", []))
        req_set = set(requirements)

        # Requirements satisfied
        satisfied = len(prereqs.intersection(req_set))
        total_reqs = len(prereqs)

        if total_reqs == 0:
            requirement_score = 1.0
        else:
            requirement_score = satisfied / total_reqs

        # Base reliability
        reliability = primitive.get("reliability", 0.5)

        return (requirement_score * 0.7) + (reliability * 0.3)

    def add_primitive(self, primitive_type: ExploitPrimitive, primitive_data: dict[str, Any]):
        """Add new primitive to library."""
        self.primitives[primitive_type].append(primitive_data)
        logger.info(f"Added new primitive: {primitive_type.value}")


class ExploitChainFramework:
    """Framework for building exploit chains."""

    def __init__(self, primitive_library: ExploitPrimitiveLibrary | None = None):
        """Initialize the exploit chain framework.

        Args:
            primitive_library: Optional existing library of exploit primitives

        """
        self.primitive_lib = primitive_library or ExploitPrimitiveLibrary()
        self.chain_templates: dict[str, list[ExploitPrimitive]] = {}
        self.success_patterns: dict[str, float] = {}
        self._initialize_templates()

    def _initialize_templates(self):
        """Initialize common exploit chain templates."""
        # Simple buffer overflow chain
        self.chain_templates["simple_buffer_overflow"] = [
            ExploitPrimitive.MEMORY_CORRUPTION,
            ExploitPrimitive.CONTROL_FLOW_HIJACK,
            ExploitPrimitive.SHELLCODE_INJECTION,
        ]

        # ROP-based exploitation
        self.chain_templates["rop_exploitation"] = [
            ExploitPrimitive.MEMORY_CORRUPTION,
            ExploitPrimitive.BYPASS_ASLR,
            ExploitPrimitive.ROP_GADGET,
            ExploitPrimitive.SHELLCODE_INJECTION,
        ]

        # Advanced heap exploitation
        self.chain_templates["heap_exploitation"] = [
            ExploitPrimitive.HEAP_MANIPULATION,
            ExploitPrimitive.ARBITRARY_WRITE,
            ExploitPrimitive.CONTROL_FLOW_HIJACK,
            ExploitPrimitive.BYPASS_DEP,
            ExploitPrimitive.SHELLCODE_INJECTION,
        ]

        # Information disclosure chain
        self.chain_templates["info_disclosure"] = [
            ExploitPrimitive.INFORMATION_DISCLOSURE,
            ExploitPrimitive.BYPASS_ASLR,
            ExploitPrimitive.MEMORY_CORRUPTION,
            ExploitPrimitive.CONTROL_FLOW_HIJACK,
        ]

    @profile_ai_operation("build_exploit_chain")
    def build_exploit_chain(self, vulnerability: Vulnerability,
                            requirements: dict[str, Any] | None = None) -> ExploitChain | None:
        """Build exploit chain for vulnerability."""
        logger.info(
            f"Building exploit chain for {vulnerability.vuln_type.value}")

        # Select appropriate template
        template = self._select_chain_template(vulnerability)
        if not template:
            logger.warning(
                f"No template found for {vulnerability.vuln_type.value}")
            return None

        # Build chain steps
        steps = []
        chain_requirements = requirements or {}

        # Use chain_requirements to guide primitive selection
        required_capabilities = chain_requirements.get("capabilities", [])
        max_complexity = chain_requirements.get("max_complexity", 10)

        logger.debug(
            f"Building chain with requirements: capabilities={required_capabilities}, max_complexity={max_complexity}")

        for i, primitive_type in enumerate(template):
            step_reqs = self._get_step_requirements(
                vulnerability, primitive_type, steps)

            # Apply chain requirements to step requirements
            if required_capabilities:
                step_reqs["required_capabilities"] = required_capabilities
            step_reqs["max_complexity"] = max_complexity

            primitive = self.primitive_lib.get_primitive(
                primitive_type, step_reqs)

            if not primitive:
                logger.warning(
                    f"No primitive found for {primitive_type.value}")
                continue

            step = ExploitStep(
                step_id=f"step_{i+1}_{primitive_type.value}",
                step_type=primitive_type,
                description=primitive.get("description", ""),
                code_template=primitive.get("template", ""),
                prerequisites=primitive.get("prerequisites", []),
                reliability=primitive.get("reliability", 0.5),
            )

            steps.append(step)

        if not steps:
            return None

        # Calculate chain metrics
        complexity = self._calculate_complexity(steps)
        success_prob = self._calculate_success_probability(steps)
        exec_time = sum(step.execution_time for step in steps)
        memory_footprint = max(step.memory_requirements for step in steps)

        chain = ExploitChain(
            chain_id=str(uuid.uuid4()),
            name=f"{vulnerability.vuln_type.value}_exploit_chain",
            target_vulnerability=vulnerability.vuln_id,
            steps=steps,
            complexity=complexity,
            success_probability=success_prob,
            total_execution_time=exec_time,
            memory_footprint=memory_footprint,
            stealth_rating=self._calculate_stealth_rating(steps),
            stability_rating=self._calculate_stability_rating(steps),
        )

        logger.info(
            f"Built exploit chain: {chain.chain_id} (success prob: {success_prob:.2f})")
        return chain

    def _select_chain_template(self, vulnerability: Vulnerability) -> list[ExploitPrimitive] | None:
        """Select appropriate chain template for vulnerability."""
        vuln_type = vulnerability.vuln_type

        # Map vulnerability types to templates
        template_mapping = {
            ExploitType.BUFFER_OVERFLOW: "simple_buffer_overflow",
            ExploitType.USE_AFTER_FREE: "heap_exploitation",
            ExploitType.FORMAT_STRING: "rop_exploitation",
            ExploitType.INTEGER_OVERFLOW: "simple_buffer_overflow",
            ExploitType.PRIVILEGE_ESCALATION: "rop_exploitation",
        }

        template_name = template_mapping.get(vuln_type)
        if template_name:
            return self.chain_templates.get(template_name)

        # Default template
        return self.chain_templates.get("simple_buffer_overflow")

    def _get_step_requirements(self, vulnerability: Vulnerability,
                               primitive_type: ExploitPrimitive,
                               previous_steps: list[ExploitStep]) -> dict[str, Any]:
        """Get requirements for specific step based on context."""
        requirements = {
            "basic_requirements": [],
            "required_capabilities": [],
            "max_complexity": 5,
        }

        # Add vulnerability-specific requirements
        if vulnerability.metadata:
            requirements["basic_requirements"].extend(
                vulnerability.metadata.get("requirements", []))

        # Add requirements from previous steps
        for step in previous_steps:
            requirements["basic_requirements"].extend(step.outputs)

        # Add primitive-specific requirements
        primitive_requirements = {
            ExploitPrimitive.BYPASS_ASLR: ["information_leak", "base_calculation"],
            ExploitPrimitive.ROP_GADGET: ["gadget_addresses", "register_control"],
            ExploitPrimitive.SHELLCODE_INJECTION: [
                "executable_memory", "payload_space"],
        }

        if primitive_type in primitive_requirements:
            requirements["basic_requirements"].extend(
                primitive_requirements[primitive_type])

        # Remove duplicates
        requirements["basic_requirements"] = list(
            set(requirements["basic_requirements"]))
        return requirements

    def _calculate_complexity(self, steps: list[ExploitStep]) -> ChainComplexity:
        """Calculate exploit chain complexity."""
        num_steps = len(steps)
        avg_reliability = sum(step.reliability for step in steps) / len(steps)

        if num_steps <= 2 and avg_reliability >= 0.8:
            return ChainComplexity.SIMPLE
        if num_steps <= 3 and avg_reliability >= 0.7:
            return ChainComplexity.MODERATE
        if num_steps <= 5 and avg_reliability >= 0.6:
            return ChainComplexity.COMPLEX
        if num_steps <= 7:
            return ChainComplexity.ADVANCED
        return ChainComplexity.EXPERT

    def _calculate_success_probability(self, steps: list[ExploitStep]) -> float:
        """Calculate overall success probability."""
        if not steps:
            return 0.0

        # Multiply individual step reliabilities
        prob = 1.0
        for step in steps:
            prob *= step.reliability

        return prob

    def _calculate_stealth_rating(self, steps: list[ExploitStep]) -> float:
        """Calculate stealth rating (lower is more stealthy)."""
        # Simple heuristic based on step types
        stealth_scores = {
            ExploitPrimitive.INFORMATION_DISCLOSURE: 0.9,  # High stealth
            ExploitPrimitive.MEMORY_CORRUPTION: 0.6,      # Medium stealth
            ExploitPrimitive.SHELLCODE_INJECTION: 0.3,     # Low stealth
        }

        total_score = 0.0
        for step in steps:
            total_score += stealth_scores.get(step.step_type, 0.5)

        return total_score / len(steps) if steps else 0.0

    def _calculate_stability_rating(self, steps: list[ExploitStep]) -> float:
        """Calculate stability rating."""
        return self._calculate_success_probability(steps)


class SafetyVerificationSystem:
    """System for verifying exploit chain safety."""

    def __init__(self):
        """Initialize the safety verification system.

        Sets up safety rules and risk thresholds for evaluating
        exploit chains to ensure controlled and safe execution.
        """
        self.safety_rules: list[dict[str, Any]] = []
        self.risk_thresholds = {
            "system_damage": 0.1,
            "data_corruption": 0.05,
            "service_disruption": 0.2,
            "unauthorized_access": 0.3,
        }
        self._initialize_safety_rules()

    def _initialize_safety_rules(self):
        """Initialize safety verification rules."""
        self.safety_rules = [
            {
                "rule_id": "no_destructive_operations",
                "description": "Prevent destructive file operations",
                "pattern": r"(rm|del|format|erase|destroy)",
                "severity": "critical",
                "action": "block",
            },
            {
                "rule_id": "no_system_modification",
                "description": "Prevent system file modification",
                "pattern": r"(registry|system32|boot|kernel)",
                "severity": "high",
                "action": "warn",
            },
            {
                "rule_id": "no_network_propagation",
                "description": "Prevent network propagation",
                "pattern": r"(worm|propagate|spread|replicate)",
                "severity": "high",
                "action": "block",
            },
            {
                "rule_id": "limited_privilege_escalation",
                "description": "Limit privilege escalation scope",
                "pattern": r"(sudo|admin|root|SYSTEM)",
                "severity": "medium",
                "action": "review",
            },
        ]

    @profile_ai_operation("verify_chain_safety")
    def verify_chain_safety(self, chain: ExploitChain) -> dict[str, Any]:
        """Verify exploit chain safety."""
        logger.info(f"Verifying safety for chain: {chain.chain_id}")

        safety_report = {
            "is_safe": True,
            "risk_level": "low",
            "violations": [],
            "warnings": [],
            "recommendations": [],
            "overall_score": 1.0,
        }

        # Check each step against safety rules
        for step in chain.steps:
            step_violations = self._check_step_safety(step)
            safety_report["violations"].extend(step_violations)

        # Calculate risk level
        if safety_report["violations"]:
            critical_violations = [
                v for v in safety_report["violations"] if v["severity"] == "critical"]
            high_violations = [
                v for v in safety_report["violations"] if v["severity"] == "high"]

            if critical_violations:
                safety_report["is_safe"] = False
                safety_report["risk_level"] = "critical"
                safety_report["overall_score"] = 0.0
            elif len(high_violations) > 2:
                safety_report["is_safe"] = False
                safety_report["risk_level"] = "high"
                safety_report["overall_score"] = 0.2
            elif high_violations:
                safety_report["risk_level"] = "medium"
                safety_report["overall_score"] = 0.6

        # Add recommendations
        if not safety_report["is_safe"]:
            safety_report["recommendations"].extend([
                "Review exploit chain for safety violations",
                "Consider sandboxed testing environment",
                "Implement additional safety checks",
                "Reduce scope of exploitation",
            ])

        logger.info(
            f"Safety verification complete: {safety_report['risk_level']} risk")
        return safety_report

    def _check_step_safety(self, step: ExploitStep) -> list[dict[str, Any]]:
        """Check individual step safety."""
        violations = []

        # Check code template against safety rules
        for rule in self.safety_rules:
            if self._matches_rule(step.code_template + " " + step.description, rule):
                violation = {
                    "rule_id": rule["rule_id"],
                    "step_id": step.step_id,
                    "severity": rule["severity"],
                    "description": rule["description"],
                    "action": rule["action"],
                }
                violations.append(violation)

        return violations

    def _matches_rule(self, text: str, rule: dict[str, Any]) -> bool:
        """Check if text matches safety rule."""
        import re
        pattern = rule.get("pattern", "")
        return bool(re.search(pattern, text, re.IGNORECASE))


class AutomatedExploitChainBuilder:
    """Main automated exploit chain builder."""

    def __init__(self, llm_manager: LLMManager | None = None):
        """Initialize the automated exploit chain builder.

        Args:
            llm_manager: Optional LLM manager for AI-enhanced chain building

        """
        self.llm_manager = llm_manager or LLMManager()
        self.primitive_lib = ExploitPrimitiveLibrary()
        self.chain_framework = ExploitChainFramework(self.primitive_lib)
        self.safety_system = SafetyVerificationSystem()
        self.built_chains: dict[str, ExploitChain] = {}
        self.chain_cache: dict[str, str] = {}  # vuln_hash -> chain_id

    @profile_ai_operation("build_automated_chain")
    def build_exploit_chain(self, vulnerability: Vulnerability,
                            requirements: dict[str, Any] | None = None) -> ExploitChain | None:
        """Build and verify exploit chain."""
        # Check cache first
        vuln_hash = self._hash_vulnerability(vulnerability)
        if vuln_hash in self.chain_cache:
            cached_chain_id = self.chain_cache[vuln_hash]
            if cached_chain_id in self.built_chains:
                logger.info(f"Using cached chain: {cached_chain_id}")
                return self.built_chains[cached_chain_id]

        # Build new chain
        chain = self.chain_framework.build_exploit_chain(
            vulnerability, requirements)
        if not chain:
            return None

        # Enhance with AI
        enhanced_chain = self._enhance_chain_with_ai(chain, vulnerability)

        # Verify safety
        safety_report = self.safety_system.verify_chain_safety(enhanced_chain)
        enhanced_chain.safety_verified = safety_report["is_safe"]
        enhanced_chain.safety_report = safety_report

        if not safety_report["is_safe"]:
            logger.warning(
                f"Chain {enhanced_chain.chain_id} failed safety verification")

            # Try to create safer alternative
            safe_chain = self._create_safe_alternative(
                enhanced_chain, vulnerability)
            if safe_chain:
                enhanced_chain = safe_chain

        # Store chain
        self.built_chains[enhanced_chain.chain_id] = enhanced_chain
        self.chain_cache[vuln_hash] = enhanced_chain.chain_id

        # Record learning
        get_learning_engine().record_exploit_chain_creation(
            vulnerability=vulnerability,
            chain=enhanced_chain,
            success=True,
        )

        logger.info(f"Built exploit chain: {enhanced_chain.chain_id}")
        return enhanced_chain

    def _hash_vulnerability(self, vulnerability: Vulnerability) -> str:
        """Create hash for vulnerability caching."""
        hash_data = f"{vulnerability.vuln_type.value}_{vulnerability.description}_{vulnerability.location}"
        return hashlib.md5(hash_data.encode(), usedforsecurity=False).hexdigest()

    def _enhance_chain_with_ai(self, chain: ExploitChain, vulnerability: Vulnerability) -> ExploitChain:
        """Enhance exploit chain using AI assistance."""
        # Create AI prompt for chain enhancement
        prompt = f"""
Analyze and enhance this exploit chain for {vulnerability.vuln_type.value}:

Vulnerability: {vulnerability.description}
Current chain has {len(chain.steps)} steps with {chain.success_probability:.2f} success probability.

Please suggest improvements for:
1. Reliability enhancement
2. Stealth optimization
3. Error handling
4. Alternative approaches

Focus on practical, working techniques.
"""

        try:
            messages = [LLMMessage(role="user", content=prompt)]
            response = self.llm_manager.generate_response(messages)

            if response and response.content:
                # Parse AI suggestions and apply improvements
                enhanced_chain = self._apply_ai_suggestions(
                    chain, response.content)
                return enhanced_chain

        except Exception as e:
            logger.warning(f"AI enhancement failed: {e}")

        return chain

    def _apply_ai_suggestions(self, chain: ExploitChain, ai_suggestions: str) -> ExploitChain:
        """Apply AI suggestions to enhance chain."""
        # Simple implementation - in production would parse suggestions more thoroughly

        # Improve reliability based on AI feedback
        for step in chain.steps:
            if "reliability" in ai_suggestions.lower():
                step.reliability = min(1.0, step.reliability * 1.1)

        # Recalculate chain metrics
        chain.success_probability = self.chain_framework._calculate_success_probability(
            chain.steps)
        chain.stability_rating = self.chain_framework._calculate_stability_rating(
            chain.steps)

        return chain

    def _create_safe_alternative(self, unsafe_chain: ExploitChain,
                                 vulnerability: Vulnerability) -> ExploitChain | None:
        """Create safer alternative to unsafe chain."""
        logger.info(
            f"Creating safe alternative to unsafe chain with {len(unsafe_chain.steps)} steps")

        # Analyze the unsafe chain to understand what makes it risky
        risk_factors = self._analyze_chain_risks(unsafe_chain)
        logger.debug(f"Identified risk factors: {risk_factors}")

        # Build more conservative chain using safer primitives
        safe_requirements = {
            "max_risk_level": "low",
            "avoid_destructive": True,
            "prefer_reversible": True,
            "stealth_priority": True,
            "no_system_modification": True,
            "limited_scope": True,
        }

        alternative = self.chain_framework.build_exploit_chain(
            vulnerability, safe_requirements)
        if alternative:
            alternative.name = f"safe_{alternative.name}"
            logger.info(f"Created safe alternative: {alternative.chain_id}")

        return alternative

    def _analyze_chain_risks(self, chain: ExploitChain) -> list[str]:
        """Analyze risks in an exploit chain."""
        risks = []

        for step in chain.steps:
            # Check for destructive operations
            if step.primitive_type in ["file_deletion", "registry_modification", "system_shutdown"]:
                risks.append(f"Destructive operation: {step.primitive_type}")

            # Check for high-privilege requirements
            if step.required_privileges and "admin" in step.required_privileges.lower():
                risks.append(f"Requires admin privileges: {step.name}")

            # Check for network operations
            if "network" in step.primitive_type.lower():
                risks.append(f"Network operation: {step.primitive_type}")

        return risks

    def validate_chain(self, chain_id: str) -> ChainValidationResult:
        """Validate existing exploit chain."""
        if chain_id not in self.built_chains:
            return ChainValidationResult(
                is_valid=False,
                confidence=0.0,
                issues=["Chain not found"],
            )

        chain = self.built_chains[chain_id]

        # Perform validation checks
        issues = []
        warnings = []
        optimizations = []

        # Check step dependencies
        available_outputs = set()
        for step in chain.steps:
            missing_prereqs = set(step.prerequisites) - available_outputs
            if missing_prereqs:
                issues.append(
                    f"Step {step.step_id} missing prerequisites: {missing_prereqs}")
            available_outputs.update(step.outputs)

        # Check reliability
        if chain.success_probability < 0.5:
            warnings.append("Low success probability")
            optimizations.append("Consider alternative approaches")

        # Check complexity
        if chain.complexity in [ChainComplexity.ADVANCED, ChainComplexity.EXPERT]:
            warnings.append("High complexity may affect reliability")

        is_valid = len(issues) == 0
        confidence = chain.success_probability if is_valid else 0.0

        return ChainValidationResult(
            is_valid=is_valid,
            confidence=confidence,
            issues=issues,
            warnings=warnings,
            optimizations=optimizations,
            estimated_success_rate=chain.success_probability,
            risk_assessment=chain.safety_report,
        )

    def get_chain_statistics(self) -> dict[str, Any]:
        """Get statistics about built chains."""
        if not self.built_chains:
            return {"total_chains": 0}

        chains = list(self.built_chains.values())

        stats = {
            "total_chains": len(chains),
            "avg_success_probability": sum(c.success_probability for c in chains) / len(chains),
            "complexity_distribution": {},
            "safety_verified": sum(1 for c in chains if c.safety_verified),
            "tested_chains": sum(1 for c in chains if c.tested),
        }

        # Complexity distribution
        for complexity in ChainComplexity:
            count = sum(1 for c in chains if c.complexity == complexity)
            stats["complexity_distribution"][complexity.value] = count

        return stats


# Global instance
exploit_chain_builder = AutomatedExploitChainBuilder()
