"""
Automated Exploit Chain Builder

Copyright (C) 2025 Zachary Flint

This file is part of Intellicrack.

Intellicrack is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Intellicrack is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Intellicrack.  If not, see <https://www.gnu.org/licenses/>.
"""

import hashlib
import uuid
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, List, Optional

from ..utils.logger import get_logger
from .learning_engine import learning_engine
from .llm_backends import LLMManager, LLMMessage
from .performance_monitor import profile_ai_operation

logger = get_logger(__name__)


class ExploitType(Enum):
    """Types of exploit primitives."""
    BUFFER_OVERFLOW = "buffer_overflow"
    FORMAT_STRING = "format_string"
    USE_AFTER_FREE = "use_after_free"
    DOUBLE_FREE = "double_free"
    INTEGER_OVERFLOW = "integer_overflow"
    NULL_POINTER_DEREF = "null_pointer_deref"
    PRIVILEGE_ESCALATION = "privilege_escalation"
    CODE_INJECTION = "code_injection"
    COMMAND_INJECTION = "command_injection"
    SQL_INJECTION = "sql_injection"
    CROSS_SITE_SCRIPTING = "cross_site_scripting"
    RACE_CONDITION = "race_condition"
    AUTHENTICATION_BYPASS = "authentication_bypass"
    AUTHORIZATION_BYPASS = "authorization_bypass"
    CRYPTOGRAPHIC_WEAKNESS = "cryptographic_weakness"
    DESERIALIZATION = "deserialization"
    PATH_TRAVERSAL = "path_traversal"
    REMOTE_CODE_EXECUTION = "remote_code_execution"


class ExploitPrimitive(Enum):
    """Basic exploit building blocks."""
    MEMORY_CORRUPTION = "memory_corruption"
    CONTROL_FLOW_HIJACK = "control_flow_hijack"
    DATA_LEAK = "data_leak"
    ARBITRARY_WRITE = "arbitrary_write"
    ARBITRARY_READ = "arbitrary_read"
    HEAP_MANIPULATION = "heap_manipulation"
    STACK_MANIPULATION = "stack_manipulation"
    ROP_GADGET = "rop_gadget"
    JOP_GADGET = "jop_gadget"
    SHELLCODE_INJECTION = "shellcode_injection"
    BYPASS_ASLR = "bypass_aslr"
    BYPASS_DEP = "bypass_dep"
    BYPASS_STACK_CANARY = "bypass_stack_canary"
    INFORMATION_DISCLOSURE = "information_disclosure"
    TIMING_ATTACK = "timing_attack"


class ChainComplexity(Enum):
    """Complexity levels of exploit chains."""
    SIMPLE = "simple"
    MODERATE = "moderate"
    COMPLEX = "complex"
    ADVANCED = "advanced"
    EXPERT = "expert"


@dataclass
class Vulnerability:
    """Vulnerability information for exploit development."""
    vuln_id: str
    vuln_type: ExploitType
    severity: str  # low, medium, high, critical
    description: str
    location: Dict[str, Any]  # file, function, line, etc.
    prerequisites: List[str] = field(default_factory=list)
    impact: Dict[str, str] = field(default_factory=dict)
    cve_id: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    confidence: float = 0.0
    exploitability: float = 0.0
    discovery_method: str = "unknown"


@dataclass
class ExploitStep:
    """Individual step in an exploit chain."""
    step_id: str
    step_type: ExploitPrimitive
    description: str
    code_template: str
    prerequisites: List[str] = field(default_factory=list)
    outputs: List[str] = field(default_factory=list)
    success_criteria: List[str] = field(default_factory=list)
    failure_modes: List[str] = field(default_factory=list)
    reliability: float = 0.0
    execution_time: float = 0.0
    memory_requirements: int = 0
    payload_data: Optional[str] = None
    verification_code: Optional[str] = None


@dataclass
class ExploitChain:
    """Complete exploit chain."""
    chain_id: str
    name: str
    target_vulnerability: str
    steps: List[ExploitStep]
    complexity: ChainComplexity
    success_probability: float
    total_execution_time: float
    memory_footprint: int
    stealth_rating: float
    stability_rating: float
    created_at: datetime = field(default_factory=datetime.now)
    tested: bool = False
    test_results: Dict[str, Any] = field(default_factory=dict)
    safety_verified: bool = False
    safety_report: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ChainValidationResult:
    """Results of exploit chain validation."""
    is_valid: bool
    confidence: float
    issues: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    optimizations: List[str] = field(default_factory=list)
    estimated_success_rate: float = 0.0
    risk_assessment: Dict[str, str] = field(default_factory=dict)


class ExploitPrimitiveLibrary:
    """Library of exploit primitives and templates."""

    def __init__(self):
        self.primitives: Dict[ExploitPrimitive, List[Dict[str, Any]]] = defaultdict(list)
        self.templates: Dict[str, str] = {}
        self.gadgets: Dict[str, List[str]] = defaultdict(list)
        self._initialize_library()

    def _initialize_library(self):
        """Initialize with common exploit primitives."""

        # Buffer overflow primitives
        self.primitives[ExploitPrimitive.MEMORY_CORRUPTION].extend([
            {
                "name": "stack_buffer_overflow",
                "description": "Classic stack buffer overflow",
                "template": """
// Stack buffer overflow exploit
char payload[BUFFER_SIZE];
memset(payload, 'A', OVERFLOW_OFFSET);
*(void**)(payload + OVERFLOW_OFFSET) = (void*)TARGET_ADDRESS;
""",
                "prerequisites": ["buffer_size_known", "return_address_offset"],
                "reliability": 0.8
            },
            {
                "name": "heap_buffer_overflow",
                "description": "Heap buffer overflow with metadata corruption",
                "template": """
// Heap buffer overflow
char* heap_buffer = malloc(BUFFER_SIZE);
memset(heap_buffer, 'B', OVERFLOW_SIZE);
// Corrupt heap metadata
*(size_t*)(heap_buffer + BUFFER_SIZE) = FAKE_SIZE;
""",
                "prerequisites": ["heap_layout_known", "allocator_knowledge"],
                "reliability": 0.6
            }
        ])

        # Control flow hijacking
        self.primitives[ExploitPrimitive.CONTROL_FLOW_HIJACK].extend([
            {
                "name": "return_address_overwrite",
                "description": "Overwrite return address on stack",
                "template": """
// Return address overwrite
void exploit_function() {
    char buffer[BUFFER_SIZE];
    // Overflow to overwrite return address
    strcpy(buffer, MALICIOUS_INPUT);
}
""",
                "prerequisites": ["stack_layout", "return_offset"],
                "reliability": 0.9
            },
            {
                "name": "function_pointer_overwrite",
                "description": "Overwrite function pointer",
                "template": """
// Function pointer overwrite
struct vtable {
    void (*func_ptr)(void);
};
struct vtable* obj = (struct vtable*)TARGET_OBJECT;
obj->func_ptr = (void*)SHELLCODE_ADDRESS;
""",
                "prerequisites": ["object_layout", "function_pointer_location"],
                "reliability": 0.7
            }
        ])

        # ROP gadgets
        self.primitives[ExploitPrimitive.ROP_GADGET].extend([
            {
                "name": "pop_ret_gadget",
                "description": "POP instruction followed by RET",
                "template": "pop rax; ret",
                "prerequisites": ["gadget_address"],
                "reliability": 0.95
            },
            {
                "name": "syscall_gadget",
                "description": "System call gadget",
                "template": "syscall; ret",
                "prerequisites": ["syscall_number", "register_setup"],
                "reliability": 0.85
            }
        ])

        # Shellcode injection
        self.primitives[ExploitPrimitive.SHELLCODE_INJECTION].extend([
            {
                "name": "nop_sled_shellcode",
                "description": "NOP sled with shellcode",
                "template": """
// NOP sled + shellcode
unsigned char exploit[] = {
    // NOP sled
    0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90, 0x90,
    // Shellcode starts here
    SHELLCODE_BYTES
};
""",
                "prerequisites": ["executable_memory", "shellcode"],
                "reliability": 0.8
            }
        ])

        # ASLR bypass techniques
        self.primitives[ExploitPrimitive.BYPASS_ASLR].extend([
            {
                "name": "information_leak",
                "description": "Leak addresses to bypass ASLR",
                "template": """
// Information leak to bypass ASLR
void* leaked_address = get_leaked_pointer();
void* base_address = (void*)((uintptr_t)leaked_address & ~0xfff);
void* target_function = base_address + FUNCTION_OFFSET;
""",
                "prerequisites": ["leak_primitive", "base_calculation"],
                "reliability": 0.7
            }
        ])

    @profile_ai_operation("get_primitive")
    def get_primitive(self, primitive_type: ExploitPrimitive,
                     requirements: Optional[List[str]] = None) -> Optional[Dict[str, Any]]:
        """Get best matching primitive for requirements."""
        candidates = self.primitives.get(primitive_type, [])

        if not candidates:
            return None

        if not requirements:
            return candidates[0]  # Return first available

        # Score candidates based on requirement matching
        best_candidate = None
        best_score = 0.0

        for candidate in candidates:
            score = self._score_primitive(candidate, requirements)
            if score > best_score:
                best_score = score
                best_candidate = candidate

        return best_candidate

    def _score_primitive(self, primitive: Dict[str, Any], requirements: List[str]) -> float:
        """Score primitive based on requirement matching."""
        prereqs = set(primitive.get("prerequisites", []))
        req_set = set(requirements)

        # Requirements satisfied
        satisfied = len(prereqs.intersection(req_set))
        total_reqs = len(prereqs)

        if total_reqs == 0:
            requirement_score = 1.0
        else:
            requirement_score = satisfied / total_reqs

        # Base reliability
        reliability = primitive.get("reliability", 0.5)

        return (requirement_score * 0.7) + (reliability * 0.3)

    def add_primitive(self, primitive_type: ExploitPrimitive, primitive_data: Dict[str, Any]):
        """Add new primitive to library."""
        self.primitives[primitive_type].append(primitive_data)
        logger.info(f"Added new primitive: {primitive_type.value}")


class ExploitChainFramework:
    """Framework for building exploit chains."""

    def __init__(self, primitive_library: Optional[ExploitPrimitiveLibrary] = None):
        self.primitive_lib = primitive_library or ExploitPrimitiveLibrary()
        self.chain_templates: Dict[str, List[ExploitPrimitive]] = {}
        self.success_patterns: Dict[str, float] = {}
        self._initialize_templates()

    def _initialize_templates(self):
        """Initialize common exploit chain templates."""

        # Simple buffer overflow chain
        self.chain_templates["simple_buffer_overflow"] = [
            ExploitPrimitive.MEMORY_CORRUPTION,
            ExploitPrimitive.CONTROL_FLOW_HIJACK,
            ExploitPrimitive.SHELLCODE_INJECTION
        ]

        # ROP-based exploitation
        self.chain_templates["rop_exploitation"] = [
            ExploitPrimitive.MEMORY_CORRUPTION,
            ExploitPrimitive.BYPASS_ASLR,
            ExploitPrimitive.ROP_GADGET,
            ExploitPrimitive.SHELLCODE_INJECTION
        ]

        # Advanced heap exploitation
        self.chain_templates["heap_exploitation"] = [
            ExploitPrimitive.HEAP_MANIPULATION,
            ExploitPrimitive.ARBITRARY_WRITE,
            ExploitPrimitive.CONTROL_FLOW_HIJACK,
            ExploitPrimitive.BYPASS_DEP,
            ExploitPrimitive.SHELLCODE_INJECTION
        ]

        # Information disclosure chain
        self.chain_templates["info_disclosure"] = [
            ExploitPrimitive.INFORMATION_DISCLOSURE,
            ExploitPrimitive.BYPASS_ASLR,
            ExploitPrimitive.MEMORY_CORRUPTION,
            ExploitPrimitive.CONTROL_FLOW_HIJACK
        ]

    @profile_ai_operation("build_exploit_chain")
    def build_exploit_chain(self, vulnerability: Vulnerability,
                          requirements: Optional[Dict[str, Any]] = None) -> Optional[ExploitChain]:
        """Build exploit chain for vulnerability."""
        logger.info(f"Building exploit chain for {vulnerability.vuln_type.value}")

        # Select appropriate template
        template = self._select_chain_template(vulnerability)
        if not template:
            logger.warning(f"No template found for {vulnerability.vuln_type.value}")
            return None

        # Build chain steps
        steps = []
        chain_requirements = requirements or {}

        # Use chain_requirements to guide primitive selection
        required_capabilities = chain_requirements.get('capabilities', [])
        max_complexity = chain_requirements.get('max_complexity', 10)

        logger.debug(f"Building chain with requirements: capabilities={required_capabilities}, max_complexity={max_complexity}")

        for i, primitive_type in enumerate(template):
            step_reqs = self._get_step_requirements(vulnerability, primitive_type, steps)

            # Apply chain requirements to step requirements
            if required_capabilities:
                step_reqs['required_capabilities'] = required_capabilities
            step_reqs['max_complexity'] = max_complexity

            primitive = self.primitive_lib.get_primitive(primitive_type, step_reqs)

            if not primitive:
                logger.warning(f"No primitive found for {primitive_type.value}")
                continue

            step = ExploitStep(
                step_id=f"step_{i+1}_{primitive_type.value}",
                step_type=primitive_type,
                description=primitive.get("description", ""),
                code_template=primitive.get("template", ""),
                prerequisites=primitive.get("prerequisites", []),
                reliability=primitive.get("reliability", 0.5)
            )

            steps.append(step)

        if not steps:
            return None

        # Calculate chain metrics
        complexity = self._calculate_complexity(steps)
        success_prob = self._calculate_success_probability(steps)
        exec_time = sum(step.execution_time for step in steps)
        memory_footprint = max(step.memory_requirements for step in steps)

        chain = ExploitChain(
            chain_id=str(uuid.uuid4()),
            name=f"{vulnerability.vuln_type.value}_exploit_chain",
            target_vulnerability=vulnerability.vuln_id,
            steps=steps,
            complexity=complexity,
            success_probability=success_prob,
            total_execution_time=exec_time,
            memory_footprint=memory_footprint,
            stealth_rating=self._calculate_stealth_rating(steps),
            stability_rating=self._calculate_stability_rating(steps)
        )

        logger.info(f"Built exploit chain: {chain.chain_id} (success prob: {success_prob:.2f})")
        return chain

    def _select_chain_template(self, vulnerability: Vulnerability) -> Optional[List[ExploitPrimitive]]:
        """Select appropriate chain template for vulnerability."""
        vuln_type = vulnerability.vuln_type

        # Map vulnerability types to templates
        template_mapping = {
            ExploitType.BUFFER_OVERFLOW: "simple_buffer_overflow",
            ExploitType.USE_AFTER_FREE: "heap_exploitation",
            ExploitType.FORMAT_STRING: "rop_exploitation",
            ExploitType.INTEGER_OVERFLOW: "simple_buffer_overflow",
            ExploitType.PRIVILEGE_ESCALATION: "rop_exploitation"
        }

        template_name = template_mapping.get(vuln_type)
        if template_name:
            return self.chain_templates.get(template_name)

        # Default template
        return self.chain_templates.get("simple_buffer_overflow")

    def _get_step_requirements(self, vulnerability: Vulnerability,
                             primitive_type: ExploitPrimitive,
                             previous_steps: List[ExploitStep]) -> Dict[str, Any]:
        """Get requirements for specific step based on context."""
        requirements = {
            'basic_requirements': [],
            'required_capabilities': [],
            'max_complexity': 5
        }

        # Add vulnerability-specific requirements
        if vulnerability.metadata:
            requirements['basic_requirements'].extend(vulnerability.metadata.get("requirements", []))

        # Add requirements from previous steps
        for step in previous_steps:
            requirements['basic_requirements'].extend(step.outputs)

        # Add primitive-specific requirements
        primitive_requirements = {
            ExploitPrimitive.BYPASS_ASLR: ["information_leak", "base_calculation"],
            ExploitPrimitive.ROP_GADGET: ["gadget_addresses", "register_control"],
            ExploitPrimitive.SHELLCODE_INJECTION: ["executable_memory", "payload_space"]
        }

        if primitive_type in primitive_requirements:
            requirements['basic_requirements'].extend(primitive_requirements[primitive_type])

        # Remove duplicates
        requirements['basic_requirements'] = list(set(requirements['basic_requirements']))
        return requirements

    def _calculate_complexity(self, steps: List[ExploitStep]) -> ChainComplexity:
        """Calculate exploit chain complexity."""
        num_steps = len(steps)
        avg_reliability = sum(step.reliability for step in steps) / len(steps)

        if num_steps <= 2 and avg_reliability >= 0.8:
            return ChainComplexity.SIMPLE
        elif num_steps <= 3 and avg_reliability >= 0.7:
            return ChainComplexity.MODERATE
        elif num_steps <= 5 and avg_reliability >= 0.6:
            return ChainComplexity.COMPLEX
        elif num_steps <= 7:
            return ChainComplexity.ADVANCED
        else:
            return ChainComplexity.EXPERT

    def _calculate_success_probability(self, steps: List[ExploitStep]) -> float:
        """Calculate overall success probability."""
        if not steps:
            return 0.0

        # Multiply individual step reliabilities
        prob = 1.0
        for step in steps:
            prob *= step.reliability

        return prob

    def _calculate_stealth_rating(self, steps: List[ExploitStep]) -> float:
        """Calculate stealth rating (lower is more stealthy)."""
        # Simple heuristic based on step types
        stealth_scores = {
            ExploitPrimitive.INFORMATION_DISCLOSURE: 0.9,  # High stealth
            ExploitPrimitive.MEMORY_CORRUPTION: 0.6,      # Medium stealth
            ExploitPrimitive.SHELLCODE_INJECTION: 0.3     # Low stealth
        }

        total_score = 0.0
        for step in steps:
            total_score += stealth_scores.get(step.step_type, 0.5)

        return total_score / len(steps) if steps else 0.0

    def _calculate_stability_rating(self, steps: List[ExploitStep]) -> float:
        """Calculate stability rating."""
        return self._calculate_success_probability(steps)


class SafetyVerificationSystem:
    """System for verifying exploit chain safety."""

    def __init__(self):
        self.safety_rules: List[Dict[str, Any]] = []
        self.risk_thresholds = {
            "system_damage": 0.1,
            "data_corruption": 0.05,
            "service_disruption": 0.2,
            "unauthorized_access": 0.3
        }
        self._initialize_safety_rules()

    def _initialize_safety_rules(self):
        """Initialize safety verification rules."""
        self.safety_rules = [
            {
                "rule_id": "no_destructive_operations",
                "description": "Prevent destructive file operations",
                "pattern": r"(rm|del|format|erase|destroy)",
                "severity": "critical",
                "action": "block"
            },
            {
                "rule_id": "no_system_modification",
                "description": "Prevent system file modification",
                "pattern": r"(registry|system32|boot|kernel)",
                "severity": "high",
                "action": "warn"
            },
            {
                "rule_id": "no_network_propagation",
                "description": "Prevent network propagation",
                "pattern": r"(worm|propagate|spread|replicate)",
                "severity": "high",
                "action": "block"
            },
            {
                "rule_id": "limited_privilege_escalation",
                "description": "Limit privilege escalation scope",
                "pattern": r"(sudo|admin|root|SYSTEM)",
                "severity": "medium",
                "action": "review"
            }
        ]

    @profile_ai_operation("verify_chain_safety")
    def verify_chain_safety(self, chain: ExploitChain) -> Dict[str, Any]:
        """Verify exploit chain safety."""
        logger.info(f"Verifying safety for chain: {chain.chain_id}")

        safety_report = {
            "is_safe": True,
            "risk_level": "low",
            "violations": [],
            "warnings": [],
            "recommendations": [],
            "overall_score": 1.0
        }

        # Check each step against safety rules
        for step in chain.steps:
            step_violations = self._check_step_safety(step)
            safety_report["violations"].extend(step_violations)

        # Calculate risk level
        if safety_report["violations"]:
            critical_violations = [v for v in safety_report["violations"] if v["severity"] == "critical"]
            high_violations = [v for v in safety_report["violations"] if v["severity"] == "high"]

            if critical_violations:
                safety_report["is_safe"] = False
                safety_report["risk_level"] = "critical"
                safety_report["overall_score"] = 0.0
            elif len(high_violations) > 2:
                safety_report["is_safe"] = False
                safety_report["risk_level"] = "high"
                safety_report["overall_score"] = 0.2
            elif high_violations:
                safety_report["risk_level"] = "medium"
                safety_report["overall_score"] = 0.6

        # Add recommendations
        if not safety_report["is_safe"]:
            safety_report["recommendations"].extend([
                "Review exploit chain for safety violations",
                "Consider sandboxed testing environment",
                "Implement additional safety checks",
                "Reduce scope of exploitation"
            ])

        logger.info(f"Safety verification complete: {safety_report['risk_level']} risk")
        return safety_report

    def _check_step_safety(self, step: ExploitStep) -> List[Dict[str, Any]]:
        """Check individual step safety."""
        violations = []

        # Check code template against safety rules
        for rule in self.safety_rules:
            if self._matches_rule(step.code_template + " " + step.description, rule):
                violation = {
                    "rule_id": rule["rule_id"],
                    "step_id": step.step_id,
                    "severity": rule["severity"],
                    "description": rule["description"],
                    "action": rule["action"]
                }
                violations.append(violation)

        return violations

    def _matches_rule(self, text: str, rule: Dict[str, Any]) -> bool:
        """Check if text matches safety rule."""
        import re
        pattern = rule.get("pattern", "")
        return bool(re.search(pattern, text, re.IGNORECASE))


class AutomatedExploitChainBuilder:
    """Main automated exploit chain builder."""

    def __init__(self, llm_manager: Optional[LLMManager] = None):
        self.llm_manager = llm_manager or LLMManager()
        self.primitive_lib = ExploitPrimitiveLibrary()
        self.chain_framework = ExploitChainFramework(self.primitive_lib)
        self.safety_system = SafetyVerificationSystem()
        self.built_chains: Dict[str, ExploitChain] = {}
        self.chain_cache: Dict[str, str] = {}  # vuln_hash -> chain_id

    @profile_ai_operation("build_automated_chain")
    def build_exploit_chain(self, vulnerability: Vulnerability,
                          requirements: Optional[Dict[str, Any]] = None) -> Optional[ExploitChain]:
        """Build and verify exploit chain."""

        # Check cache first
        vuln_hash = self._hash_vulnerability(vulnerability)
        if vuln_hash in self.chain_cache:
            cached_chain_id = self.chain_cache[vuln_hash]
            if cached_chain_id in self.built_chains:
                logger.info(f"Using cached chain: {cached_chain_id}")
                return self.built_chains[cached_chain_id]

        # Build new chain
        chain = self.chain_framework.build_exploit_chain(vulnerability, requirements)
        if not chain:
            return None

        # Enhance with AI
        enhanced_chain = self._enhance_chain_with_ai(chain, vulnerability)

        # Verify safety
        safety_report = self.safety_system.verify_chain_safety(enhanced_chain)
        enhanced_chain.safety_verified = safety_report["is_safe"]
        enhanced_chain.safety_report = safety_report

        if not safety_report["is_safe"]:
            logger.warning(f"Chain {enhanced_chain.chain_id} failed safety verification")

            # Try to create safer alternative
            safe_chain = self._create_safe_alternative(enhanced_chain, vulnerability)
            if safe_chain:
                enhanced_chain = safe_chain

        # Store chain
        self.built_chains[enhanced_chain.chain_id] = enhanced_chain
        self.chain_cache[vuln_hash] = enhanced_chain.chain_id

        # Record learning
        learning_engine.record_exploit_chain_creation(
            vulnerability=vulnerability,
            chain=enhanced_chain,
            success=True
        )

        logger.info(f"Built exploit chain: {enhanced_chain.chain_id}")
        return enhanced_chain

    def _hash_vulnerability(self, vulnerability: Vulnerability) -> str:
        """Create hash for vulnerability caching."""
        hash_data = f"{vulnerability.vuln_type.value}_{vulnerability.description}_{vulnerability.location}"
        return hashlib.md5(hash_data.encode()).hexdigest()

    def _enhance_chain_with_ai(self, chain: ExploitChain, vulnerability: Vulnerability) -> ExploitChain:
        """Enhance exploit chain using AI assistance."""

        # Create AI prompt for chain enhancement
        prompt = f"""
Analyze and enhance this exploit chain for {vulnerability.vuln_type.value}:

Vulnerability: {vulnerability.description}
Current chain has {len(chain.steps)} steps with {chain.success_probability:.2f} success probability.

Please suggest improvements for:
1. Reliability enhancement
2. Stealth optimization  
3. Error handling
4. Alternative approaches

Focus on practical, working techniques.
"""

        try:
            messages = [LLMMessage(role="user", content=prompt)]
            response = self.llm_manager.generate_response(messages)

            if response and response.content:
                # Parse AI suggestions and apply improvements
                enhanced_chain = self._apply_ai_suggestions(chain, response.content)
                return enhanced_chain

        except Exception as e:
            logger.warning(f"AI enhancement failed: {e}")

        return chain

    def _apply_ai_suggestions(self, chain: ExploitChain, ai_suggestions: str) -> ExploitChain:
        """Apply AI suggestions to enhance chain."""
        # Simple implementation - in production would parse suggestions more thoroughly

        # Improve reliability based on AI feedback
        for step in chain.steps:
            if "reliability" in ai_suggestions.lower():
                step.reliability = min(1.0, step.reliability * 1.1)

        # Recalculate chain metrics
        chain.success_probability = self.chain_framework._calculate_success_probability(chain.steps)
        chain.stability_rating = self.chain_framework._calculate_stability_rating(chain.steps)

        return chain

    def _create_safe_alternative(self, unsafe_chain: ExploitChain,
                               vulnerability: Vulnerability) -> Optional[ExploitChain]:
        """Create safer alternative to unsafe chain."""
        logger.info(f"Creating safe alternative to unsafe chain with {len(unsafe_chain.steps)} steps")

        # Analyze the unsafe chain to understand what makes it risky
        risk_factors = self._analyze_chain_risks(unsafe_chain)
        logger.debug(f"Identified risk factors: {risk_factors}")

        # Build more conservative chain using safer primitives
        safe_requirements = {
            'max_risk_level': 'low',
            'avoid_destructive': True,
            'prefer_reversible': True,
            "stealth_priority": True,
            "no_system_modification": True,
            "limited_scope": True
        }

        alternative = self.chain_framework.build_exploit_chain(vulnerability, safe_requirements)
        if alternative:
            alternative.name = f"safe_{alternative.name}"
            logger.info(f"Created safe alternative: {alternative.chain_id}")

        return alternative

    def _analyze_chain_risks(self, chain: ExploitChain) -> List[str]:
        """Analyze risks in an exploit chain."""
        risks = []

        for step in chain.steps:
            # Check for destructive operations
            if step.primitive_type in ['file_deletion', 'registry_modification', 'system_shutdown']:
                risks.append(f"Destructive operation: {step.primitive_type}")

            # Check for high-privilege requirements
            if step.required_privileges and 'admin' in step.required_privileges.lower():
                risks.append(f"Requires admin privileges: {step.name}")

            # Check for network operations
            if 'network' in step.primitive_type.lower():
                risks.append(f"Network operation: {step.primitive_type}")

        return risks

    def validate_chain(self, chain_id: str) -> ChainValidationResult:
        """Validate existing exploit chain."""
        if chain_id not in self.built_chains:
            return ChainValidationResult(
                is_valid=False,
                confidence=0.0,
                issues=["Chain not found"]
            )

        chain = self.built_chains[chain_id]

        # Perform validation checks
        issues = []
        warnings = []
        optimizations = []

        # Check step dependencies
        available_outputs = set()
        for step in chain.steps:
            missing_prereqs = set(step.prerequisites) - available_outputs
            if missing_prereqs:
                issues.append(f"Step {step.step_id} missing prerequisites: {missing_prereqs}")
            available_outputs.update(step.outputs)

        # Check reliability
        if chain.success_probability < 0.5:
            warnings.append("Low success probability")
            optimizations.append("Consider alternative approaches")

        # Check complexity
        if chain.complexity in [ChainComplexity.ADVANCED, ChainComplexity.EXPERT]:
            warnings.append("High complexity may affect reliability")

        is_valid = len(issues) == 0
        confidence = chain.success_probability if is_valid else 0.0

        return ChainValidationResult(
            is_valid=is_valid,
            confidence=confidence,
            issues=issues,
            warnings=warnings,
            optimizations=optimizations,
            estimated_success_rate=chain.success_probability,
            risk_assessment=chain.safety_report
        )

    def get_chain_statistics(self) -> Dict[str, Any]:
        """Get statistics about built chains."""
        if not self.built_chains:
            return {"total_chains": 0}

        chains = list(self.built_chains.values())

        stats = {
            "total_chains": len(chains),
            "avg_success_probability": sum(c.success_probability for c in chains) / len(chains),
            "complexity_distribution": {},
            "safety_verified": sum(1 for c in chains if c.safety_verified),
            "tested_chains": sum(1 for c in chains if c.tested)
        }

        # Complexity distribution
        for complexity in ChainComplexity:
            count = sum(1 for c in chains if c.complexity == complexity)
            stats["complexity_distribution"][complexity.value] = count

        return stats


# Global instance
exploit_chain_builder = AutomatedExploitChainBuilder()
