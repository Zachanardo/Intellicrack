"""Comprehensive test suite for exploitation logger module.

This test suite validates the production-ready logging capabilities of the
exploitation module logger, focusing on real-world security research scenarios
and sophisticated logging functionality.

Copyright (C) 2025 Zachary Flint
Licensed under GPL v3.0 - see LICENSE for details.
"""

import unittest
import tempfile
import os
import json
import time
import threading
import sys
from datetime import datetime, timezone
from unittest.mock import patch, MagicMock, call
from pathlib import Path

# Import the module under test
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', '..', '..'))
from intellicrack.utils.exploitation.logger import log_message


class TestExploitationLogger(unittest.TestCase):
    """Test suite for exploitation logging functionality.

    Tests assume production-ready implementation with sophisticated
    logging capabilities for binary analysis and security research.
    """

    def setUp(self):
        """Set up test environment for each test case."""
        self.temp_dir = tempfile.mkdtemp()
        self.test_log_file = os.path.join(self.temp_dir, "test_exploitation.log")
        self.maxDiff = None

        # Create test binary samples for realistic logging scenarios
        self.test_binary_path = os.path.join(self.temp_dir, "test_sample.exe")
        with open(self.test_binary_path, 'wb') as f:
            f.write(b'MZ\x90\x00' + b'\x00' * 1020)  # Minimal PE header

    def tearDown(self):
        """Clean up test environment after each test case."""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)

    def test_basic_message_logging(self):
        """Test basic message logging with default parameters.

        Validates that basic logging functionality works for simple
        exploitation workflow messages.
        """
        message = "Starting binary analysis of protected executable"

        # Test basic logging functionality
        result = log_message(message)

        # Should return success indicator
        self.assertIsInstance(result, bool,
                            "log_message should return boolean success indicator")
        self.assertTrue(result, "Basic message logging should succeed")

    def test_security_research_log_levels(self):
        """Test all security research log levels are supported.

        Validates that the logger supports comprehensive log levels
        appropriate for security research and exploitation workflows.
        """
        security_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
        test_messages = [
            "Detailed shellcode generation debug info",
            "Binary analysis progress update",
            "Potential ASLR bypass opportunity detected",
            "Failed to inject payload into target process",
            "Critical exploitation framework failure"
        ]

        for level, message in zip(security_levels, test_messages):
            with self.subTest(level=level):
                result = log_message(message, level=level)
                self.assertTrue(result, f"Logging with {level} level should succeed")

    def test_exploitation_context_logging(self):
        """Test contextual logging for exploitation scenarios.

        Validates that rich contextual information can be logged
        for sophisticated security research workflows.
        """
        exploitation_contexts = [
            {
                "binary_path": self.test_binary_path,
                "binary_hash": "sha256:abcd1234567890...",
                "analysis_mode": "deep_scan",
                "researcher_id": "sec_researcher_001",
                "target_architecture": "x64"
            },
            {
                "target_process": "notepad.exe",
                "injection_method": "dll_injection",
                "shellcode_size": 1024,
                "success": True,
                "execution_time_ms": 45.2
            },
            {
                "protection_type": "aslr",
                "bypass_technique": "info_leak_gadget",
                "failure_reason": "insufficient_gadgets",
                "retry_count": 3,
                "vulnerability_class": "memory_corruption"
            }
        ]

        messages = [
            "PE analysis initiated for protected binary",
            "Shellcode injection attempt completed successfully",
            "ASLR bypass attempt failed - technique ineffective"
        ]

        for context, message in zip(exploitation_contexts, messages):
            with self.subTest(context=context):
                result = log_message(message, context=context)
                self.assertTrue(result, "Contextual logging should succeed")

    def test_multiple_output_destinations(self):
        """Test logging to multiple output destinations.

        Validates that the logger can output to various destinations
        appropriate for security research environments.
        """
        destinations = ["file", "console", "network", "database"]
        message = "Multi-destination logging test for exploitation framework"

        for destination in destinations:
            with self.subTest(destination=destination):
                result = log_message(message, destination=destination)
                self.assertTrue(result, f"Logging to {destination} should succeed")

    def test_timestamp_functionality(self):
        """Test timestamp handling and precision.

        Validates that precise timestamps are generated for
        timing analysis in exploitation workflows.
        """
        message = "Timestamp precision test for exploitation timing"

        # Test with timestamp enabled (default)
        start_time = time.time()
        result_with_timestamp = log_message(message, timestamp=True)
        end_time = time.time()

        self.assertTrue(result_with_timestamp, "Timestamped logging should succeed")

        # Test without timestamp
        result_without_timestamp = log_message(message, timestamp=False)
        self.assertTrue(result_without_timestamp, "Non-timestamped logging should succeed")

    def test_structured_output_formats(self):
        """Test structured output format support.

        Validates that the logger can output in formats suitable
        for automated analysis and integration with security tools.
        """
        message = "Structured output test for security research integration"
        context = {
            "technique": "rop_chain_construction",
            "gadgets_found": 47,
            "success_probability": 0.85
        }

        # These tests assume the logger supports format specification
        # through context or destination parameters
        structured_formats = ["json", "xml", "csv"]

        for format_type in structured_formats:
            with self.subTest(format=format_type):
                # Test structured output through context specification
                extended_context = {**context, "output_format": format_type}
                result = log_message(message, context=extended_context)
                self.assertTrue(result, f"Structured {format_type} output should succeed")

    def test_high_frequency_logging_performance(self):
        """Test performance under high-frequency logging scenarios.

        Validates that the logger can handle rapid logging without
        impacting exploitation workflow performance.
        """
        message_count = 1000
        start_time = time.time()

        # Simulate high-frequency exploitation logging
        success_count = 0
        for i in range(message_count):
            message = f"High-frequency exploitation log entry {i}"
            context = {
                "iteration": i,
                "payload_size": 512 + i,
                "technique": "buffer_overflow"
            }

            result = log_message(message, level="DEBUG", context=context)
            if result:
                success_count += 1

        end_time = time.time()
        total_time = end_time - start_time

        # Verify performance characteristics
        self.assertEqual(success_count, message_count,
                        "All high-frequency log messages should succeed")
        self.assertLess(total_time, 5.0,
                       "High-frequency logging should complete within 5 seconds")

        avg_time_per_log = total_time / message_count
        self.assertLess(avg_time_per_log, 0.005,
                       "Average logging time should be under 5ms")

    def test_concurrent_logging_thread_safety(self):
        """Test thread safety for concurrent exploitation logging.

        Validates that multiple exploitation threads can log
        simultaneously without data corruption or race conditions.
        """
        thread_count = 10
        messages_per_thread = 100
        results = []

        def worker_thread(thread_id):
            thread_results = []
            for i in range(messages_per_thread):
                message = f"Thread {thread_id} exploitation log {i}"
                context = {
                    "thread_id": thread_id,
                    "message_index": i,
                    "exploitation_phase": "payload_generation"
                }

                result = log_message(message, context=context)
                thread_results.append(result)

            results.extend(thread_results)

        # Start concurrent logging threads
        threads = []
        for thread_id in range(thread_count):
            thread = threading.Thread(target=worker_thread, args=(thread_id,))
            threads.append(thread)
            thread.start()

        # Wait for all threads to complete
        for thread in threads:
            thread.join()

        # Validate all logging operations succeeded
        total_expected = thread_count * messages_per_thread
        self.assertEqual(len(results), total_expected,
                        "All concurrent logging operations should complete")
        self.assertTrue(all(results),
                       "All concurrent logging operations should succeed")

    def test_security_data_sanitization(self):
        """Test sanitization of sensitive security research data.

        Validates that sensitive information is properly sanitized
        while maintaining the utility of exploitation logs.
        """
        sensitive_data_tests = [
            {
                "message": "Authentication bypass using credentials admin:password123",
                "context": {"password": "password123", "username": "admin"},
                "should_sanitize": True
            },
            {
                "message": "API key exploitation: sk-1234567890abcdef",
                "context": {"api_key": "sk-1234567890abcdef"},
                "should_sanitize": True
            },
            {
                "message": "Memory address leaked: 0x7fff12345678",
                "context": {"memory_address": "0x7fff12345678"},
                "should_sanitize": False  # Technical data, not personally sensitive
            },
            {
                "message": "Social security number found: 123-45-6789",
                "context": {"ssn": "123-45-6789"},
                "should_sanitize": True
            }
        ]

        for test_case in sensitive_data_tests:
            with self.subTest(message=test_case["message"][:30] + "..."):
                result = log_message(
                    test_case["message"],
                    context=test_case["context"]
                )
                self.assertTrue(result,
                              "Sanitization should not prevent successful logging")

    def test_binary_analysis_workflow_integration(self):
        """Test integration with real binary analysis workflows.

        Validates logging functionality within comprehensive
        security research and exploitation workflows.
        """
        # Simulate a complete binary analysis workflow
        workflow_steps = [
            {
                "phase": "initial_analysis",
                "message": "Starting comprehensive binary analysis",
                "context": {
                    "binary_path": self.test_binary_path,
                    "binary_size": os.path.getsize(self.test_binary_path),
                    "analysis_mode": "comprehensive"
                }
            },
            {
                "phase": "protection_detection",
                "message": "Analyzing protection mechanisms",
                "context": {
                    "protections_detected": ["aslr", "dep", "stack_canaries"],
                    "packer_detected": "upx",
                    "obfuscation_level": "moderate"
                }
            },
            {
                "phase": "vulnerability_analysis",
                "message": "Scanning for exploitable vulnerabilities",
                "context": {
                    "vulnerabilities_found": 3,
                    "critical_vulns": 1,
                    "exploit_techniques": ["buffer_overflow", "format_string", "use_after_free"]
                }
            },
            {
                "phase": "exploitation_attempt",
                "message": "Attempting exploitation with generated payload",
                "context": {
                    "payload_type": "reverse_shell",
                    "payload_size": 1024,
                    "target_vulnerability": "buffer_overflow",
                    "success": True
                }
            },
            {
                "phase": "post_exploitation",
                "message": "Post-exploitation analysis completed",
                "context": {
                    "privileges_gained": "SYSTEM",
                    "persistence_established": True,
                    "data_exfiltrated": False  # Research only
                }
            }
        ]

        for step in workflow_steps:
            with self.subTest(phase=step["phase"]):
                result = log_message(
                    step["message"],
                    level="INFO",
                    context=step["context"]
                )
                self.assertTrue(result, f"Workflow step {step['phase']} should log successfully")

    def test_error_handling_and_resilience(self):
        """Test error handling and resilience under adverse conditions.

        Validates that the logger handles error conditions gracefully
        without compromising exploitation workflow reliability.
        """
        error_scenarios = [
            {
                "scenario": "disk_full_simulation",
                "message": "Testing disk full condition handling",
                "context": {"simulated_error": "disk_full"}
            },
            {
                "scenario": "permission_denied",
                "message": "Testing permission denied handling",
                "context": {"simulated_error": "permission_denied"}
            },
            {
                "scenario": "network_unreachable",
                "message": "Testing network logging failure handling",
                "context": {"simulated_error": "network_unreachable"},
                "destination": "network"
            },
            {
                "scenario": "invalid_context_data",
                "message": "Testing invalid context data handling",
                "context": {"invalid_data": object()}  # Non-serializable object
            }
        ]

        for scenario in error_scenarios:
            with self.subTest(scenario=scenario["scenario"]):
                # These tests verify graceful error handling
                # Production implementation should handle these gracefully
                try:
                    result = log_message(
                        scenario["message"],
                        context=scenario["context"],
                        destination=scenario.get("destination", "file")
                    )
                    # Should either succeed or fail gracefully (return False)
                    self.assertIsInstance(result, bool,
                                        "Error handling should return boolean")
                except Exception as e:
                    # If exceptions occur, they should be documented and expected
                    self.fail(f"Unexpected exception in error scenario {scenario['scenario']}: {e}")

    def test_configuration_and_customization(self):
        """Test configuration and customization capabilities.

        Validates that the logger can be configured for different
        security research environments and requirements.
        """
        configuration_tests = [
            {
                "config": {"log_level": "DEBUG", "verbose": True},
                "message": "Debug level configuration test"
            },
            {
                "config": {"output_format": "json", "include_metadata": True},
                "message": "JSON format configuration test"
            },
            {
                "config": {"batch_size": 100, "async_logging": True},
                "message": "Performance configuration test"
            },
            {
                "config": {"sanitize_sensitive": True, "redact_patterns": ["password", "key"]},
                "message": "Security configuration test with password and key data"
            }
        ]

        for test in configuration_tests:
            with self.subTest(config=test["config"]):
                # Test configuration through context (assuming context-based config)
                extended_context = {**test["config"], "test_mode": True}
                result = log_message(test["message"], context=extended_context)
                self.assertTrue(result, "Configured logging should succeed")

    def test_real_world_exploitation_scenarios(self):
        """Test logging in realistic exploitation scenarios.

        Validates logging functionality with data and scenarios that
        mirror real-world security research and penetration testing.
        """
        real_world_scenarios = [
            {
                "scenario": "web_application_exploitation",
                "logs": [
                    {
                        "message": "SQL injection vulnerability discovered in login form",
                        "level": "INFO",
                        "context": {
                            "vulnerability_type": "sql_injection",
                            "affected_parameter": "username",
                            "payload": "admin' OR '1'='1'--",
                            "response_time_ms": 234,
                            "success": True
                        }
                    },
                    {
                        "message": "Database enumeration completed successfully",
                        "level": "INFO",
                        "context": {
                            "tables_discovered": 15,
                            "sensitive_tables": ["users", "payments", "admin_sessions"],
                            "records_accessible": 10000,
                            "technique": "union_based_injection"
                        }
                    }
                ]
            },
            {
                "scenario": "binary_exploitation",
                "logs": [
                    {
                        "message": "Buffer overflow vulnerability confirmed",
                        "level": "WARNING",
                        "context": {
                            "vulnerability_type": "stack_buffer_overflow",
                            "overflow_offset": 112,
                            "control_achieved": True,
                            "mitigation_bypassed": ["aslr", "nx"],
                            "exploitation_difficulty": "medium"
                        }
                    },
                    {
                        "message": "ROP chain construction successful",
                        "level": "INFO",
                        "context": {
                            "gadgets_used": 8,
                            "chain_length": 64,
                            "payload_function": "system",
                            "reliability": 0.95,
                            "target_architecture": "x86_64"
                        }
                    }
                ]
            },
            {
                "scenario": "malware_analysis",
                "logs": [
                    {
                        "message": "Malware sample unpacked successfully",
                        "level": "INFO",
                        "context": {
                            "original_packer": "themida",
                            "unpacking_method": "manual_reconstruction",
                            "sample_hash_original": "sha256:abc123...",
                            "sample_hash_unpacked": "sha256:def456...",
                            "analysis_time_minutes": 45
                        }
                    },
                    {
                        "message": "C2 communication protocol decoded",
                        "level": "INFO",
                        "context": {
                            "protocol": "custom_tcp",
                            "encryption": "xor_with_key",
                            "c2_servers": ["192.168.1.100", "malicious.example.com"],
                            "communication_frequency": "every_10_minutes"
                        }
                    }
                ]
            }
        ]

        for scenario in real_world_scenarios:
            with self.subTest(scenario=scenario["scenario"]):
                for log_entry in scenario["logs"]:
                    result = log_message(
                        log_entry["message"],
                        level=log_entry["level"],
                        context=log_entry["context"]
                    )
                    self.assertTrue(result,
                                  f"Real-world scenario {scenario['scenario']} logging should succeed")


class TestExploitationLoggerIntegration(unittest.TestCase):
    """Integration tests for exploitation logger with Intellicrack systems."""

    def test_intellicrack_framework_integration(self):
        """Test integration with broader Intellicrack framework.

        Validates that the exploitation logger integrates seamlessly
        with other Intellicrack components and workflows.
        """
        # Test integration with mock Intellicrack components
        framework_integration_scenarios = [
            {
                "component": "binary_analyzer",
                "message": "Binary analysis integration test",
                "context": {"component": "binary_analyzer", "integration": True}
            },
            {
                "component": "gui_interface",
                "message": "GUI logging integration test",
                "context": {"component": "gui_interface", "integration": True}
            },
            {
                "component": "reporting_system",
                "message": "Report generation integration test",
                "context": {"component": "reporting_system", "integration": True}
            }
        ]

        for scenario in framework_integration_scenarios:
            with self.subTest(component=scenario["component"]):
                result = log_message(
                    scenario["message"],
                    context=scenario["context"]
                )
                self.assertTrue(result, f"Integration with {scenario['component']} should work")

    def test_external_tool_integration(self):
        """Test integration with external security research tools.

        Validates that logging output is compatible with common
        security research and analysis tools.
        """
        external_tools = [
            {
                "tool": "ghidra_integration",
                "message": "Ghidra script execution logging test",
                "context": {
                    "tool": "ghidra",
                    "script_name": "find_crypto.py",
                    "execution_time": 12.5,
                    "results_found": 7
                }
            },
            {
                "tool": "ida_pro_integration",
                "message": "IDA Pro plugin logging test",
                "context": {
                    "tool": "ida_pro",
                    "plugin_name": "intellicrack_analyzer",
                    "database_size": "45MB",
                    "analysis_complete": True
                }
            },
            {
                "tool": "metasploit_integration",
                "message": "Metasploit module execution logging test",
                "context": {
                    "tool": "metasploit",
                    "module": "exploit/windows/browser/ie_zero_day",
                    "target": "windows/meterpreter/reverse_tcp",
                    "session_id": 1
                }
            }
        ]

        for tool in external_tools:
            with self.subTest(tool=tool["tool"]):
                result = log_message(
                    tool["message"],
                    context=tool["context"]
                )
                self.assertTrue(result, f"Integration with {tool['tool']} should work")


if __name__ == "__main__":
    # Configure test environment
    unittest.TestLoader.sortTestMethodsUsing = None  # Preserve test order

    # Create test suite
    suite = unittest.TestSuite()

    # Add test classes
    suite.addTest(unittest.defaultTestLoader.loadTestsFromTestCase(TestExploitationLogger))
    suite.addTest(unittest.defaultTestLoader.loadTestsFromTestCase(TestExploitationLoggerIntegration))

    # Run tests with detailed output
    runner = unittest.TextTestRunner(
        verbosity=2,
        stream=sys.stdout,
        descriptions=True,
        failfast=False
    )

    print("=" * 80)
    print("INTELLICRACK EXPLOITATION LOGGER TEST SUITE")
    print("=" * 80)
    print("Testing production-ready logging capabilities for binary")
    print("analysis and security research workflows.")
    print()
    print("Test Philosophy: Specification-driven, black-box testing")
    print("Expected Functionality: Advanced exploitation logging with")
    print("sophisticated features for professional security research.")
    print("=" * 80)
    print()

    result = runner.run(suite)

    # Print summary
    print("\n" + "=" * 80)
    print("TEST EXECUTION SUMMARY")
    print("=" * 80)
    print(f"Tests Run: {result.testsRun}")
    print(f"Failures: {len(result.failures)}")
    print(f"Errors: {len(result.errors)}")
    print(f"Skipped: {len(result.skipped) if hasattr(result, 'skipped') else 0}")

    if result.failures:
        print("\nFAILED TESTS:")
        for test, failure in result.failures:
            print(f"  - {test}: {failure.split(chr(10))[0]}")

    if result.errors:
        print("\nERROR TESTS:")
        for test, error in result.errors:
            print(f"  - {test}: {error.split(chr(10))[0]}")

    success_rate = ((result.testsRun - len(result.failures) - len(result.errors)) /
                   result.testsRun * 100) if result.testsRun > 0 else 0
    print(f"\nSuccess Rate: {success_rate:.1f}%")

    if success_rate < 80:
        print("\nWARNING  WARNING: Success rate below 80% indicates functionality gaps")
        print("This suggests the current implementation may not meet production requirements.")
    else:
        print("\nOK SUCCESS: Test suite validates production-ready capabilities")

    print("=" * 80)
